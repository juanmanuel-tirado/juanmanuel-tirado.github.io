[{"content":"","date":"20 December 2023","permalink":"/posts/","section":"","summary":"","title":""},{"content":"","date":"20 December 2023","permalink":"/categories/","section":"Categories","summary":"","title":"Categories"},{"content":"","date":"20 December 2023","permalink":"/","section":"jmtirado.net","summary":"","title":"jmtirado.net"},{"content":"On July 2022 I decided to migrate from Wordpress to Jekyll and host the blog on GitHub (see my previous post ( here).\nWhy another migration? # There is not a particular reason for this. I was OK with Jekyll and its simplicity. However, I was a bit disappointed installing Ruby dependencies. I do not use it for any other purpose rather than generating static content for Jekyll. A powerful reason to make the migration is the fact that Hugo is written in Go, and guess what I\u0026rsquo;m familiar with that language\n","date":"20 December 2023","permalink":"/posts/20231220_hugo_migration/","section":"","summary":"On July 2022 I decided to migrate from Wordpress to Jekyll and host the blog on GitHub (see my previous post ( here).\nWhy another migration? # There is not a particular reason for this.","title":"Migration from Jekyll to Hugo"},{"content":"","date":"20 December 2023","permalink":"/categories/opinion/","section":"Categories","summary":"","title":"Opinion"},{"content":"","date":"20 December 2023","permalink":"/tags/optinion/","section":"Tags","summary":"","title":"optinion"},{"content":"Public events # * OpenSouthCode23 (June 9th). Juju or how to deploy your applications magically Applications can be deployed on a large variety of platforms: bare metal, private clouds, virtual machines, public clouds (AWS, Azure, GCE), etc. Afterwards, they have to be updated, configured, and maintained. This requires a lot of work, a deep understanding of the platforms, and imposes additional complexity to your solutions. I will show you what Juju and the Charmed Operator Lifecycle Manager can do to simplify your deployments and let you focus on what is really important. Link\nBooks # Build Systems with Go: everything a Gopher must know (2021) Link Journal articles # Tirado, J.M. et al. (2014). “CONDESA: A Framework for Controlling Data Distribution on Elastic Server Architectures”. In: IEEE Transactions on Parallel and Distributed Systems. Tirado, Juan M., Daniel Higuero, et al. (2010). “Affinity P2P: A self-organizing content-based locality-aware collaborative peer-to-peer network”. In: Computer Networks. Higuero, D. Juan M. Tirado, J. Carretero, et al (2009). “HIDDRA: a highly independent data distribution and retrieval architecture for space observation missions”. In: Astrophysics and Space Science. Conference proceedings # Galindo, José A. et al. (2016). “Exploiting the Enumeration of All Feature Model Configurations: A New Perspective with Distributed Computing”. In: International Systems and Software Product Line Conference Kermarrec, Anne-Marie, Francois Taiani, and Juan M. Tirado (2015). “Scaling Out Link Prediction with SNAPLE: 1 Billion Edges and Beyond”. In: Middleware Conference. Kermarrec, Anne-Marie, Francois Taïani, and Juan M. Tirado (2015). “Cheap and Cheerful: Trading Speed and Quality for Scalable Social-Recommenders”. In: Distributed Applications and Interoperable Systems. Higuero, D., Juan M. Tirado, F. Isaila, et al. (2012). “Enhancing file transfer scheduling and server utilization in data distribution infrastructures”. In: Modeling, Analysis \u0026amp; Simulation of Computer and Telecommunication Systems. Tirado, Juan M., D. Higuero, et al. (2012). “Reconciling dynamic system sizing and content locality through hierarchical workload forecasting”. In: Parallel and Distributed Systems. Tirado, Juan M., Francois Taiani, et al. (2012). “Geology: Modular Georecommendation In Gossip-Based Social Networks”. In: International Conference on Distributed Computing Systems. Tirado, Juan M., D. Higuero, et al. (2011a). “Analyzing the impact of events in an online music community”. In: Workshop on Social Network Systems. Tirado, Juan M., D. Higuero, et al. (2011b). “Multi-model prediction for enhancing content locality in elastic server infrastructures”. In: International Conference on High Performance Computing. Tirado, Juan M., D. Higuero, et al. (2011c). “Predictive Data Grouping and Placement for Cloud-based Elastic Server Infrastructures”. In: IEEE International Symposium on Cluster Computing and the Grid. Technical reports # Tirado, Juan M., Ovidiu Serban, et al. (Mar. 2016). Web data knowledge extraction. Tech. rep. University of Cambridge, Computer Laboratory. [Link](http: //www.cl.cam.ac.uk/techreports/UCAM-CL-TR-881.pdf) ","date":"20 December 2023","permalink":"/publications/","section":"jmtirado.net","summary":"Public events # * OpenSouthCode23 (June 9th). Juju or how to deploy your applications magically Applications can be deployed on a large variety of platforms: bare metal, private clouds, virtual machines, public clouds (AWS, Azure, GCE), etc.","title":"Publications"},{"content":"","date":"20 December 2023","permalink":"/tags/","section":"Tags","summary":"","title":"Tags"},{"content":"Who are you? # Juan M. Tirado\nWhat do you do? # I am a computer scientist with a particular interest in how data can leverage large distributed systems. I help individuals and companies to design, understand, and implement data-driven solutions. In my free time, I enjoy (in no particular order) music, mountaineering, and tapas. Why this blog?\nI collect a large a mount of examples, solutions, and ideas on a daily basis. This is a good channel to share this content with a broader audience. Maybe somebody will find it useful.\nHow can I contact you? # You can contact me through social media. If you want to contact me for professional purposes use LinkedIn.\n","date":"19 December 2023","permalink":"/about/","section":"jmtirado.net","summary":"Who are you? # Juan M. Tirado\nWhat do you do? # I am a computer scientist with a particular interest in how data can leverage large distributed systems. I help individuals and companies to design, understand, and implement data-driven solutions.","title":"About"},{"content":"","date":"6 September 2023","permalink":"/tags/opensource/","section":"Tags","summary":"","title":"opensource"},{"content":"","date":"6 September 2023","permalink":"/tags/opinion/","section":"Tags","summary":"","title":"opinion"},{"content":"","date":"6 September 2023","permalink":"/categories/programming/","section":"Categories","summary":"","title":"Programming"},{"content":"","date":"6 September 2023","permalink":"/categories/technology/","section":"Categories","summary":"","title":"Technology"},{"content":"If you\u0026rsquo;re a developer or simply an open-source adopter you have probably come across Emacs. For the eyes of the inexperienced an incomprehensible bunch of letters in an ugly layout, for the experienced an awesome productivity tool. This is my personal view of this historical editor and why I have used it, stopped using it, and finally made it my default editor.\nEmacs Logo How the ******** do you write something here? # The first time I saw something called Emacs was in a computer lab back in 2002. I was completely new to Linux and in that lab, I had my first crash course. As a newbie, I had to write and compile a helloworld.c. The first task was to write the code and for that, you need a text editor. I don\u0026rsquo;t remember the distribution installed in those computers but I vividly remember a terrible UX experience trying to find a text editor. I found something similar to a Windows notepad and wrote the code, compiled it using the terminal, and felt like a hacker after displaying a Hello World! message in the terminal prompt.\nWhen I opened my helloworld.c file again something unexpected happened. The code was shown to me in a grey background editor. I cannot say the look-and-feel was catchy or pretty. That was XEmacs, which was the default editor in that distribution. From there\u0026hellip; chaos. First thing, how do I write here? I could see the cursor moving up and down but typing was simply showing incomprehensible messages and suddenly I typed something. This is a text editor. Why I can\u0026rsquo;t simply write as I type? okay, this is finally done. How do I save it? Oh, there is a save button. Click on it. Mmmm has it been saved? I probably spent more than five minutes doing such a simple task. That was not a nice experience. Nevertheless\u0026hellip;\nFar beyond a text editor # For some reason I cannot explain (sadism maybe?) I started using Emacs on a daily basis. No need to say that I kept spending an embarrassingly amount of time trying to do simple things (copying, pasting, selecting text regions). Anyway, I remember that we had to explore some large text files and Emacs was the only editor that could load the files and search for text chunks without exhausting the memory. Obviously, I had no idea about the existence of grep or awk. The most perplexing thing for me was discovering that there were games inside Emacs. Some examples here.\nEmacs is from an architectural point of view more than a simple text editor. It has a Lisp interpreter. Actually, a dialect of LISP called Emacs Lisp(Elisp). Most of the functionality is written in this dialect, while the remainder is written in C. This means that you can easily port your scripts to other machines without recompiling.\nEmacs is terribly configurable. You have the freedom to configure everything. You can control everything. Total freedom. Everything can be changed, and everything can be customised. If you start playing with your init.el file, you can fall into a neverending loop. The worst thing is copying and pasting others\u0026rsquo; solutions to certain problems such as using CTRL+c and CTRL+v instead of the mysterious shortcut, having larger fonts, using a different fancy layout\u0026hellip; You maintain that file for years and it keeps growing and growing. And because you copied and pasted everything guess what\u0026hellip; You have no idea how it works and what it was for. So you have to start paying attention and learning something about how everything works. Because it will eventually break. Somehow this is the philosophy of a software developer. This is not a price everybody will pay when using a text editor.\nYou can do everything in the same place # You can do everything (or almost everything) ONLY using Emacs. I do not exaggerate. You can send email, checkout GIT branches, compile a LaTex file, display a PDF file, run terminals, run your own agenda\u0026hellip; Everything in the same place. Have I mentioned that there is syntax highlighting or drop down suggestions? You can find experienced developers using Emacs as their only tool. Buffer content that appears and disappears after \u0026ldquo;secret\u0026rdquo; key combinations displaying this and that. Some users can be extremmely efficient and run awesome pipelines in their daily tasks. After how many years using Emacs? God knows.\nA client server architecture # Not many people know this, but Emacs can work in a server-client modality. This is interesting for two reasons. First, you can start Emacs as a server when booting your machine. Now you can run clients connecting to this instance and you don\u0026rsquo;t have to wait for package loading. Second, this can be an interesting option if you want to have a machine only for development/computation. Imaging connecting from your laptop using Emacs to the server in that machine and operate as usual. Particularly useful if you develop on a different platform an cross-compilation is not an option.\nMore functionality by adding packages # Emacs does not have a marketplace like other solutions such as VSCode or JetBrains IDEs. In this case, Emacs relies on MELPA (Milkypostman\u0026rsquo;s Emacs Lisp Package Archive) as the main source of curated packages. You can directly install them and make them run in your running instance. Obviously, this is not as fancy as a marketplace running plugins but it is closer to a developers mentality.\n40 years of history!!! # This was the first GNU release done back in 1985 by Richard Stallman himself. That\u0026rsquo;s almost 40 years!!! The code we have today differs from the original release, but the essence is still the same. Do you imagine yourself using VSCode or JetBrains for 40 years? Touché.\nThe Org mode # Org Mode logo This for me one of the main reasons to use Emacs. The Org mode is a plain text quite easy to learn but with enough descriptiveness to do things such as maintaining an agenda, displaying and showing code results, and many others. Something like:\n#+begin_src python import random return [random.random() for i in range(5)] #+end_src Is not only going to be beautifully coloured, it can also be interpreted and the result displayed inline like some sort of Jupyter Notebook.\n#+RESULTS: | 0.9802045044675929 | 0.7814182240285613 | 0.7945518565001365 | 0.7875101844790451 | 0.999364766119846 | There are a good number of packages that permit to export from the Org mode to other formats such as HTML, Markdown, LaTex, ODT, RevealJS, etc. This combined with the definition of pipelines can be extremely useful for those working on content that requires to go into different target formats.\nIt is almost impossible to cover all the possibilities here, but if you\u0026rsquo;re curious you can take a look at the official documentation. There you can find some examples of what people do with the Org mode. Meanwhile, you can check some of the current features.\nThe learning curve # If you have read so far, you have probably noticed that Emacs is not easy to learn. It takes time. And a lot of frustration. The main problem comes with the UX. Emacs is mainly developed to be used with your keyboard, although the mouse is supported. Unfortunately, it takes a while to understand and memorize the keyboard shortcuts. This is more of a repetitive process rather than a pure memorization exercise. Concepts such as buffers or modes are strange things to learn for somebody who wants to sit down and start typing.\nIf you\u0026rsquo;re a developer and you want to contribute or simply write your own program you will have to learn Emacs Lisp. This is not a widely used language, so you will probably have to learn something new.\nSome historical complaints # Many users publicly complain about Emacs feeling clunky, specially compared with today\u0026rsquo;s IDEs. And they are probably right. Although I have to say, it has improved a lot since my first contact with this software. However, we still have a steep learning curve.\nAs I mentioned earlier, Emacs is based on Lisp and this reduces the number of developers willing to design new features. Other languages such as Javascript have been proposed to be supported. However, Stallman himself is clear about why Javascript is not welcome at this moment. Finally, the fact that Emacs is single-threaded is a big limitation compared with other solutions. Nevertheless, if you take a look at the memory footprint you will probably embrace Emacs and ignore this limitation.\nHelpful Emacs frameworks # While Emacs is fully configurable, and it already exists good documentation to start your own vanilla Emacs, existing frameworks will simply save you a lot of time and effort. I only mention two of them:\nspacemacs it is designed to help users with all the emacs commands in a consistent manner. spacemacs Doom Emacs is another already configured emacs with a good number of curated packages. Doom Emacs Where to start # The very first thing is to install it. There are installation packages for all distributions and platforms. If you\u0026rsquo;re looking for a book, Mastering Emacs can be a good (but expensive) option. If you want to learn more about Emacs Lisp go for the official Introduction to Programming in Emacs Lisp.\nIf you\u0026rsquo;re more into video tutorials, there are some nice resources on YouTube. These are my recommendations:\nThe Absolute Beginner\u0026rsquo;s Guide to Emacs contains all you need to understand the main concepts. Org Mode Basics is a wonderful summary of how the Org mode works. DistroTube has a list of videos about how to configure Emacs. This is the first one on the list. Finally, you can find Emacs enthusiasts on StackExchange or reddit.\nConclusion # Emacs is a rara avis in the software ecosystem. The community has been maintaining and developing it for almost 40 years. It is an awesome tool with a steep learning curve that can lead adopters to frustration. For those who have already adopted Emacs as their main programming tool congratulations. For those who have forgotten about the beast, give it a second try. And for those who have no idea what it feels just, give it a try by using one of the existing frameworks.\nFor me, it has become my default editor. This doesn\u0026rsquo;t mean I am not going to use any other tool. This only means, that Emacs is my baseline and I will compare every solution with this software beast.\nThanks for reading.\n","date":"6 September 2023","permalink":"/emacs_is_my_default_editor_again/","section":"","summary":"If you\u0026rsquo;re a developer or simply an open-source adopter you have probably come across Emacs. For the eyes of the inexperienced an incomprehensible bunch of letters in an ugly layout, for the experienced an awesome productivity tool.","title":"Why Emacs is my default programming editor... again"},{"content":"","date":"19 June 2023","permalink":"/categories/community/","section":"Categories","summary":"","title":"community"},{"content":"","date":"19 June 2023","permalink":"/categories/conference/","section":"Categories","summary":"","title":"conference"},{"content":"","date":"19 June 2023","permalink":"/categories/opensource/","section":"Categories","summary":"","title":"opensource"},{"content":"I had the honour of attending the OpenSouthCode 2023 held at the beatiful Málaga between June 9th and 10th. This is my brief summary of the event, and an invitation for everybody to attend this community-driven event.\nAbout OpenSouthCode # For those who have never heard about this conference, the OpenSouthCode has been celebrated for many years in Málaga. The main goal of the event is to promote open source technologies and it serves as a meeting point for the community, practitioners, and open source adopters. Over the years this annual meeting has evolved into a major conference attracting not only the community, but also sponsoring companies (VMWare and Mattermost sponsored this edition), professionals, free lancers, students, and families (I will explain this later) celebrating open source in an open and friendly environment. This edition was particularly special for the community, because it was the first after the Covid restrictions.\nI have to say that part of the success of this conference is due to the huge efforts and dedication of the organizers (all of them volunteers) combined with the attraction that Malaga is having in the tech industry. Apart from being a well known destination for digital nomads, large companies such as Google or Vodafone have created divisions there. Other companies are following the same path creating a new vibrant ecosystem that grows and grows. Some peope talk about a new Silicon Valley in the South of Europe.\nThe organization # The conference had 8 simultaneous tracks. One of them was fully dedicated to the KDE community which had its annual meeting inside the conference. Apart from the talks there were dedicated workshops. I could not attend all the talks for obvious reasons, but I would say DevOps/DevSecOps, data processing, and AI were major topics in this edition.\nSponsors had their stands in the main corridor and organized some quizs and raffles.\nI was there # My presentation entitled \u0026ldquo;Juju or how to deploy your applications magically\u0026rdquo; was an introduction to Juju for those who have never heard of it providing a basic explanation around the Juju ecosystem (Juju architecture + CLI, python-libjuju, charms, the terraform provider, etc.). I was unsure about the technical level of the audience, but most of them were familiar with the problems and found interesting the solutions brought by Juju. There were some follow-ups, clarifications, and chatting.\nSome interesting topics # Obviously, I could not attend the 8 simultaneous tracks. Therefore, this is a biased summary of some of the talks I attended.\nOn Saturday, Jürgen Gmach presented \u0026ldquo;Behind the Scenes of tox: The Journey of Rewriting a Python Tool with Over 10 Million Monthly Downloads\u0026rdquo; where he explained the story of Tox and how he became a maintainer of the project after participating in the library rewrite. No need to mention that Tox is one of the most sucessful libraries written in Python, and hearing the lessons learnt by Jürgen during the process was really valuable. The audience made several questions not only about his experience as a maintainer, but also around other Python libraries and the future of the Tox project.\nJorge Hidalgo from Accenture presented DevSecOps mythbusters, an interesting collection of experiences around his daily work looking for code vulnerabilities, open tools, methodologies, good practices, etc. Really instructional, and it follows many of the ideas Canonical puts into practice for security.\nI\u0026rsquo;m not a videogame developer, but I really enjoyed the presentation from Francesco Cavallari about Open games for good. He explained how his NGO Video Games Without Borders develops open source videogames that can help vulnerable people. I found really powerful the example of how his video game could help Syrian refugees to continue his reading courses in the middle of a humanitarian crisis. This was a very illustrative example of how open source can help people even under terrible circumstances in their daily lives. Furthermore, there was an interesting round of questions about how to find funding for these projects, how to engage developers, or what happens with the intellectual property.\nI would like to highlight this talk from Alexander Sander from the Free Software Foundation \u0026ldquo;EU: Proposed liability rules will harm Free Software\u0026rdquo; where he explained how the EU parlament is legislating around the creation of a CE label for software products. For those of us with none or little understanding of legal issues, this talk was very explanatory. Long made short, the EU wants to regulate how software can have a CE label like in many other industrial products. Unfortunately, for open source developers the legislation is not being favorable because the EU in a first iteration decided that developers are liable when their software is used. Alexander explained how the FSF is participating in the regulation process, and how they managed to add amendments changing the liability from the software developers to the deployers, the future of the discussion, and how the industry is participating here.\nFinally, I would like to mention the OpenSouthKids. This is a workshop made for kids to start in the programming/tech world. The kids had to complete challenges using programming tools, a 3D printer, a tablet, VR devices, etc. to get all the stamps of some sort of passport. An awesome experience for the kids who had tons of fun. Their parents were also having a lot of fun too. I\u0026rsquo;m sure some of them went straight back home and bought a 3D printer using their kids as a excuse.\nSocial events # No community event is complete without social events and this is not going to be the exception. The organizers made an awesome job: live music with a band playing videogame soundtracks, beer, food trucks, more food, beer, and espetos. If you don\u0026rsquo;t know what espetos are see the picture below.\nConclusions # The OpenSouthCode is a nice event that is growing in numbers. Their organizers started this whole story in a local community, and now they have 700 attendees from all Europe! I hope this event keeps growing and start attracting more talent. Who knows what synergies can be created there.\n","date":"19 June 2023","permalink":"/opensouthcode23/","section":"","summary":"I had the honour of attending the OpenSouthCode 2023 held at the beatiful Málaga between June 9th and 10th. This is my brief summary of the event, and an invitation for everybody to attend this community-driven event.","title":"OpenSouthCode 2023"},{"content":"","date":"19 June 2023","permalink":"/categories/software/","section":"Categories","summary":"","title":"Software"},{"content":"","date":"20 June 2022","permalink":"/categories/education/","section":"Categories","summary":"","title":"education"},{"content":"","date":"20 June 2022","permalink":"/categories/phd/","section":"Categories","summary":"","title":"phd"},{"content":"This post is dedicated to all of you who are thinking about pursuing a PhD. If you have ever applied for a job, you would probably know that you are expected to do your homework to maximize your opportunities and be sure that you are applying for the correct position. If you\u0026rsquo;re considering applying for a PhD this is the same case.\nYou can easily find good posts and books on how to maximize your opportunities when applying for a job. Unfortunately, numbers are much more humble when we refer to PhDs. The percentage of people applying for a PhD is much smaller, so we can expect a limited amount of references. During these years, I have realized that those starting a PhD do not understand the idiosyncrasy of research, PhDs, funding, and other basic words in the PhD vocabulary.\nIt is interesting how universities do not inform students about what is a PhD, what does it mean to be a PhD, the benefits, the drawbacks, the impact on your career, etc. There is an aura of mysticism that prevents academia from evaluating the pros and cons of the first years of the researcher. A PhD qualifies a person as a researcher. Purely. That is what you should try to achieve: learn how to do research in an area of your interest.\nUnfortunately, a PhD differs from a regular job in many aspects. For example, it is a known fact that PhD candidates suffer from mental disorders such as depression or even suicidal tendencies. Not to mention that remuneration is not competitive while the workload can be excessive. Why does nobody tell you about this before applying?\nI do not want to discourage anybody from pursuing a PhD. Nevertheless, asking yourself the correct questions and doing some initial investigation will save you time and effort. The following is a list of questions/ideas you should explore before deciding whether a PhD is for you or not. I have classified them into five sections to be easily digested:\nDownsides nobody explains to you Your advisor The research group The topic Funding Consider the PhD downsides nobody explains to you # These are some of the things you discover during your PhD that nobody openly talks about. These are some of the downsides you have to bear in mind.\nPhD candidates are poorly paid. That\u0026rsquo;s a fact. During your PhD days, you are considered to be a student/candidate. This implies (with some exceptions) that you will get a basic salary. It doesn\u0026rsquo;t matter how complex your topic is, or how minany hours you dedicate to it. When compared to your colleagues working for the industry you may easily find that you are earning two or three times less.\nYour PhD will be your family. Somehow getting a PhD becomes a lifestyle. Huge dedication, travelling, workshops, symposiums, teaching\u0026hellip; Do not expect to have spare time for certain \u0026ldquo;adult life adventures\u0026rdquo; such as having your own family. I have met people who managed to finish their PhDs and raise children. I am conscious of the humongous effort they did and how their PhD, family life, and quality of life was impacted.\nMental health. This topic is gaining the relevance it deserves. Mental disorders are common among PhD candidates. You can extend this to academics. The levels of stress, the long working hours, and continuous uncertainty are the perfect mix to dismantle your sanity. Holding a diploma is not necessarily good if your health is in ruins. If you are under treatment do not hesitate to query your specialist before starting this adventure.\nA PhD may not boost your career. Holding a PhD will not guarantee you a better position or salary. I\u0026rsquo;m sorry. It can be a plus depending on your area, your expertise, and what are employers looking for. However, let me tell you that in some scenarios PhDs are even not well-considered. Why? Because outside academia those extra years you spent at the university to end up applying for a \u0026ldquo;regular\u0026rdquo; job are considered a distraction in your CV. This is not always the case but somebody could indeed have used those years to gain more practical experience. This is something that can be discussed but it is a common thought.\nThere are more PhD graduates than positions in Academia. If you dream of being a professor at the university let me tell you something: that is improbable. Maths do not lie here. Universities are generating PhD graduates at a speed never seen before. Just to give you some numbers in 2014 only in the US 67,449 people graduated with a PhD ( here).\nYour advisor # Your advisor will be the main authority who has to guide you in the darkness of your PhD adventure. I do not exaggerate when I say that finding a good advisor will make the difference between an enjoyable journey and a descent to the seventh circle of hell. Just google around about what people think about their advisors.\nYou will indeed end up having an emotional link with your advisor. We know that emotions are unpredictable. That lovely person who was genuinely interested in your ideas may end up being the person you hate the most. However, from a more practical perspective, there are some items you may want to check.\nList of publications. I know you are probably new to the world of research but this is the most basic indicator of how proficient is a researcher. Google your potential advisor or directly search him in Google Research. Does he have papers? How many? Do the titles contain some of the topics you would like to explore? Is he the main author or not?\nThe resume. I do not expect you to understand all the different publications, books, awards, fundings, and who-knows-what-other entries the resume of a full professor can have. Not always the larger the resume, the better it is. You can check other things. For example, has this person collaborated with other universities or has he developed his entire career in the same place? Does he get funding from companies or the administration? Does he teach any courses? How many? Is he on the program committee of any conference? Maybe in an editorial board? The more open-minded and experienced is your advisor, the better help you can get from him.\nThe position. Depending on the country and university, professors have to go through different positions before getting a permanent one. And then, there are several types of permanent positions. We can expect from a full professor a dilatated career, plenty of experience, a clear understanding of the state of the art, knowing the names of the important people in the area, hundreds of papers, etc. Unfortunately, full professors can be overloaded with bureaucracy, and spend a lot of time looking for funding, projects, and other staff. If that is the case, your advisor will be the person you need to sign some of your documents rather than a person you can share your research with. There is a big difference between an absent advisor and somebody with a genuine interest in what you do. On the other side, junior advisors who are struggling to get a permanent or more senior position may show a more genuine interest in your PhD. Why? Because they need to accumulate more merits to step up in the hierarchy. Think that for these researchers you are more a collaborator rather than a simple student. It is true that in other circumstances junior researchers try to accumulate as many PhD students as possible to grow their list of publications at the cost of the students. I have also seen very senior researchers working hand in hand with their students. As you can see the position of your advisor may not have a direct impact on your PhD experience. A full professor can facilitate your access to resources and may extend your connections to other universities. However, you can be easily ignored. As explained before, a more junior researcher can be more helpful but will have more difficulties in terms of funding and other resources. But in both cases, there is something you must know: how long will your advisor be in that university? Depending on their positions, contracts, and other issues they may not be in the same place before you get your title. And that would be a real problem.\nFormer students. A good indicator of a good PhD advisor is his former students. Take a look at those who got a PhD under his supervision. Did they publish in relevant journals or conferences? Are they professors or have positions in places you find interesting? How many people has he supervised so far? Maybe you can send them a message and kindly ask him about his experience.\nThe research group # During your PhD, you will be part of a larger research group. Finding a research group is not only about finding a group of individuals publishing amazing papers that move forward the borders of science. There is a lot about the internal dynamics of the group, the happiness of the researchers, and the interactions between them. You have to think that a research group will not only give you a bunch of workmates. You are going to spend a wild number of hours with them working in the lab, attending seminars, writing papers, going to workshops, travelling to conferences, etc. It is important to take a look at the researchers in the team you are considering joining. Almost every research group owns a web page with plenty of information so you can take a look at this.\nThe people in the group. How many researchers are in the group? How many are PhD candidates? How many are full professors? How many are postdocs? How long do people stay in that group? Do they only stay for their PhD and then leave? Do they continue with postdocs? Are they from different countries? Some research groups tend to gather researchers of the same nationality.\nThe ratio of PhD candidates per researcher. This is a particularly important factor. You have to consider that if you divide the number of PhD candidates by the number of researchers that can supervise them and you get a large number, this means that PhD candidates do not get the attention they deserve. In some countries, this ratio is regulated to avoid \u0026ldquo;PhD farming\u0026rdquo;.\nFunding. It is important to understand where the funding comes from. Universities do normally have their resources and open positions with their own money. Generally, this is not enough for the number of positions research groups aspire to open. Is here where funding becomes important for you as a PhD candidate. When a group receives funding for a project, state grant, or other sources they announce it. Check if this is the case, and how many funding sources you can find. The more diverse the sources, the better for the team.\nFormer members. What happened to the former members? Did they go to places you would like to be? Are they in other universities? You can do some research and decide if you like what you see. As a side note, I just found my former INRIA website and I can still see my name in the Cambridge Systems Research Group :D\nThe topic # During a PhD, you will have the opportunity to explore different ideas regarding a certain topic. Some of these ideas may become interesting, relevant or even successful. However, you will have to discard thousands of these ideas. This means that you will work iteratively on the same topic over and over. Sometimes you may wonder if somebody cares about all these ideas, or if it is worth the effort.\nYou must be aware that a PhD is the first step in a research career. Most of the ideas explored are contributions to a broader topic rather than ground-breaking discoveries. And many of those ideas may simply go stale over time, especially in STEM-related fields. You must love the topic. Purely. Your determination to spend time exploring ideas, reading about new findings and spending time on a topic must be guaranteed even before starting. If you are not sure about a topic proposed to work on, take your time and discuss potential alternatives. Put your ideas on the table and evaluate if you like what you see.\nFunding # This is not a petty matter. I have already mentioned that PhD candidates are not well-paid. Putting this aside, you must consider where is your funding coming from. Is the university paying your salary? For how long do you have funding? Three? Four years? What happens if you do not finish on time? I know cases that after four years of full funding had to survive on tuition fees and freelance jobs. In other cases, you can renew contracts year after year. If that is the case, be careful to be funded by a solvent entity. You would not be the first one with a funder gone bankrupt in the middle of your research.\nAbout the salary. Is that money enough for a living? Some universities (especially the top ones) are located in non-affordable cities with overpriced rentals. Get familiar with the quality of life you may have according to your salary. Especially if you are planning to land in a different country. Make numbers. Is that OK with your lifestyle? Read other people\u0026rsquo;s thoughts.\nSummary # Getting a PhD is a long journey. If you are considering applying for a PhD position, I think these are some of the questions you must answer before making this decision. Do not be dazzled by the aura of academia and the promises of joining an elite of highly talented minds. A PhD requires sacrifices not everybody is willing to take, independently of their intellectual skills, high marks, or brilliant ideas. Take your time, do your homework, and decide if this adventure is for you. If not, there are a million itineraries you can follow. If so, you enjoy the experience :)\nThanks for reading.\n","date":"20 June 2022","permalink":"/questions-before-starting-phd/","section":"","summary":"This post is dedicated to all of you who are thinking about pursuing a PhD. If you have ever applied for a job, you would probably know that you are expected to do your homework to maximize your opportunities and be sure that you are applying for the correct position.","title":"Questions you should ask before starting a PhD"},{"content":"","date":"20 June 2022","permalink":"/categories/science/","section":"Categories","summary":"","title":"science"},{"content":"","date":"20 June 2022","permalink":"/categories/stem/","section":"Categories","summary":"","title":"stem"},{"content":"","date":"29 May 2022","permalink":"/categories/books/","section":"Categories","summary":"","title":"books"},{"content":"Time flies by. That\u0026rsquo;s a known fact. It\u0026rsquo;s been more than a year since I published [Build Systems with Go]({% link _pages/2021-04-05-build-systems-with-go.md %}). I have already talked about [how does it feel to write a programming book]({% post_url 2021-04-05-writing-a-programming-book-in-2021 %}). What I have not talked about is the publication process itself. There is a bunch of platforms out there but in my case I only used Amazon\u0026rsquo;s Kindle Direct Publishing (KDP). There is a lot written about self-publication using KDP. These articles are mainly focused on explaining the steps to follow and do not go into some of the details and problems you may find.\nIf you\u0026rsquo;re still reading this I can anticipate you that my experience has been bittersweet. I will divide my thoughts into:\nPublishing User experience Analytics Amazon provides some tools to help publishers in the process and there are many freelancers and companies out there that can help you in the process. Publishing a book is a process that involves a lot of people: editors, designers, proofreaders, marketing, etc. However, I did everything myself and this may impact my vision of the process. Furthermore, I was a naive newbie in the self-publishing business :)\nPublishing # The main idea behind KDP is simple: you write a book, you upload it to the system, you choose the price, you make it available, Amazon\u0026rsquo;s customers buy your book, and Amazon will pay you your royalties after some discounts. Let\u0026rsquo;s remember that this service is intended for self-publication. This means that we are not going to print copies ourselves, and we delegate all the additional work (taxes, ISBN, hosting, etc.) to Amazon.\nLet me go with some basics. In KDP you have two basic formats: printed and e-book. Amazon offers you an on-demand printing service. That is cool. Somebody, somewhere, runs a huge printer, gets you a book, and sends it to you. Unfortunately, the service is not available in all the markets yet. For example, during the last year, the Australian Amazon market became available. In any case, this printing service covers most of the Amazon markets. In the case of e-books, everything is much easier; you get a copy of your e-book on your Kindle device or Amazon account. There is much more to say about Kindle devices, proprietary formats, who is really the owner of the book, etc., but I will not cover this for now.\nFor both formats (printed and e-book) the publishing process is fairly similar:\nFill some metadata: author, name of the book, book series, etc. Upload the content Chose a price Make it available Now let\u0026rsquo;s see the differences between both formats.\nPrinted version # If you have already written your book you must have some sort of file (PDF, MS Word, raw text?). In my case, I wrote the content using LaTeX. If you are not into LaTeX yet, you should read [this]({% post_url 2020-05-10-why-you-should-learn-latex-or-at-least-give-it-a-try %}). Additionally, I designed a front cover using my best (and limited) GIMP skills. With both, the PDF and your cover design you go through the upload process.\nThe KDP platform has to verify certain aspects of your PDF and book cover. Think that this has to be printed on a piece of paper that will be trimmed. This means that you must follow some standards in quality (image resolution) and sizes (margins, paper size, etc.). I know a lot of people struggle with these steps. You only have to take a look at the KDP help center to see how all the topics around this problem.\nI have to say that I had previous experience preparing books to be printed so I was aware of the issues I could find. This was really helpful to understand the steps to be carried out and imagine what the printed copy would look like. My main advice here is decide from the beginning what format (book dimensions) you are going to use. Notice that changing the final format will require a new formatting process. For example, this year a new hardcover format was made available but I have not used it because the current dimensions of my book are not valid for this format. I would have to maintain two different formats, review them, and format them (sigh). Another interesting fact is that other publishing platforms may not support your format. For example, my current format is not available at LeanPub at their standard rate (sigh again).\nE-book version # In the very beginning, I planned my book to be something that I could touch. Only when I started the publishing process I consider having an e-book version. Everything I read about self-publishing did strongly recommend publishing several formats to maximize your potential audience. It sounded reasonable.\nWhat is an e-book? Basically a collection of web pages. Your reader renders web content and sends it to a display. This means that if you have your book converted into HTML you can easily have an ebook right? If you find some irony in these words you are a keen reader. Let\u0026rsquo;s go by parts.\nThe most widely extended e-book format is epub supported by the W3C. It is open, easy to understand and seems to be well supported (or at least minimally supported) by a large number of e-readers. But guess what\u0026hellip; Amazon uses its own proprietary format for Kindle. However, they admit epub formatted books to be uploaded.\nNow let\u0026rsquo;s remember that my original manuscript was written using LaTeX. I imagined that I would find a tool to convert my manuscript to epub. I was right. Tex4ht could make the job for me. I am not going to say that it was easy\u0026hellip; There was a lot of stuff I had to learn, a good bunch of tweaks, visits to forums, modifying the default CSS, learning about the epub format\u0026hellip; And after a ridiculous number of hours, I managed to have what I considered a reflection of my printed book in this electronic format. Unfortunately, I was wrong.\nThe following snapshot is an excerpt from the printed version.\nAnd this is the same except extracted from the e-book.\nYou may find some differences there in terms of colours and fonts, but they are done on purpose. So far, so good. They look quite similar. The big thing here is that I am using Calibre, not an e-reader device. And this makes a huge difference. Why? Because we are using a much more sophisticated rendering solution. We have to be careful and check that our e-book can be correctly displayed in an e-book reader. Thank sounds almost impossible because there are a hundred devices out there. For KDP you can consider this problem to be smaller because everything is intended for the Kindle devices only.\nIn the case of KDP, after uploading the manuscript you can preview the content in your browser with a previsualization tool. And guess what\u0026hellip; It is going to render your e-book with the full potential of your browser. Many potential issues that may spoil your visualization are going to be automatically fixed by your browser. And in the worst case (and this is my case) the layout may fail at the destination target after spending quite a while revisiting everything. One solution is to send the e-book (in a mobi format, no e-pub, no kindle format) to your Kindle. You can also download it in HTML to be checked\u0026hellip; If I cannot be sure how my content is going to be displayed on the target device\u0026hellip; why am I not warned? And what is the purpose of this reviewing process?\nUser Experience # If you have attended any introductory programming course, you have been probably told to make a library management program. It is a good example of an application with a database. You have books, books have ISBNs, authors, these books are items in a database, items can be added and removed\u0026hellip; Well\u0026hellip; for some reason KDP is not like this and I find it difficult to understand why.\nLet\u0026rsquo;s say you have published an e-book and eventually you decide that you no longer want to sell this e-book. In this case, you can cancel the publication. Then you would probably remove it because it is no longer available for purchase. Good luck. You cannot do it. You heard correctly. You cannot remove an e-book that is not published. If you want to do such a thing you have to ask for human assistance\u0026hellip; Why? There must be a good reason for that but I cannot understand what reason.\nSomething different is printed books. Amazon assigns them an ISBN which is mandatory for printed books to be sold. In fact, they have to pay for that. I know is not particularly expensive if you buy bunches of these identifiers but it sounds reasonable to not remove a book if it has been assigned an id that should be available for everybody.\nWhy there is not a concept such as removed books? A label, a tag, something that can let authors organize their libraries. Other much more humble platforms use this approach. I do not think I am asking for something impossible.\nAnalytics # If you are one of those successful self-published writers who can live on their KDP sells you probably want to check how much money you make. KDP offers authors analytics describing how copies have been sold, how many pages of KDP unlimited have been read, and how much money will be made. Remember that from every final price you see there in the Amazon store you have to deduct some Amazon fees.\nIn general, I would say the analytics service is OK. It helps you to know how are you doing and in which markets you stand out.\nSome things would be particularly helpful. It would be nice to understand how users consume your content like on other platforms like YouTube. Those users coming from KindleUnlimited are monitored so it would be nice to know on what page they land, how much time they spend reading, if they quit soon, etc.\nConclusions # Self-publishing is not easy. Apart from polishing your content, you have to go through a large number of steps that require some experience. Existing platforms can indeed help authors to make their content available to large audiences at almost no cost. But it is also true, that the process is still painful even if you have previous experience. My general recommendation is to keep it simple and focus on the content.\nAs I mentioned at the beginning of this post I consider my experience using KDP to be bittersweet. My expectations of what you can get on an e-book exceeded reality. Or at least, what you can get in investing a reasonable amount of time. I guess that nobody expects exquisitely curated formats in their e-books.\nTo be honest, during the last year I have seen improvements in the KDP\u0026rsquo;s user experience and analytics platform. However, it can be remarkably frustrating to make a book available. And things can be even worse if you think about releasing new editions from time to time. I do strongly believe that there is a good number of features that are still missing in this field. Who knows maybe there is room for some entrepreneurship here.\nI hope you find something interesting or helpful in this post.\nThanks for reading :)\n","date":"29 May 2022","permalink":"/one-year-kdp/","section":"","summary":"Time flies by. That\u0026rsquo;s a known fact. It\u0026rsquo;s been more than a year since I published [Build Systems with Go]({% link _pages/2021-04-05-build-systems-with-go.md %}). I have already talked about [how does it feel to write a programming book]({% post_url 2021-04-05-writing-a-programming-book-in-2021 %}).","title":"Thoughts after one year using Amazon's KDP"},{"content":"This blog was born almost two years ago during the Covid-19 lockdown. At that time I decided to use a hosted WordPress solution. I have to say that I did not spend too much time checking alternatives. In this sense a hosted WordPress seemed to be a good idea. And actually, I think it is a good solution for many people. However, not for me. I have been postponing the decision of moving away from WordPress for a long time. Some reasons for this migration:\nI have no need to generate dynamic content. The management browser was painfully slow. I do not know if this was a specific problem of my WordPress provider, but moving from one menu to the other was terribly slow. For my use case (a humble blog) there was a lot of over-engineering. I got tired of continuous attacks. Obviously, I installed the corresponding security solutions. Anyway, I got daily notifications of IPs being banned after 100 attempts to find WordPress vulnerabilities. I had the terrible feeling of not being under the control of my own site. The number of available plugins and configuration options do simply multiply the chances of something to be wrong. These are the main reasons for this migration. Everything has been moved to GitHubPages. Static content, nothing particularly complex, and the hosting power of GitHub for free. I did my best migrating some of the old posts. You may find that some of them look weird, specially those with some math notation. Nothing that cannot be solved spending some time.\nI am still experimenting with the layout and I do not discard some changes in the next days.\nThanks for reading,\nP.S.: If you are interested, you can find the content used to generate this blog in my repo.\n","date":"18 March 2022","permalink":"/migration-wordpress-jekyll/","section":"","summary":"This blog was born almost two years ago during the Covid-19 lockdown. At that time I decided to use a hosted WordPress solution. I have to say that I did not spend too much time checking alternatives.","title":"Migration from Wordpress to Jekyll"},{"content":"","date":"22 June 2021","permalink":"/tags/books/","section":"Tags","summary":"","title":"books"},{"content":"","date":"22 June 2021","permalink":"/tags/go/","section":"Tags","summary":"","title":"go"},{"content":"","date":"22 June 2021","permalink":"/categories/go/","section":"Categories","summary":"","title":"go"},{"content":"","date":"22 June 2021","permalink":"/categories/golang/","section":"Categories","summary":"","title":"Golang"},{"content":"The Summer can be a perfect time to learn Go. Take your ebook to the beach and checkout all the examples and explanations from \u0026ldquo;Build Systems with Go: everything a Gopher must know\u0026rdquo;.\nBecause money does not grow on trees and to encourage you all, I have started a discount campaign at Amazon. You can find the ebook for 6.99$. The price will get closer to the original as Friday approaches.\nClick the image below to go to Amazon.\nHappy Summer!\nJust in case :) ( https://www.amazon.com/dp/B091FX4CZX)\n","date":"22 June 2021","permalink":"/no-excuses-to-learn-go-this-summer/","section":"","summary":"The Summer can be a perfect time to learn Go. Take your ebook to the beach and checkout all the examples and explanations from \u0026ldquo;Build Systems with Go: everything a Gopher must know\u0026rdquo;.","title":"No excuses to learn Go this Summer"},{"content":"","date":"22 June 2021","permalink":"/tags/programming/","section":"Tags","summary":"","title":"programming"},{"content":"","date":"27 May 2021","permalink":"/tags/golang/","section":"Tags","summary":"","title":"golang"},{"content":"You probably use a CLI on a daily basis. Depending on the solution the CLI works, the number of available options can be overwhelming. Because no visual recognition is available, we depend on the help to find the correct arguments. An appealing extension to CLIs is to provide users with shell completion. This simplifies the life of the user by displaying the expected arguments of the command without interruptions. This post explains how to get a shell completion CLI in Go using the Cobra library.\nThe Cobra library is a solution that well deserves your time if you are developing using Go. This post assumes you are familiar with this library and its main concepts (Commands, Flags, etc.). Shell completion is not provided by the Go program itself. Actually, this is done with a shell script generated by Cobra that captures users interaction with our CLI.\nYou can find the source code for these examples and more in the Build Systems with Go repo in the 15_cli folder.\nThe following CLI is a Cobra naive example that uses commands to print a message (hello or bye) in the console.\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;github.com/spf13/cobra\u0026#34; \u0026#34;os\u0026#34; ) var RootCmd = \u0026amp;cobra.Command{ Use: \u0026#34;say\u0026#34;, Long: \u0026#34;Root command\u0026#34;, } var HelloCmd = \u0026amp;cobra.Command{ Use: \u0026#34;hello\u0026#34;, Short: \u0026#34;Say hello\u0026#34;, Run: func(cmd *cobra.Command, args []string) { fmt.Println(\u0026#34;Hello!!!\u0026#34;) }, } var ByeCmd = \u0026amp;cobra.Command{ Use: \u0026#34;bye\u0026#34;, Short: \u0026#34;Say goodbye\u0026#34;, Run: func(cmd *cobra.Command, args []string) { fmt.Println(\u0026#34;Bye!!!\u0026#34;) }, } func init() { RootCmd.AddCommand(HelloCmd,ByeCmd) } func main() { if err := RootCmd.Execute(); err != nil { fmt.Fprintln(os.Stderr, err) os.Exit(1) } } Create a shell completion script # Cobra generates shell completion scripts for Bash, Zsh, Fish and PowerShell. The Command type offers methods with a signature like GetXXXCompletions to generate the corresponding completion script for the shell XXX. Normally, a completion command is added to the CLI to permit users to generate the corresponding completion script for their shells. When the script is loaded into the shell, pressing the tab key twice displays the valid commands and the help. The example below shows a possible implementation of a completion command using the root command (check the documentation for additional details).\npackage cmd import ( \u0026#34;github.com/spf13/cobra\u0026#34; \u0026#34;os\u0026#34; ) var CompletionCmd = \u0026amp;cobra.Command{ Use: \u0026#34;completion [bash|zsh|fish|powershell]\u0026#34;, Short: \u0026#34;Generate completion script\u0026#34;, Long: \u0026#34;To load completions\u0026#34;, DisableFlagsInUseLine: true, ValidArgs: []string{\u0026#34;bash\u0026#34;, \u0026#34;zsh\u0026#34;, \u0026#34;fish\u0026#34;, \u0026#34;powershell\u0026#34;}, Args: cobra.ExactValidArgs(1), Run: func(cmd *cobra.Command, args []string) { switch args[0] { case \u0026#34;bash\u0026#34;: cmd.Root().GenBashCompletion(os.Stdout) case \u0026#34;zsh\u0026#34;: cmd.Root().GenZshCompletion(os.Stdout) case \u0026#34;fish\u0026#34;: cmd.Root().GenFishCompletion(os.Stdout, true) case \u0026#34;powershell\u0026#34;: cmd.Root().GenPowerShellCompletionWithDesc(os.Stdout) } }, } If we add this command to our previous code using\nRootCmd.AddCommand(HelloCmd,ByeCmd,CompletionCommand) we can generate and load the shell completion script for Bash as follows.\n\u0026gt;\u0026gt;\u0026gt; ./say completion bash \u0026gt; /tmp/completion \u0026gt;\u0026gt;\u0026gt; source /tmp/completion Now we have that pressing the tab key twice displays the following completions:\n\u0026gt;\u0026gt;\u0026gt; ./say [tab][tab] bye -- Say goodbye completion -- Generate completion script hello -- Say hello help -- Help about any command Command arguments can be displayed for additional help. A list of valid arguments can be provided with the ValidArgs field of the Command type. Our completion command has already filled this field showing the following list.\n\u0026gt;\u0026gt; ./say completion [tab][tab] bash fish powershell zsh Get command arguments at runtime # In some scenarios, the arguments of a command can only be determined at runtime. For example, assume we have an application that queries the information of a certain user in a database using her identifier. The user id is only a valid argument if it exists in the database. For these scenarios, the list of valid arguments can be defined using a function in the field ValidArgsFunction. The following example emulates the availability of different users with a random selector in the UserGet function. The ShellCompDirective is a binary flag used to modify the shell behaviour. Check the documentation for more information about this flag and what it does.\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;github.com/juanmanuel-tirado/savetheworldwithgo/15_cli/cobra/advanced/example_05/cmd\u0026#34; \u0026#34;github.com/spf13/cobra\u0026#34; \u0026#34;os\u0026#34; \u0026#34;math/rand\u0026#34; \u0026#34;time\u0026#34; ) var RootCmd = \u0026amp;cobra.Command{ Use: \u0026#34;db\u0026#34;, Long: \u0026#34;Root command\u0026#34;, } var GetCmd = \u0026amp;cobra.Command{ Use: \u0026#34;get\u0026#34;, Short: \u0026#34;Get user data\u0026#34;, Args: cobra.ExactValidArgs(1), Run: func(cmd *cobra.Command, args []string) { fmt.Printf(\u0026#34;Get user %s!!!\\n\u0026#34;,args[0]) }, ValidArgsFunction: UserGet, } func UserGet (cmd *cobra.Command, args []string, toComplete string) ([]string, cobra.ShellCompDirective) { rand.Seed(time.Now().UnixNano()) if rand.Int() % 2 == 0 { return []string{\u0026#34;John\u0026#34;, \u0026#34;Mary\u0026#34;}, cobra.ShellCompDirectiveNoFileComp } return []string{\u0026#34;Ernest\u0026#34;, \u0026#34;Rick\u0026#34;, \u0026#34;Mary\u0026#34;}, cobra.ShellCompDirectiveNoFileComp } func init() { RootCmd.AddCommand(GetCmd, cmd.CompletionCmd) } func main() { if err := RootCmd.Execute(); err != nil { fmt.Fprintln(os.Stderr, err) os.Exit(1) } } After generating and loading the shell completion script, the completion dynamically suggests user ids with the UserGet function as shown below.\n\u0026gt;\u0026gt;\u0026gt; ./db get [tab][tab] John Mary \u0026gt;\u0026gt;\u0026gt; ./db [tab][tab] completion -- Generate completion script get -- Get user data help -- Help about any command \u0026gt;\u0026gt;\u0026gt; ./db get [tab][tab] Ernest Mary Rick As you can see generating a shell completion script is pretty straight forward using Cobra. I hope you consider adding shell completion in your next project.\nThanks for reading.\n","date":"27 May 2021","permalink":"/shell-completion-with-cobra-and-go/","section":"","summary":"You probably use a CLI on a daily basis. Depending on the solution the CLI works, the number of available options can be overwhelming. Because no visual recognition is available, we depend on the help to find the correct arguments.","title":"Shell completion with Cobra and Go"},{"content":"","date":"27 May 2021","permalink":"/tags/tips/","section":"Tags","summary":"","title":"tips"},{"content":"Build Systems with Go has exceeded all my expectations. Actually, I started the book with no expectations or predefined goals.\nIt has been for one month the number one in the Amazon new releases in the category of distributed systems. And it is in the third position of best sellers for the same category at the moment of writing this post. A tremendous achievement for a self-publishing book. Thank you all for your support and nice comments. I hope the content of this book can help you.\nA new edition versioned v0.2.0 has already been published!!! This new version comes with enhanced descriptions, revisited explanations, extra figures, a new chapter about Cgo, and more awesome gophers. Additional code is available in the repo under tag v0.2.0.\nCheck the book page for more details.\nWhy there is no version v1.0.0? Why semantic versioning for a book? # When Donald Knuth published the first volume of The Art of Computer Programming back in 1962 he started the largest foundational collection of programming books ever written. Actually, the book is considered to be unfinished after several volumes published since 1962. Volume 5 is planned to be released in 2025.\nFor me reaching version v1.0.0 would mean that I have reached a perfect book with all the topics I would like to include. I do not think that is possible, at least for now. Is for this reason that you will not probably see this version :)\n","date":"14 May 2021","permalink":"/build-systems-with-go-v0-2-0-is-here/","section":"","summary":"Build Systems with Go has exceeded all my expectations. Actually, I started the book with no expectations or predefined goals.\nIt has been for one month the number one in the Amazon new releases in the category of distributed systems.","title":"Build Systems with Go v0.2.0 is here!!!"},{"content":"","date":"13 May 2021","permalink":"/categories/dark-theme/","section":"Categories","summary":"","title":"dark-theme"},{"content":"","date":"13 May 2021","permalink":"/categories/literature/","section":"Categories","summary":"","title":"literature"},{"content":"This post is a continuation of a previous one where I explained some aspects of my experience writing and self-publishing a programming book. Well, yesterday the blog multiplied its traffic by 100,000. I thought the server was under attack. Someone sent the link to Hacker News and the traffic started flowing. Thanks to all those who took their time to read the post. There was a good number of comments about self-publishing, programming books, publication-quality, editors, etc. Some of the comments were written by other published authors and shared their experiences and concerns. Comments were respectful, something I do really appreciate in these times of trolls and bullying.\nMy original post was not intended to be a profound discussion about the aspects of self-publishing. There is already a lot said about this topic and I only wanted to exchange part of my experience. It was published a month ago and now that I read it over I can confirm it only overviews part of my experiences in the self-publishing journey. I write this second part to comment on some of the ideas and concerns exposed in the mentioned comments.\nTL;DR\nWriting a book gives you credibility # Someone mentioned that nowadays adding an extra line to your CV saying you wrote a book cannot give you any credibility. I do agree. Then why I said that? Simple. If you see in my CV that I worked at Google that gives me more credibility? Maybe. Or maybe I was invited to leave after two months. If I affirm to program in C/C++, Python, and Java do I have more credibility than someone who only does Python? No. Additional checking has to be done.\nI understand this comment. Writing a book per se cannot boost your credibility. Any line in your CV should give you more credibility without context or some additional checking. I have seen CVs where candidates affirmed to have been researchers in top universities. They simply attended seminars without any real collaboration.\nYour credibility cannot boost only because you wrote a book. Is the book itself and the content what can boost your credibility, not the fact of writing a book.\nSelf-publishing is the new vanity-press # I do agree. There is some vanity in every publication. That is why researchers fight for the order their names appear in the list of authors. Holding a paper or book in your hands with your name in it is always a pleasant experience. Something similar happens with blogs. Probably this blog itself is proof of my vanity. Think about Twitter, the most devastating example of self-published vanity press.\nThe main difference could be on the value or utility of what you have to say. However, I do not think books are sold because of their utility, quality or publishing house. And this applies to well-recognized publishing houses. We all have bought a book about a topic we thought it would be helpful and two days after it was in the trash can. At least the author boost her vanity.\nA good editor makes the difference # Absolutely. A good editor can be the difference between a poor, a decent, and an extraordinary book. The editor must show enthusiasm about what you are writing. This is easy to know. As soon as you get comments, reasonable comments I mean, you can expect a fruitful collaboration. A good editor will tell you things you will not like hearing just to ensure quality.\nFrom the comments, I get there is a general concern about pirate editors. A pirate editor is looking for money and does not care about the outcome. Some self-publishing houses charge authors to pay part of the expenses of the editorial process. They do not expect to sell books because the author already pays for everything. Theoretically, they provide you with reviewers, proofreaders, etc. Unfortunately, reviewers and proofreaders do not usually do their job ending in the same outcome you would have got doing everything on your own. If you have doubts just check Google before signing any contract.\nTechnical writers do not know what they write about # I think this is linked with the dichotomy between quality and quantity. Editors have a lot of pressure for making money. They have to look around for new topics to make people buying books. Think that the first book explaining that buzzy concept you read on the Wire magazine can make good money. This forces editors to look for authors to finish their work in a short time under tons of pressure. You cannot expect an impressive result.\nTechnical authors looking for profits # If you published a book (fiction or not) and you think you will live on the royalties you have a problem. Profitability is the last word that should come to your mind when thinking of publishing a book. It is a matter of statistics. Do you think all the authors you may find selling books on Amazon can make a living out of that? If you have published a technical book and things go extraordinarily well you cannot quit your job. Even the same for fiction books. However, in this case, you may be lucky and get awesome royalties for merchandising, movies, TV series, etc.\nEditors, publishing houses, Amazon, and other platforms steal your money # A long topic I do not want to discuss in detail. In my case, I only published with Amazon KDP. I have no experience in other platforms and editors so I do not have an opinion about that. Indeed, the amount of money you get from every book on Amazon varies. For ebooks, you can get up to 70% of the final price if you fulfil some requirements. If not you only get 35%. For printed books you get 60% after discounting the printing costs. Are these percentages appealing? Or is this a tyrannic tax imposed by Amazon? If you consider that you had to go through the whole publishing process on your own this might be excessive.\nAbout eager editors\u0026hellip; Well, editors have to be paid. I mean, editing a book is a job and it is not an easy one. If you consider your editor is charging you too much, you must not go with that. It is impossible to work with someone you think is taking something from your pocket.\nRecent language adopters are more empathetic # I do agree. For early adopters, it is easier to point out those pieces of technology that generate friction during the learning process. If you have recently learnt a programming language you have gone through a process that changed your mindset. Even when you use several languages daily, you have to change the way you think depending on the used language. Early adopters are more conscious of the particularities of a programming language. When you spent 10 years programming in Java you probably are no longer aware of its verbosity. Experienced developers understand (or should understand) the motivation behind certain aspects of a language. And what is more interesting, they get more insights about what makes a language particularly suitable for certain tasks. Although, this can only be achieved if you learn new languages from time to time so you can compare.\nLanguages are learnt through examples # I do agree. Children learn by imitation. They watch their parents, they listen to how they talk and then repeat what they hear. Programming languages are the same. Developers must learn to read code. In particular, others code. The more examples you have available, the easier will be for you to adopt that new language. Once you learn the foundations you can go deep into the particularities of that language.\nWrite for an audience # In my previous post, I wrote this quote attributed to Einstein \u0026ldquo;You don\u0026rsquo;t really understand something unless you can explain it to your grandma\u0026rdquo;. I find it surprising how people reacted to this quote talking about my paternalistic style and suggesting its removal ¿? I think this an example of how quotes are taking out of context, or simply assumed to be literal (an example of postmodernism?). I am not sure if Einstein really said that. What I can confirm is that you fully understand a topic if you can elaborate an explanation from scratch independently of your audience. And I think that is what this quote is all about.\nIf you are an expert in quantum computing but you can only talk to your lab colleagues about the topic I would not say you fully understand the topic. Why? Because one of the most difficult tasks is to develop the empathy needed to expose complex ideas to different audiences. And this includes understanding all the derivatives of a complex topic. This is why the best mathematician may not be the best teacher for six-year-old children. Someone will say that these children do not need to know how Perelman demonstrated Pointcaré\u0026rsquo;s conjecture. Of course not. But I am sure they would enjoy, some nice plot figures, an introduction to geometry, Perelman\u0026rsquo;s story\u0026hellip; That is your topic adapted to your audience.\nThanks for reading.\n","date":"13 May 2021","permalink":"/writing-a-programming-book-in-2021-part-ii/","section":"","summary":"This post is a continuation of a previous one where I explained some aspects of my experience writing and self-publishing a programming book. Well, yesterday the blog multiplied its traffic by 100,000.","title":"Writing a Programming Book in 2021 (part II)"},{"content":"","date":"16 April 2021","permalink":"/tags/kafka/","section":"Tags","summary":"","title":"kafka"},{"content":"","date":"16 April 2021","permalink":"/tags/software/","section":"Tags","summary":"","title":"software"},{"content":"","date":"16 April 2021","permalink":"/categories/systems/","section":"Categories","summary":"","title":"Systems"},{"content":"This post is an adaption of a fragment extracted from my book Build systems with Go: everything a Gopher must know\u0026quot;. You can find more details here.\nApache Kafka has demonstrated to be a scalable solution and it is widely adopted as a publish/subscribe event streaming platform. The ecosystem around Kafka is vast and rich and you have probably used one of its many solutions.\nKafka uses its own protocol over TCP that can be implemented in any programming language. In the case of Go, there are some available implementations. The official one provided by Confluent is a wrapper around the librdkafka library. Others like the one offered by Segmentio are Go native and do not require any additional libraries.\nThe utilization of wrappers around libraries may not be the most convenient solution for all scenarios, particularly if you fully trust Go. In the case of native implementations, not all the operations offered by the official Kafka driver are available. You can always implement your driver following the specification mentioned above\u0026hellip; But be honest. We all know you are not going to do that.\nA potential solution is to use the REST API proxy offered by Kafka ( here is the API documentation). In this post, I will show you how to implement a producer and a consumer using this API. To do so, I will only use the Go standard net/http package.\nThese examples assume the Kafka REST Proxy is running and listening to localhost:8082. You can find some help on how to install a local Kafka using a Docker compose in the official guide or you can check my repo for additional help. The code I will show here can be found there.\nProducer # First, we have to know the REST method for message publishing. After checking the API specification, we find out that we have to use the POST method /topics/\u0026lt;name\u0026gt;/partitions/\u0026lt;number\u0026gt;. Where \u0026lt;name\u0026gt; and \u0026lt;number\u0026gt; are the topic name and the corresponding partition respectively. The message must be attached to the body request. This method expects the encoding format to be specified in the request Content-Type header. The API admits data encoding in JSON, Avro, binary, and Protobuf formats. In these examples, we use JSON encoding with the value application/vnd.kafka.json.v2+json for our header. On how to specify other formats see the official documentation. The following object contains two messages (records) to be published.\n{ \u0026#34;records\u0026#34;: [ {\u0026#34;value\u0026#34;:{\u0026#34;name\u0026#34;:\u0026#34;John\u0026#34;,\u0026#34;email\u0026#34;:\u0026#34;john@gmail.com\u0026#34;}}, {\u0026#34;value\u0026#34;:{\u0026#34;name\u0026#34;:\u0026#34;Mary\u0026#34;,\u0026#34;email\u0026#34;:\u0026#34;mary@email.com\u0026#34;}} ] } Note that for simplicity we are only indicating the content of our messages, but other fields such as the key or the timestamp can be set. Check the reference for more details.\nNow, we can send this object in the body of a POST request to the API using the code below.\nThe BuildBody function is a helper to generate the body of the request. Note that we set the corresponding Content-Type header with the CONTENT_TYPE constant to the Post function. Finally, the program prints the body from the server response. If we run this code, we can see that the body of the server response contains information about the partitions and offsets of every written message. Notice that these values will vary according to your topic status.\n{\u0026#34;records\u0026#34;: [{\u0026#34;value\u0026#34;:{\u0026#34;name\u0026#34;:\u0026#34;John\u0026#34;,\u0026#34;email\u0026#34;:\u0026#34;john@gmail.com\u0026#34;}},{\u0026#34;value\u0026#34;:{\u0026#34;name\u0026#34;:\u0026#34;Mary\u0026#34;,\u0026#34;email\u0026#34;:\u0026#34;mary@email.com\u0026#34;}}]} 200 OK {\u0026#34;offsets\u0026#34;:[{\u0026#34;partition\u0026#34;:0,\u0026#34;offset\u0026#34;:165,\u0026#34;error_code\u0026#34;:null,\u0026#34;error\u0026#34;:null},{\u0026#34;partition\u0026#34;:0,\u0026#34;offset\u0026#34;:166,\u0026#34;error_code\u0026#34;:null,\u0026#34;error\u0026#34;:null}],\u0026#34;key_schema_id\u0026#34;:null,\u0026#34;value_schema_id\u0026#34;:null} Consumer # Consuming messages from the API REST requires more steps than producing them. To get a message a consumer must: 1) create a new consumer instance, 2) subscribe to a topic and group, 3) fetch the messages, and finally 4) delete the instance if no more messages are going to be consumed. The program is split into four pieces corresponding to these steps.\n![Kafka consumer API REST communication diagram]({{ source }}/assets/2021/04/kafkadiagram-300x137.png \u0026ldquo;Kafka consumer API REST communication diagram\u0026rdquo;)\nTo create a new consumer, we use the POST method /consumers/testGroup where testGroup is the name of the consumers\u0026rsquo; group. The body request contains the name of the consumer (testConsumer) and the format to be used by the messages (json).\n{\u0026#34;name\u0026#34;:\u0026#34;testConsumer\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;json\u0026#34;} The response body contains the consumer id and the base URI to be used by this consumer. This can be achieved with the following piece of code. The DoHelper is a helper function for the POST requests.\nNext, the consumer is subscribed to the topic helloTopic where the producer sent the messages. The target POST method matches the base URI received in the response from the previous creation of the consumer instance extended with the suffix subscription. In our case, /consumers/testGroup/instances/testConsumer/subscription. The answer\u0026rsquo;s body contains the list of topics this consumer requested to be subscribed to. The response returns a 204 code indicating a correct response without a body.\nNow the consumer is ready to receive records from the topics it has been subscribed to. A GET request to the base URI with the records suffix will return any available record. In this case, /consumers/testGroup/instances/testConsumer/records. Note that the Accept header must be set with the corresponding content type we want the incoming messages to be encoded (JSON in this case). The response body will contain the available messages. if no messages are available at the time of sending the request, the returned response will be empty. Additional query parameters are timeout to specify the maximum time the server will spend fetching records and max_bytes with the maximum size of the returned records.\nFinally, we delete the consumer instance to release resources with a POST request to /consumers/testGroup/instances/testConsumer. The body from the response is empty with a 204 status.\nBy running the program we can see the requests, responses, and received bodies during the process.\n--\u0026gt;Call http://localhost:8082/consumers/testGroup --\u0026gt;Body {\u0026#34;name\u0026#34;:\u0026#34;testConsumer\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;json\u0026#34;} \u0026lt;--Response 200 OK \u0026lt;--Body {\u0026#34;instance_id\u0026#34;:\u0026#34;testConsumer\u0026#34;,\u0026#34;base_uri\u0026#34;:\u0026#34;http://rest-proxy:8082/consumers/testGroup/instances/testConsumer\u0026#34;} --\u0026gt;Call http://localhost:8082/consumers/testGroup/instances/testConsumer/subscription --\u0026gt;Body {\u0026#34;topics\u0026#34;:[\u0026#34;helloTopic\u0026#34;]} \u0026lt;--Response 204 No Content --\u0026gt;Call http://localhost:8082/consumers/testGroup/instances/testConsumer/records \u0026lt;--Response 200 OK \u0026lt;--Body [{\u0026#34;topic\u0026#34;:\u0026#34;helloTopic\u0026#34;,\u0026#34;key\u0026#34;:null,\u0026#34;value\u0026#34;:{\u0026#34;name\u0026#34;:\u0026#34;John\u0026#34;,\u0026#34;email\u0026#34;:\u0026#34;john@gmail.com\u0026#34;},\u0026#34;partition\u0026#34;:0,\u0026#34;offset\u0026#34;:179},{\u0026#34;topic\u0026#34;:\u0026#34;helloTopic\u0026#34;,\u0026#34;key\u0026#34;:null,\u0026#34;value\u0026#34;:{\u0026#34;name\u0026#34;:\u0026#34;Mary\u0026#34;,\u0026#34;email\u0026#34;:\u0026#34;mary@email.com\u0026#34;},\u0026#34;partition\u0026#34;:0,\u0026#34;offset\u0026#34;:180}] --\u0026gt;Call http://localhost:8082/consumers/testGroup/instances/testConsumer \u0026lt;--Response 204 No Content Note that in this case, we are not modifying the consumer offset. Calling the POST method at /consumers/testGroup/instances/testConsumer/positions with the next offset indicated in the body prepares the consumer for the next batch of messages. For our example where the latest record had offset 180, we could send the following body to set the next offset to 181.\n{ \u0026#34;offsets\u0026#34;: [ { \u0026#34;topic\u0026#34;: \u0026#34;helloTopic\u0026#34;, \u0026#34;partition\u0026#34;: 0, \u0026#34;offset\u0026#34;: 181 } ] } Summary # In certain situations, it is not possible nor convenient to use Kafka drivers. In these situations, the Kafka API REST Proxy can be a good solution. In this case, I have shown the steps to implement a producer and consumer in Go using the standard net/http package. I hope you find it useful.\nThanks for reading.\n","date":"16 April 2021","permalink":"/using-the-kafka-rest-api-with-go/","section":"","summary":"This post is an adaption of a fragment extracted from my book Build systems with Go: everything a Gopher must know\u0026quot;. You can find more details here.\nApache Kafka has demonstrated to be a scalable solution and it is widely adopted as a publish/subscribe event streaming platform.","title":"Using the Kafka REST API with Go"},{"content":"","date":"5 April 2021","permalink":"/tags/systems/","section":"Tags","summary":"","title":"systems"},{"content":"It is said that there are three things you must do in your lifetime: plant a tree, have a child, and write a book. I recently finished the last one. Nevertheless, I wrote a programming book. A programming book in the 21st century with all those wonderful online resources you can find for free? Yes. Are you some sort of romantic fool? Maybe. Let serve these lines to describe my experience and give you some do\u0026rsquo;s and dont\u0026rsquo;s that can save your time.\nWhy a book? # Now we have many available formats to broadcast any educational material. They are handy and easy to use. We have platforms where everybody can develop content like YouTube. Or platforms such as Coursera where universities or recognized education institutions can publish their courses and materials. Then, why a book?\nSome reasons I find convincing for writing a book.\nWriting well improves communication It recently became a trending topic how Jeff Bezos banned PowerPoints in favor of six-page memos. The richest man on Earth asking his employees to elaborate his ideas instead of writing a bunch of bullets. A craving? I don\u0026rsquo;t think so. Only if you can elaborate your ideas using your language, you can properly disseminate them. This is something we have forgotten because of our laziness in writing.\nYou don\u0026rsquo;t really understand something unless you can explain it to your grandma This is something Einstein said. He was right. I already knew all the concepts I explain in the book. However, I have to say that explaining them was not so easy. I had to revisit some of them, write and rewrite many examples until I was happy with the given explanation. I can say this exercise has improved some of my programming skills.\nReading is a good exercise for your brain The expansion of video on-demand platforms, in particular YouTube, has made easily available a large number of educational resources. I use it, you use it, we all use it. However, it has been demonstrated that reading is more efficient and brings great benefits to your brain. This doesn\u0026rsquo;t mean you shouldn\u0026rsquo;t use online video platforms. I find them a very valuable complement.\nMake a valuable contribution By making a valuable contribution I don\u0026rsquo;t mean my book to be the next Hamlet or a ground breaking best-seller. Not at all. What I expect is to help someone, somewhere to fix, solve, improve or advance in his life or career. That should be the ultimate goal.\nCredibility You are going to spend a while collecting, redacting, and formatting ideas. Finally, you will put your name there. If you have made a good or bad job your name will be attached to it. This is something that boosts your credibility.\nYou need almost no resources Writing is the cheapest dissemination activity. No cameras, no microphones only a computer and your favourite text processor.\nSelf-publishing or looking for a publisher? # If you write a book, the next step is to look for a publisher. Someone who will publish your book, pay for the production costs, marketing, proofreaders, marketing, etc. Sometimes publishers contact you looking for a book about a specific topic. In any case, the publisher acts as a quality filter and brings some resources that can be difficult (or expensive) to be obtained by the author. We all know the stories of great authors getting rejected manuscripts one after the other. Well, this is not my case. I wrote a programming book. Of course, we all know there are publishing houses with a long catalogue of programming languages. And they do a good job.\nSelf-publishing has always been an option for authors. Some publishing houses offer you resources to publish your work. You simply pay them and they review and print your book. It seems to be a fair arrangement. However, after five minutes in Google, you will find out that many self-published authors had terrible experiences or were scammed. Finding a decent and professional publishing house requires time. You cannot trust the first one you find in Google.\nOther platforms try to digitalize the process and help you reach a larger audience. Probably the most popular is Kindle Publishing by Amazon, but there are others. You write your book, prepare a cover, upload it, and make it available to the readers. It sounds awesome, and actually, it is. However, you have to be your own publisher which means that you must deal with the layout, cover, proof-reading, title, marketing, and all the tasks an editor is supposed to do. I can tell you this can be an overwhelming task if you want to reach a certain level of quality.\nI chose self-publishing. Why? Well, I wanted to go through all the journey from writing the content to the cover design, and I did not feel in the mood of looking for a publisher. I knew it would be quite a ride, and it was. I was particularly ambitious, especially with the book layout, but this is something I will explain in another post. Then comes the marketing and all that\u0026hellip;\nMy recommendations # There is a good bunch of resources on the Internet about self-publishing so if you find you had enough you can skip this. However, I would like to share some of my thoughts with you.\nStay to your table of contents A book has a beginning and an ending. Remember that. Write down your table of contents before you start writing. Spend your time defining the contents, they must make sense in your head. Remember, a book tells a story, and a programming book does it too. It is tempting to start writing. Don\u0026rsquo;t do it. You will regret it.\nStructure your contents A programming book is particularly demanding in terms of contents. Inside a chapter, you will have text, code, figures, tables, formulas, plots, etc. Think about everything you will need and prepare a template. Will you refer to your images as figures or pictures? Is your code going to be formatted or highlighted? Do not move forward until you have a well-defined structure.\nSet a writing routine This is really important. This is not about intensity, it is about consistency. If you stop writing for a long period, this can be catastrophic. If you stop writing, this must be scheduled. Maybe you can use this break to read your work so far. And this leads me to the next point.\nRead, read, and read again You will be your first reader. This means that you will be the first person to find typos, errors, and inconsistencies. The only way to find them is to read. If you feel stuck, you can simply read over that section you wrote weeks ago. You will probably find errors or a different perspective.\nYou don\u0026rsquo;t judge a book by its cover, but everybody do Let\u0026rsquo;s be honest. A book with a good cover is a million times more appealing than that title in Comic Sans. This is even more important if nobody knows who you are.\nBe humble When writing a technical book you must accept something: you are not the one who knows the most. Maybe you have created a new technology, but even in that case, I\u0026rsquo;m sure that someone may find an error or a misleading explanation.\nEnjoy the ride When someone pays you for your work there is a wonderful sensation of self-realisation. Be thankful to those people who trusted you. Remember that this is only the final step. Every step during the writing process brings you a bit of wisdom that improves you as a person and professional.\nBe ready for new editions Technical books are never finished. The book has just been published and I\u0026rsquo;m thinking about improvements, extensions, and new sections. Self-publication is extremely flexible and helps to release new editions.\nSummary # If you have reached a certain proficiency in any topic, art or science, a book is a wonderful channel to disseminate and share your knowledge. Be bold, make your index, and start writing. Writing will help you improve your communication skills. There are a million guidelines for authors to improve their skills. The bullets I presented here are my humble contribution from my experience self-publishing a programming book.\nThanks for reading.\n","date":"5 April 2021","permalink":"/writing-a-programming-book-in-2021/","section":"","summary":"It is said that there are three things you must do in your lifetime: plant a tree, have a child, and write a book. I recently finished the last one. Nevertheless, I wrote a programming book.","title":"Writing a Programming Book in 2021"},{"content":" Why this book? # Years have passed by and Go is no longer the new kid on the block, it has already become a mature language surrounded by a rich ecosystem of libraries, projects, and tools. Talking about Go is no longer talking about that fancy language that makes your life easier. Go is the gravity centre of a continuously growing ecosystem of amazing solutions maintained by a devoted community of developers.\nGo was originally designed to simplify the building of complex systems. However, when a developer decides to learn Go most of the learning resources simply explain the language. This book goes one step further by exploring tools, libraries, and projects from the Go ecosystem you can use to build ready-for-production systems. Everything a gopher must know in a single book.\nWho should read this book? # This book is oriented to new Go adopters and developers with programming experience in other languages. The first part of this book covers the Go language from its basics to more advanced concepts. The second part explores libraries, projects and tools from the Go ecosystem to build awesome systems. If you are new to Go you can start from the very beginning. However, if you have some experience you can start with the second part and revisit any basic concept if needed. Or you can simply go and check those chapters you find the most relevant.\nStructure of this book # This book is structured to easily find those pieces you find more interesting for your work. However, if you are an absolute beginner or you do not feel very comfortable with all the concepts explained in this book you can always start from the beginning. Whatever your use case is, these are the contents of every Chapter.\nPart I: The Go language First steps with Go The basics Arrays, slices, and maps Structs, methods, and interfaces Reflection Concurrency Input/Output Encodings HTTP Templates Testing Modules and documentation Cgo Part II: Building systems Protocol buffers gRPC Logging with Zerolog Command Line Interface Relational databases NoSQL databases Kafka The code # The book contains more than 200 executable examples with their expected outputs to help the reader to understanding the explained concepts. All the code is available at the GitHub repository under Apache License:\nhttps://github.com/juanmanuel-tirado/savetheworldwithgo\nFeel free to fork the repository at your convenience. If you find any issue or have any comment regarding the code, please let the author know.\nWhere can I find the book? # You can purchase the book at Amazon in paperback or ebook formats here\n","date":"5 April 2021","permalink":"/build-systems-with-go/","section":"jmtirado.net","summary":"Why this book? # Years have passed by and Go is no longer the new kid on the block, it has already become a mature language surrounded by a rich ecosystem of libraries, projects, and tools.","title":"BUILD SYSTEMS with GO"},{"content":"Cyberpunk 2077 is probably the most awaited game from this pandemic 2020. A lot of expectation has been around this game since it was first announced back in 2012. In 2018 it was the sensation during the E3 (the most important video games fair in the world) winning several awards. The next year, in 2019 many lines were written discussing the pros and cons of this game. Initially, the game was announced to be released in April 2020 unfortunately, the Covid-19 delayed the release. Finally, the game was released on December 10th with a huge number of sales, hundreds of thousands of fans around the world playing like maniacs, and successful reviews from specialized media. However, fans are asking for reimbursement. Why?\nAn ambitious project with hilarious bugs # CD Projekt Red, creators of the worldwide acclaimed The Witcher 3 based on the books from Andrzej Sapkowski, are the authors of this huge project. More than 500 people worked in this game. The studio received 7 million dollars from the Polish government for additional hiring, converting this project into a demonstration of their technical superiority as a nation. The team worked with two scripts, the main one in Polish and its translation into English. Urban planners were employed to design Night City, the main scenario for the game. Finally, a tremendous marketing campaign was carried out including the film star Keanu Reeves as one of the characters in the game.\nEverything in this project is amazing: the story, the scenarios, the graphics, the number of employed resources, the technology specifically designed for this game. It is difficult to not surrender to the overwhelming quality of this work. If everything is so spectacular why has Sony decided to reimburse Cyberpunk 2077 buyers? The answer is three words length: BUGS, BUGS, BUGS.\nWalking around Night City is an amazing experience. Photorealistic buildings, a futuristic skyline, and one of the best gaming mechanics up-to-date. The experience is surprisingly immersive until you see a dead body falling from the skies with no reason, or cars hitting one after the other against the wall, or a pedestrian eating an imaginary sandwich because graphics rendering fails. These are just some of the many bugs you can find. See the video below to understand what I am talking about.\nHow is it possible that such a huge project, with such a huge budget, and all those eager players ready to have blisters in their fingers after hours of restless playing has these bugs? No matter how quickly hotfixes are released, users find more and more hilarious situations. Where you were expecting an experience between Blade Runner and Ghosts in the Shell, you find a surrealistic Monty Python sketch. It is funny that is for sure, but not what you were paying for.\nAll these problems come from the technical side. The game was originally intended to offer all its capabilities in the new PlayStation 5. However, extending the game to other platforms seemed to impose technical limitations. We have to remember that the high technical specs required by this game are not available for all platforms. There is a bunch of technical solutions that implies reducing FPS, using lower resolution textures, and other solutions that reduce graphics quality. Unfortunately, in this case not only graphical performance was reduced, but the game mechanics also resulted in some non-sense festival of NPCs (non-player characters) defying the law of physics and interacting with other invisible NPCs in some sort of surrealistic ritual.\nThe main question is clear. Did somebody test this game? And by testing, I mean testing it exhaustively. These are not the kind of bugs that only 0.5% of players may find. These bugs are there for everybody. I would like to think that there was some testing done. This release was probably forced by other factors such as marketing pressure, financial planning, or simply the incoming Xmas season.\nE.T. the Extra-Terrestrial and the videogame crash. # In 1982 Atari released a videogame based on Steven Spielberg\u0026rsquo;s E. T. film. The film became a classic instantaneously, and the videogame promised good sales in the recently born videogames market. The game was designed as an adventure game where E. T. has to find the pieces of a phone to call home. You can see gameplay in the video below.\nAs you can see the gameplay is confusing, it is impossible to know what is going on, and the graphics do not help. This is users\u0026rsquo; frustration flying up like a rocket. This game has always been mentioned to be one of the worst videogames in history. It actually, made Atari lose 536 million dollars and was one of the reasons that forced Atari to disappear back in 1984. An urban legend said that Atari buried all the game cartridges somewhere in New Mexico. Surprisingly, in April 2016 an excavation found the cartridges and reopened the story behind this videogame.\nYears later Atari\u0026rsquo;s developer Howard Scott Warshaw, claimed that he made the best he could for a six-week project before the Xmas season. This was the rule of thumb during that time. In 1983 the videogame industry crashed with a 97% reduction of benefits. Some of the factors leading to this crash may sound familiar to you:\nA flooded console market. Each new console came with its own library of games.\nToo many titles. Initially, console manufacturers such as Atari developed their own games. Then, third-party developers started creating games for existing platforms. The number of vendors literally exploded and the market became saturated with companies searching for a piece of the pie.\nLack of consumer confidence. Products such as the aforementioned E.T. videogame damaged consumers\u0026rsquo; confidence in Atari\u0026rsquo;s products. Atari paid around 20 million dollars for the rights of the film. Instead of recovering the investment, 3.5 million cartridges were returned.\nThe story repeats # Creating a video game is an exciting journey that involves multidisciplinary teams. The video games industry is expected to generate 196 billion dollars by 2022 and productions become larger and larger. Some people compare these productions with Hollywood movies. It is true that both move massive amounts of money and employ thousands of people. However, this comparison is biased. Final products are completely different. While movies are a static product, video games offer a dynamic experience based on virtual interaction. This dynamism is open to the free-will of the user and requires artifacts such as testing, software fixes, and versioning. In a few words, video games are software products that are alive.They have to evolve to satisfy users.\nAs it occurred back in 1983, marketing and the search for benefits collides with the non-understood complexity of software solutions. Hopefully, the Cyberpunk 2077 case will be an isolated chapter of the successful career of CD Projekt Red and they will offer us new exciting games in the future. The industry must bet for development, testing, and quality and forget about creating inflated hypes that can be counterproductive for developers and final users.\nMeanwhile, I will enjoy the hilarious bugs from this game :)\nThanks for reading!\n","date":"21 December 2020","permalink":"/are-we-living-another-video-games-industry-crash-the-cyberpunk-2077-case/","section":"","summary":"Cyberpunk 2077 is probably the most awaited game from this pandemic 2020. A lot of expectation has been around this game since it was first announced back in 2012. In 2018 it was the sensation during the E3 (the most important video games fair in the world) winning several awards.","title":"Are we living another video games industry crash? The Cyberpunk 2077 case"},{"content":"","date":"21 December 2020","permalink":"/tags/cyberpunk/","section":"Tags","summary":"","title":"cyberpunk"},{"content":"","date":"21 December 2020","permalink":"/tags/cyberpunk2077/","section":"Tags","summary":"","title":"cyberpunk2077"},{"content":"","date":"21 December 2020","permalink":"/tags/development/","section":"Tags","summary":"","title":"development"},{"content":"","date":"21 December 2020","permalink":"/tags/videogames/","section":"Tags","summary":"","title":"videogames"},{"content":"","date":"21 December 2020","permalink":"/categories/videogames/","section":"Categories","summary":"","title":"videogames"},{"content":"","date":"23 September 2020","permalink":"/tags/edge-computing/","section":"Tags","summary":"","title":"edge computing"},{"content":"","date":"23 September 2020","permalink":"/tags/fog-computing/","section":"Tags","summary":"","title":"fog computing"},{"content":"","date":"23 September 2020","permalink":"/tags/technology/","section":"Tags","summary":"","title":"technology"},{"content":"The first time I heard about edge computing was back in 2015. Since then, I have been working for startups to enable distributed data-driven solutions for the edge. It looks like everybody (or almost everybody) is aware of what edge computing is. However, all this time I have been working in a technological paradigm without a clear definition statement.\nAt the moment of writing this post, there is no clear definition of what the edge is. These are some definitions you can find online. Some of them are the ones used in the Wikipedia entry for edge computing.\nall computing outside the cloud happening at the edge of the network, and more specifically in applications where real-time processing of data is required Karim Arabi\nYour mobile phone and all your wearables are the edge according to this definition.\nanything that\u0026rsquo;s not a traditional data center could be the \u0026rsquo;edge\u0026rsquo; to somebody ETSI\nThis may include elements such as server proxies.\nthe edge node is mostly one or two hops away from the mobile client to meet the response time constraints for real-time games' Gamelets — Multiplayer mobile games with distributed micro-clouds\nIn this case, the edge is not the final user device. It is something between the cloud and the user.\ncomputing that’s done at or near the source of the data, instead of relying on the cloud at one of a dozen data centers to do all the work Paul Miller\nThis definition points out the idea of data proximity.\nWell, I have to say that all these definitions are correct. Why? Because the edge is so abstract that it admits almost any definition. The edge is so vaguely defined, that becomes something blurry and difficult to demarcate in any architectural design. Also, we have the cloud and the fog. What does the edge have to do with the cloud? And the fog?\nWhy Do we Need the Edge Computing? # Distributed architectures are extremely dynamic, and new requirements appear every day. We constantly revisit computing paradigms and adapt them to fit these new requirements. Edge computing is not an exception.\nYouTube appeared back in 2005, offering videos by streaming was a disruptive approach to the previous download-it-and-then-watch-it solution. Data flows from servers to users for its consumption. Among the many technical challenges of this approach, we have the latency issue. The more hops data has to travel through, the higher the latency is. The best solution is to replicate data into servers near its final destination. Akamai has been doing this since 1998, and it is a 7000 employees global company.\nNowadays, data keeps flowing from servers to consumers. However, the final user is not a passive consumer any longer. Every minute 350,000 tweets are generated, 300 hours of video are uploaded to YouTube every minute, and 100M pictures and photos are uploaded to Instagram per day. Users generate vast amounts of information to be distributed and shared. Numbers are even larger if we include information produced by mobile phones, sensors, and other devices. Additionally, the promised 5G will push even harder the limits of current infrastructures.\nData is pushing hard from users to data centers. The inclusion of AI applications in the end-user loop demands additional data processing that increases the costs. Is here, where transferring part of the computation to the final user can improve the user experience while releasing computing facilities cutting down operational costs.\nEdge computing has something to do with this final step of the data consumption/production flow. However, is edge computing a set of rules, a programming paradigm, an architecture, or a library? This is what we need to define.\nThe Fog Finishes Where the Edge Starts # We cannot talk about edge computing without comparing it with fog computing. Unlike edge computing, the fog has a clear definition statement. Large companies including Intel, Cisco, or Microsoft among others joined efforts and created the Open Fog Consortium back in 2015. They define fog computing as:\nA horizontal, system-level architecture that distributes computing, storage, control and networking functions closer to the users along a cloud-to-thing continuum\nYou can check the reference architecture here.\nThis definition clearly states that fog computing is an architecture. Nor a technology, nor a protocol, nor a paradigm, an architecture with its interconnected components. And I think this is a smart decision. The complexity of a problem with so many variables and scenarios cannot be addressed by a single technology.\nThe fog occupies the room between the cloud and the edge and can be whatever. Fog is some sort of poetical name to define everything that exists between the cloud and the edge. Surprisingly, if you check the aforementioned reference architecture document, edge computing is only mentioned in the glossary. However, the term edge appears several times in the document.\nThen, What Is Edge Computing? # If we assume that the fog is an architecture composed of different interconnected elements allocated between the cloud and the edge. Then, what is the edge?\nIs the edge an architecture? This would mean that the edge is probably organized into hierarchies, layers, etc., and this does not seem to be the case.\nIs the edge a protocol? If so, which one?\nIs the edge a programming paradigm? I do not think so. If this were the case, there must already be a programming language supporting this new paradigm.\nIs the edge a set of recommendations/experiences/use cases? Maybe.\nIt is difficult to answer any of these questions. Furthermore, I ignore the fact that the fog definition somehow overlaps with what many practitioners assume is a task to be done at the edge. Is the edge a component with some personality? Or it only a passive entity consuming/producing data?\nSummary # As you can see, edge computing is an open concept that is not clearly defined yet. You can extract from my words that there is a certain agreement on what the edge is. However, I think it is time to clearly define what we talk about when we refer to edge computing. There is room for new projects to become a reference in the Edge Computing world. As Google is the reference in search engines, I think Edge Computing will become defined once we have projects that stand out.\nHopefully, this post is a starting discussion trigger. I would love to hear your opinions about this topic.\nThanks for reading!\n","date":"23 September 2020","permalink":"/define_edge_computing/","section":"","summary":"The first time I heard about edge computing was back in 2015. Since then, I have been working for startups to enable distributed data-driven solutions for the edge. It looks like everybody (or almost everybody) is aware of what edge computing is.","title":"We Have to Define What is Edge Computing"},{"content":"In the 21st century, any decent software project must use a logger. One of the worst presents a software engineer can receive is a project using print statements. This is wrong. We all know that. If you are writing projects in Go or you have just started with this language it can be difficult to find guidelines about what logger solution is the best.\nThe Golang ecosystem is large and unexplored. When I started using Go I checked some existing solutions and I even considered implementing my own logging until I found zerolog. Zerolog is efficient, easy to use, customizable, and returns JSON outputs. This is particularly interesting for easily processable log outputs.\nThe following examples contain the basics for any project. The code is available in Github for further details I hope it may serve you as a guideline.\nBasic Logging # Zerolog offers seven levels of log entries from Panic to Trace. The most basic message sets the log level and prints the message using JSON format.\npackage main import( \u0026#34;github.com/rs/zerolog/log\u0026#34; ) func main() { levels() } func levels() { // Panic of fatal messages stop the execution flow // log.Panic().Msg(\u0026#34;This is a panic message\u0026#34;) // log.Fatal().Msg(\u0026#34;This is a fatal message\u0026#34;) log.Error().Msg(\u0026#34;This is an error message\u0026#34;) log.Warn().Msg(\u0026#34;This is a warning message\u0026#34;) log.Info().Msg(\u0026#34;This is an information message\u0026#34;) log.Debug().Msg(\u0026#34;This is a debug message\u0026#34;) log.Trace().Msg(\u0026#34;This is a trace message\u0026#34;) } {\u0026#34;level\u0026#34;:\u0026#34;error\u0026#34;,\u0026#34;time\u0026#34;:\u0026#34;2020-09-18T17:51:49+02:00\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;This is an error message\u0026#34;} {\u0026#34;level\u0026#34;:\u0026#34;warn\u0026#34;,\u0026#34;time\u0026#34;:\u0026#34;2020-09-18T17:51:49+02:00\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;This is a warning message\u0026#34;} {\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;time\u0026#34;:\u0026#34;2020-09-18T17:51:49+02:00\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;This is an information message\u0026#34;} {\u0026#34;level\u0026#34;:\u0026#34;debug\u0026#34;,\u0026#34;time\u0026#34;:\u0026#34;2020-09-18T17:51:49+02:00\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;This is a debug message\u0026#34;} {\u0026#34;level\u0026#34;:\u0026#34;trace\u0026#34;,\u0026#34;time\u0026#34;:\u0026#34;2020-09-18T17:51:49+02:00\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;This is a trace message\u0026#34;} Change Log Levels # Not every execution requires the same level of logging. For example, you can go from a Debug level to a less verbose level like Info.\nfunc setGlobalLevel() { zerolog.SetGlobalLevel(zerolog.DebugLevel) log.Debug().Msg(\u0026#34;Debug message is displayed\u0026#34;) log.Info().Msg(\u0026#34;Info Message is displayed\u0026#34;) zerolog.SetGlobalLevel(zerolog.InfoLevel) log.Debug().Msg(\u0026#34;Degug message is no longer displayed\u0026#34;) log.Info().Msg(\u0026#34;Info message is displayed\u0026#34;) } Notice that in the output the second debug message is ignored because we have upgraded the logging level.\n{\u0026#34;level\u0026#34;:\u0026#34;debug\u0026#34;,\u0026#34;time\u0026#34;:\u0026#34;2020-09-18T17:54:44+02:00\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;Debug message is displayed\u0026#34;} {\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;time\u0026#34;:\u0026#34;2020-09-18T17:54:44+02:00\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;Info Message is displayed\u0026#34;} {\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;time\u0026#34;:\u0026#34;2020-09-18T17:54:44+02:00\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;Info message is displayed\u0026#34;} Structured Output # Because we are using the always-easy-to change JSON format, we can have additional fields in every logging entry apart from the textual message.\nfunc structured() { log.Info().Str(\u0026#34;mystr\u0026#34;,\u0026#34;this is a string\u0026#34;).Msg(\u0026#34;\u0026#34;) log.Info().Int(\u0026#34;myint\u0026#34;,1234).Msg(\u0026#34;\u0026#34;) log.Info().Int(\u0026#34;myint\u0026#34;,1234).Str(\u0026#34;str\u0026#34;,\u0026#34;some string\u0026#34;).Msg(\u0026#34;And a regular message\u0026#34;) } {\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;mystr\u0026#34;:\u0026#34;this is a string\u0026#34;,\u0026#34;time\u0026#34;:\u0026#34;2020-09-18T17:57:39+02:00\u0026#34;} {\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;myint\u0026#34;:1234,\u0026#34;time\u0026#34;:\u0026#34;2020-09-18T17:57:39+02:00\u0026#34;} {\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;myint\u0026#34;:1234,\u0026#34;str\u0026#34;:\u0026#34;some string\u0026#34;,\u0026#34;time\u0026#34;:\u0026#34;2020-09-18T17:57:39+02:00\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;And a regular message\u0026#34;} Logging Errors # Logging errors is similar to adding extra fields as shown above.\nfunc logError() { err := errors.New(\u0026#34;there as an error\u0026#34;) log.Error().Err(err).Msg(\u0026#34;this is the way to log errors\u0026#34;) } {\u0026#34;level\u0026#34;:\u0026#34;error\u0026#34;,\u0026#34;error\u0026#34;:\u0026#34;there as an error\u0026#34;,\u0026#34;time\u0026#34;:\u0026#34;2020-09-18T17:58:35+02:00\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;this is the way to log errors\u0026#34;} Subloggers # We can have several log instances running simultaneously. This is particularly useful when we deploy new components in our code that require additional information to be displayed. In this example, we have a sublogger that always displays the name of the component.\nfunc sublogger() { mainLogger := zerolog.New(os.Stderr).With().Logger() mainLogger.Info().Msg(\u0026#34;This is the output from the main logger\u0026#34;) subLogger := mainLogger.With().Str(\u0026#34;component\u0026#34;,\u0026#34;componentA\u0026#34;).Logger() subLogger.Info().Msg(\u0026#34;This is the the extended output from the sublogger\u0026#34;) } {\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;This is the output from the main logger\u0026#34;} {\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;component\u0026#34;:\u0026#34;componentA\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;This is the the extended output from the sublogger\u0026#34;} File Output # The previous examples use the standard output. To store the output in a file, we only have to instantiate a logger with the descriptor of a new file.\nfunc fileOutput() { // create a temp file tempFile, err := ioutil.TempFile(os.TempDir(),\u0026#34;deleteme\u0026#34;) if err != nil { // Can we log an error before we have our logger? :) log.Error().Err(err).Msg(\u0026#34;there was an error creating a temporary file four our log\u0026#34;) } fileLogger := zerolog.New(tempFile).With().Logger() fileLogger.Info().Msg(\u0026#34;This is an entry from my log\u0026#34;) fmt.Printf(\u0026#34;The log file is allocated at %s\\n\u0026#34;, tempFile.Name()) } The log file is allocated at /var/folders/6h/xffhh45j077157cb5mbk48zh0000gp/T/deleteme981120707 The log is directly sent to a temporary file allocated at the temporary folder in the system.\nPretty Logging # Zerolog offers a decorator with a more visual output specially designed for consoles. I do recommend this option for command line interfaces (CLI).\nfunc prettyConsole() { log.Logger = log.Output(zerolog.ConsoleWriter{Out: os.Stderr}) log.Error().Msg(\u0026#34;This is an error message\u0026#34;) log.Warn().Msg(\u0026#34;This is a warning message\u0026#34;) log.Info().Msg(\u0026#34;This is an information message\u0026#34;) log.Debug().Msg(\u0026#34;This is a debug message\u0026#34;) } ![Zerolog pretty console output]({{ source }}/assets/2020/09/Screen-Shot-2020-09-18-at-6.16.19-PM-300x66.png \u0026ldquo;Zerolog pretty console output\u0026rdquo;)\nSummary # From a small project to a large solution, logs are always necessary. Zerolog has good performance and is easy to use. You have no excuses to use it in your projects from the very beginning and stop using print statements.\nThanks for reading.\n","date":"18 September 2020","permalink":"/338-2/","section":"","summary":"In the 21st century, any decent software project must use a logger. One of the worst presents a software engineer can receive is a project using print statements. This is wrong.","title":"Log your Go projects with Zerolog"},{"content":"It has been three years now since I started coding in Go. I think it is time for a bit of retrospective and I would like to enumerate some of the aspects I found most \u0026ldquo;weird\u0026rdquo; during my Golang learning process.\nBefore we start and just in case you fell into this post by accident, some context:\nWhat is Go? # Go is a programming language developed by Google. Similar to C, it is compiled, statically typed with garbage collection, memory safety and with a special focus on concurrency and networking. The language is deeply used at Google to develop from services to complete solutions. Actually, Kubernetes is developed using Go.\nWhy did I start using Go? # I started designing a smart orchestrator for multi-cloud applications. The target platform was Kubernetes, so it seemed to be the perfect match for the project.\nGo features newbies (and not so newbies) may find weird # Humans are wired for prejudice. When we start coding in a programming language we do not feel comfortable. We always point out those features that annoy us the most. Sometimes it is difficult to put these features into the corresponding context and understanding the reasons why they were designed in such way. I enumerate some of the features I found weirder when starting my Go experience.\nIn my case, I have experience in a bunch of languages: C, C++, Java, Scala, Python, R (if we can consider R a language) among others. In general, I did not find my transition difficult. The syntax is straight forward, and the language is not very verbose (in principle). However, I found some frustration with the following features.\nUnnecessary imports and variables Go forces the code to be minimalistic. This implies that unnused imports and variables trigger compilation errors. For example:\nimport ( \u0026#34;fmt\u0026#34; \u0026#34;os\u0026#34; //not used ) func main() { fmt.Println(\u0026#34;Hola\u0026#34;) } The compiler returns\nimported and not used: \u0026#34;os\u0026#34; Iterating collections The range function used to iterate collections returns two values. The first is the position of the entry in the collection and the second value contains the entry value itself.\nx := [4]string{\u0026#34;one\u0026#34;,\u0026#34;two\u0026#34;,\u0026#34;three\u0026#34;,\u0026#34;four\u0026#34;} for i, entry := range(x) { fmt.Printf(\u0026#34;Element at position %d is %s\\n\u0026#34;, i, entry) } This is very handy because you have in every iteration the two most common values you may use in your loops. However, they are not always required. This means that you will do something like:\nx := [4]string{\u0026#34;one\u0026#34;,\u0026#34;two\u0026#34;,\u0026#34;three\u0026#34;,\u0026#34;four\u0026#34;} for i, entry := range(x) { fmt.Printf(\u0026#34;Element %s\\n\u0026#34;, entry) } Which returns an error during compilation:\ni declared but not used Or even worse, you will skip the i variable like this:\nx := [4]string{\u0026#34;one\u0026#34;,\u0026#34;two\u0026#34;,\u0026#34;three\u0026#34;,\u0026#34;four\u0026#34;} for entry := range(x) { fmt.Printf(\u0026#34;Element %s\\n\u0026#34;, entry) } Which can be really confusing because it returns the position value in a variable we expected to be the entry value.\nElement %!s(int=0) Element %!s(int=1) Element %!s(int=2) Element %!s(int=3) We simply have to indicate an unnused variable i.\nx := [4]string{\u0026#34;one\u0026#34;,\u0026#34;two\u0026#34;,\u0026#34;three\u0026#34;,\u0026#34;four\u0026#34;} for _, entry := range(x) { fmt.Printf(\u0026#34;Element %s\\n\u0026#34;, entry) } Attributes visibility Attributes are visible if they start with an uppercase letter. If no, they are private. It is simple. However, I do regularly forget about this resulting in stupid errors.\ntype Message struct { Text string // This is public text string // This is private } What happened to overloaded methods? If you come from the Java world you are probably used to overload methods. This is, for a method we can have several signatures. Well\u0026hellip; Golang has no method overloading.\nWhat happened to inheritance? There is no inheritance. Just that. You can do some workaround like the one described here, but we cannot really say that is inheritance.\nWhat about interfaces? There are interfaces. They can be defined as a collection of method signatures. However, they are \u0026ldquo;weird\u0026rdquo; in the sense you use them in other languages. Why? Because you do not programatically indicate that your struct implements an interface (something like class A implements interface I). Your struct fulfils an interface if it has the methods enumerated by the interface. It is easier to understand with an example.\npackage main import ( \u0026#34;fmt\u0026#34; ) type Speaker interface { SayYourName() string SayHello(b Speaker) string } type HappySpeaker struct {} func(hs HappySpeaker) SayYourName() string { return \u0026#34;Happy\u0026#34; } func(hs HappySpeaker) SayHello(b Speaker) string { return fmt.Sprintf(\u0026#34;Hello %s!\u0026#34;,b.SayYourName()) } type AngrySpeaker struct {} func(as AngrySpeaker) SayYourName() string { return \u0026#34;Angry\u0026#34; } func(as AngrySpeaker) SayHello(b Speaker) string { return fmt.Sprintf(\u0026#34;I\u0026#39;m not going to say hello to %s!\u0026#34;,b.SayYourName()) } func main() { // We have two different structs happy := HappySpeaker{} angry := AngrySpeaker{} // they can say their names fmt.Println(happy.SayYourName()) fmt.Println(angry.SayYourName()) // But they are also speakers fmt.Println(happy.SayHello(angry)) fmt.Println(angry.SayHello(happy)) // This is also valid var mrSpeaker Speaker = happy fmt.Println(mrSpeaker.SayHello(angry)) } As you can imagine this has larger implications when coding. Interfaces in Go are a much deeper topic for discussion and you can find a lot of examples defining pros and cons for this approach.\nWhat about constructors? There are no constructors like the ones you may find in any object oriented language. Structs definition resembles a lot the one used in C. With one potential issue: you can skip attributes setting when instantiating a new struct. In the following code halfMessage1and halfMessage2have unset attributes.\npackage main import ( \u0026#34;fmt\u0026#34; ) type Message struct { MsgA string MsgB string } func(m Message) SayIt() { fmt.Printf(\u0026#34;[%s] - [%s]\\n\u0026#34;,m.MsgA, m.MsgB) } func main() { fullMessage1 := Message{\u0026#34;hello\u0026#34;,\u0026#34;bye\u0026#34;} fullMessage2 := Message{MsgA: \u0026#34;hello\u0026#34;, MsgB: \u0026#34;bye\u0026#34;} halfMessage1 := Message{\u0026#34;hello\u0026#34;,\u0026#34;\u0026#34;} halfMessage2 := Message{MsgA: \u0026#34;hello\u0026#34;} emptyMessage := Message{} fullMessage1.SayIt() fullMessage2.SayIt() halfMessage1.SayIt() halfMessage2.SayIt()\temptyMessage.SayIt()\t} The output is:\n[hello] - [bye] [hello] - [bye] [hello] - [] [hello] - [] [] - [] This is always a potential issue because you can have methods expecting values to be set. A way to mitigate this is to define your own static \u0026ldquo;constructors\u0026rdquo;.\npackage main import ( \u0026#34;fmt\u0026#34; ) type Message struct { MsgA string MsgB string } func(m Message) SayIt() { fmt.Printf(\u0026#34;[%s] - [%s]\\n\u0026#34;,m.MsgA, m.MsgB) } func NewMessage(msgA string, msgB string) *Message{ if len(msgA) * len(msgB) == 0 { return nil } return \u0026amp;Message{MsgA: msgA, MsgB: msgB} } func main() { // A correct message msg1 := NewMessage(\u0026#34;hello\u0026#34;,\u0026#34;bye\u0026#34;)\tif msg1 != nil { msg1.SayIt() } else { fmt.Println(\u0026#34;There was an error\u0026#34;) } // An incorrect message msg2 := NewMessage(\u0026#34;\u0026#34;,\u0026#34;\u0026#34;) if msg2 != nil { msg2.SayIt() } else { fmt.Println(\u0026#34;There was an error\u0026#34;) } } Summary # This is a small excerpt of all the potential aspects you have to consider when conding in Go. I would like to hear from your experiences? What are the Go features you find the most weird?\n","date":"7 September 2020","permalink":"/golang-features-newbies-and-not-so-newbies-may-find-weird/","section":"","summary":"It has been three years now since I started coding in Go. I think it is time for a bit of retrospective and I would like to enumerate some of the aspects I found most \u0026ldquo;weird\u0026rdquo; during my Golang learning process.","title":"Golang features newbies (and not so newbies) may find weird"},{"content":"","date":"22 July 2020","permalink":"/categories/data-science/","section":"Categories","summary":"","title":"Data science"},{"content":"","date":"22 July 2020","permalink":"/tags/datascience/","section":"Tags","summary":"","title":"datascience"},{"content":"","date":"22 July 2020","permalink":"/tags/graphs/","section":"Tags","summary":"","title":"graphs"},{"content":"","date":"22 July 2020","permalink":"/categories/graphs/","section":"Categories","summary":"","title":"Graphs"},{"content":"","date":"22 July 2020","permalink":"/tags/structures/","section":"Tags","summary":"","title":"structures"},{"content":"In a previous post I exposed my opinion about the lack of a clear platform/solution/framework/architecture for graph processing. However, what are the main issues graph processing has to deal with? Serve these lines as an appetizer for the curious minds out there.\nA graph is a collection of vertices $V$ and edges $E$ connecting these vertices. A graph $G=(V,E)$ can be directed or undirected. In a directed graph any edge from vertex $u$ to vertex $v$ has a direction ($u {\\rightarrow} v$). This is, we can go from $u$ to $v$.\ngraph LR; u((u))--\u003ev((v)); If the graph is undirected we have no direction ($u {-} v$). This is, $u$ is connected to $v$.\ngraph LR; u((u)) --- v((v)); Additionally, we can have edges with weights ($W(e)$).\ngraph LR; u((Madrid))-- 621km ---v((Barcelona)); Graphs are particularly suitable to represent absolutely anything. From social networks to interactions between atomic forces. A very computer friendly representation of a graph is using an adjacency matrix $A$. $A$ is a square matrix of size V x V where $A_{ij}$ indicates that it exists an edge between vertices $i$ and $j$. For undirected graphs $A$ is a symmetrical matrix. For example, for the graph\ngraph LR; a((a)) --\u003e a((a)); a((a)) --\u003e b((b)); c((c)) --\u003e b((b)); a((a)) --\u003e d((d)); d((d)) --\u003e b((b)); b((b)) --\u003e d((d)); we can get its representation in an adjacency matrix:\n$$ \\begin{array}{ccc} \u0026amp; \u0026amp; to \\\\ \u0026amp; \u0026amp; \\begin{array}{cccc} a \u0026amp; b \u0026amp; c \u0026amp; d \\end{array} \\\\ \\textit{from} \u0026amp; \\begin{array}{c} \u0026amp; a \\\\ \u0026amp; b \\\\ \u0026amp; c \\\\ \u0026amp; d \\end{array} \u0026amp; \\left(\\begin{array}{cccc} 1 \u0026amp; 1 \u0026amp; 0 \u0026amp; 1 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \\\\ 0 \u0026amp; 1 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 1 \u0026amp; 0 \u0026amp; 0 \\end{array}\\right) \\end{array} $$\nBecause the previous representation is a matrix, we can use all the available tools of algebraic operations. For example, by summing up the values of every column we have the number of edges targetting every vertex. Additionally, we can compute eigenvalues and eigenvectors that can give us interesting information using spectral graph theory.\nBut there are other aspects to be taken into consideration. What about memory utilization? If you think about the adjacency matrix above, for a 4 x 4 matrix (16 cells) we have 10 empty cells (equal to 0). When allocating memory for this matrix, we are wasting 62% of the allocated memory with non-relevant information. For very large sparse matrices this cost is unacceptable. And we have sparse matrices in many scenarios, think about social networks. Facebook has 2.6 billion active users and there is a limit of 5000 friends per profile. Additionally, some operations become cumbersome. For example, knowing the adjacent vertices involves operations with the whole matrix.\nFortunately, we have other solutions that are more \u0026ldquo;memory friendly\u0026rdquo; or at least more suitable to certain scenarios. An adjacency list is a list where every item represents a vertex in the graph. For every vertex, we store pointers to its adjacent vertices (neighbours). Following with the example above:\ngraph TD; start(start) --\u003e a; subgraph vertex a a(a) --\u003e a0((a)); a0 --\u003e a1((b)); a1 --\u003e a2((d)); end subgraph vertex b b(b) --\u003e b0((b)); end subgraph vertex c c(c) --\u003e c0((b)); end subgraph vertex d d(d) --\u003e d0((b)); end a --\u003e b; c --\u003e d; b --\u003e c; d --\u003e last(nil); The adjacency list limits the information to be stored to the adjacent vertices. Vertex $a$ has access to a list of references to its adjacent vertices. This facilitates the implementation of traversing operations that can travel across the graph vertices and makes possible to store additional data enhancing data locality.\nAnd here is where graphs become a really painful data structure to deal with from the point of view of performance. Let\u0026rsquo;s assume we want to visit all the neighbors of vertex $a$ ($\\Gamma(a)$). The adjacent vertices of $a$ are: itself, $b$ and $d$. This means that we have to access memory positions for $b$ and $d$. What happens if these memory fragments are not already cached? That is a cache miss, which means that we have to pick up that piece of graph from main memory. For vertices with a large number of neighbors cache misses will iteratively repeat. This directly impacts the performance of a graph traverser. The lack of locality in memory accesses is a performance limitation in most graph algorithms.\nIf we think of distributed solutions, the scenario is even worse. For a distributed memory solution with the graph split across nodes we may have something like:\ngraph TD; start(start) --\u003e a; subgraph nodeA subgraph vertex a a(a) --\u003e a0((a)); a0 --\u003e a1((b)); a1 --\u003e a2((d)); end subgraph vertex b b(b) --\u003e b0((b)); end end subgraph nodeB subgraph vertex c c(c) --\u003e c0((b)); end subgraph vertex d d(d) --\u003e d0((b)); end end a --\u003e b; c --\u003e d; b -.-\u003e c; d --\u003e last(nil); For an operation running in nodeA any access to vertices $c$ or $d$ would have to retrieve information from other node. Complexity increases if edge weights can be modified. Who is the owner of the edge? And the vertices? Solutions such as Metis can give you the best graph partition across $n$ bins or nodes. However, adapting algorithms to work with distributed partitions is not easy at all. And computing the best partition is an expensive operation.\nAnd what about parallelism? This depends on the problem or algorithm to be parallelized. However, there is a clear problem in the utilization of concurrent access data structures such as queues or stacks in the classic iterators such as Breadth First Search (BFS) or Deep First Search (DFS). For example, this is the pseudocode for BFS:\n1 procedure BFS(G, root) is 2 let Q be a queue 3 label root as discovered 4 Q.enqueue(root) 5 while Q is not empty do 6 v := Q.dequeue() 7 if v is the goal then 8 return v 9 for all edges from v to w in G.adjacentEdges(v) do 10 if w is not labeled as discovered then 11 label w as discovered 12 w.parent := v 13 Q.enqueue(w) If we consider the pseudocode above to run in parallel we have to guarantee that $Q$ and labels structures are accessed/modified atomically. This imposes a tremendous bottleneck for multiple parallel instances that can result in really poor performance.\nA really interesting problem is what happens when the graph does not fit into main memory? Cache misses are expensive in terms of performance, but accesses to secondary memory are orders of magnitude more expensive.\nTo summarize some of the elements that make graph processing a big headache:\nGraphs are \u0026ldquo;unstructured\u0026rdquo; by default Lack of locality in memory access Operations requiring shared data structures Working with graphs split across distributed memory solutions is expensive. Actually, deciding how to split the graph is computationally expensive I\u0026rsquo;m sure you can enumerate other problems regarding graph processing.\n","date":"22 July 2020","permalink":"/posts/2020-07-22-the-many-challenges-of-graph-processing/","section":"","summary":"In a previous post I exposed my opinion about the lack of a clear platform/solution/framework/architecture for graph processing. However, what are the main issues graph processing has to deal with? Serve these lines as an appetizer for the curious minds out there.","title":"The many challenges of graph processing"},{"content":"We all depend on the Internet to search for potential solutions to technical problems. For example, for big data problems after five minutes in Google you will find out that Spark may help you. Even if you have no idea about what is Spark you will come across this name. Something similar occurs to TensorFlow when searching for deep learning solutions, Kubernetes for cloud, Docker for containers\u0026hellip; It seems that there is always one platform/framework/library for every buzzword in computer science. However, try to look for a graph processing solution. You will find out that there is no clear victor. And I find this quite surprising.\nIn 2015 me and my colleagues at Inria published an article proposing a middleware that could inspire developers to offer a generic framework to implement distributed graph processing solutions. And we made that because we had a strong feeling that there was not a consistent proposal to accelerate the development of massive graph processing solutions. And this is surprising if we consider that The Graph500 benchmark has some computing intensive problems using graphs. The explosion of social networks after the born of Facebook and Twitter captured the attention of the research community and put on the table new problems in terms of computation and scalability. Additionally, there is a vast number of problems that use graphs as the underlying data structure to be used. Graphs are used for fraud detection, game theory, and a vast number of data related problems.\nThere is massive ecosystem of graph libraries. Just to mention a few available I have experience with we have: GraphTool, SNAP, igraph, NetworkX, BGL, network for R, etc. And then we have modules of extensions for other more popular platforms such as GraphX for Spark, the Hadoop extension for graphs Giraph, the Neo4j which is really a database not a library.\nThen, we jump into a more obscure world of solutions that claim to be the most efficient in some aspect. A ton of work has been carried out by research facilities and/or universities. However, I would not suggest any developer to adopt any of these solutions. Purely. Why? Because they are hardly maintained, not documented and will eventually vanish. There are some notorious examples such as Google\u0026rsquo;s Pregel which inspired Apache Giraph and was never done publicly available (correct me if I\u0026rsquo;m wrong). GraphLab code was available and slowly vanished until becoming part of Turi.\nIf you are a developer and you need to include some graph processing consider these points:\nThe programming language you want to use may reduce your options. Are you looking for a specific algorithm or you want to explore and/or implement your own solution? The first somehow simplifies the search because you can look for the best performing algorithm implementation. If you want to do your own exploration look at the next point. How large is your graph? This is extremely important. Most of the libraries I have cited above will have problems when working with millions of vertices and edges. It is even worse if your graph does not fit into memory. Unfortunately, at the moment of writing this post we do not have a clear victor in the world of graph processing solutions. Actually, most solutions used in companies are tailor made from scratch. Developers code again and again the same algorithms contributing to the mayhem of existing implementations. I am strongly convinced that there is room (and the need) for a new paradigm that may help developers to model distributed graph algorithms simplifying the development and maintainability while increasing performance. Something similar to what TensorFlow did to neural networks by abstracting common pieces and concentrating the efforts of the community into a common solution.\nI would like to hear about your comments and experiences.\n","date":"16 July 2020","permalink":"/graph-processing-a-problem-with-no-clear-victor/","section":"","summary":"We all depend on the Internet to search for potential solutions to technical problems. For example, for big data problems after five minutes in Google you will find out that Spark may help you.","title":"Graph processing: a problem with no clear victor"},{"content":"","date":"16 July 2020","permalink":"/tags/spark/","section":"Tags","summary":"","title":"spark"},{"content":"","date":"16 July 2020","permalink":"/tags/tensorflow/","section":"Tags","summary":"","title":"tensorflow"},{"content":"","date":"4 June 2020","permalink":"/tags/istio/","section":"Tags","summary":"","title":"istio"},{"content":"","date":"4 June 2020","permalink":"/tags/k8s/","section":"Tags","summary":"","title":"k8s"},{"content":"","date":"4 June 2020","permalink":"/categories/k8s/","section":"Categories","summary":"","title":"K8s"},{"content":"","date":"4 June 2020","permalink":"/tags/kubernetes/","section":"Tags","summary":"","title":"kubernetes"},{"content":"A forgotten aspect of Kubernetes deployments is access control. Kubernetes assumes services are isolated depending on the namespace they belong to. This is a simple rule of thumb that may be good enough for several solutions. However, not all the services may be accessed from any service in the same namespace.\nAccess control is particularly suitable to avoid confusion between development/production environments and to difficult malicious accesses. A good practice is to define what services can be accessed by whom in order to preserve the coherence of a solution built on top of multiple services.\nIstio offers a very handy solution to define services access control using Authorization Policies. These policies can be used for both HTTP and TCP traffic with access control based on service accounts, namespaces or HTTP methods. For example:\napiVersion: security.istio.io/v1beta1 kind: AuthorizationPolicy metadata: name: myserver namespace: foo spec: action: ALLOW rules: - from: - source: principals: [\u0026#34;cluster.local/ns/default/sa/myapp\u0026#34;] to: - operation: ports: - 9200 - from: - source: namespaces: [\u0026#34;foo\u0026#34;] to: - operation: ports: - 9200 - 9300 selector: matchLabels: app: myserver version: v1 The authorization policy defined above affects to any deployment in the same namespace that matches the selector. This affects to any service accessing a pod matching the selector. This is useful when we have several services for a single endpoint. In particular, this policy authorizes traffic coming from the same namespace foo to ports 9200 and 9300. However, only port 9200 is accessible for applications from the service account myapp. Accesses from any other source is denied.\nFort HTTP REST APIs it is possible to define a finer granularity access control.\napiVersion: security.istio.io/v1beta1 kind: AuthorizationPolicy metadata: name: myserver namespace: foo spec: action: ALLOW rules: - from: - source: principals: [\u0026#34;cluster.local/ns/default/sa/myapp\u0026#34;] to: - operation: methods: [\u0026#34;GET\u0026#34;] paths: [\u0026#34;/add*\u0026#34;] - from: - source: namespaces: [\u0026#34;foo\u0026#34;] to: - operation: methods: [\u0026#34;GET\u0026#34;] paths: [\u0026#34;/add*\u0026#34;,\u0026#34;/delete*\u0026#34;] selector: matchLabels: app: myserver version: v1 In this case we grant access to add and delete methods in the API if and only if traffic comes from namespace foo. Other traffic accesses are denied.\nI do strongly suggest you to define authorization policies if you have installed Istio. In case you have not done it yet, visit my previous post with some tips for Istio. Check out the official Istio documentation for more details about Authorization Policies.\n","date":"4 June 2020","permalink":"/kubernetes-services-access-control-with-istio/","section":"","summary":"A forgotten aspect of Kubernetes deployments is access control. Kubernetes assumes services are isolated depending on the namespace they belong to. This is a simple rule of thumb that may be good enough for several solutions.","title":"Kubernetes services access control with Istio"},{"content":"","date":"26 May 2020","permalink":"/tags/services/","section":"Tags","summary":"","title":"services"},{"content":"A forgotten aspect of Kubernetes deployments is access control. Kubernetes assumes services are isolated depending on the namespace they belong to. This is a simple rule of thumb that may be good enough for several solutions. However, not all the services may be accessed from any service in the same namespace.\nAccess control is particularly suitable to avoid confusion between development/production environments and to difficult malicious accesses. A good practice is to define what services can be accessed by whom in order to preserve the coherence of a solution built on top of multiple services.\nIstio offers a very handy solution to define services access control using Authorization Policies. These policies can be used for both HTTP and TCP traffic with access control based on service accounts, namespaces or HTTP methods. For example:\napiVersion: security.istio.io/v1beta1 kind: AuthorizationPolicy metadata: name: myserver namespace: foo spec: action: ALLOW rules: - from: - source: principals: [\u0026#34;cluster.local/ns/default/sa/myapp\u0026#34;] to: - operation: ports: - 9200 - from: - source: namespaces: [\u0026#34;foo\u0026#34;] to: - operation: ports: - 9200 - 9300 selector: matchLabels: app: myserver version: v1 The authorization policy defined above affects to any deployment in the same namespace that matches the selector. This affects to any service accessing a pod matching the selector. This is useful when we have several services for a single endpoint. In particular, this policy authorizes traffic coming from the same namespace foo to ports 9200 and 9300. However, only port 9200 is accessible for applications from the service account myapp. Accesses from any other source is denied.\nFort HTTP REST APIs it is possible to define a finer granularity access control.\napiVersion: security.istio.io/v1beta1 kind: AuthorizationPolicy metadata: name: myserver namespace: foo spec: action: ALLOW rules: - from: - source: principals: [\u0026#34;cluster.local/ns/default/sa/myapp\u0026#34;] to: - operation: methods: [\u0026#34;GET\u0026#34;] paths: [\u0026#34;/add*\u0026#34;] - from: - source: namespaces: [\u0026#34;foo\u0026#34;] to: - operation: methods: [\u0026#34;GET\u0026#34;] paths: [\u0026#34;/add*\u0026#34;,\u0026#34;/delete*\u0026#34;] selector: matchLabels: app: myserver version: v1 In this case we grant access to add and delete methods in the API if and only if traffic comes from namespace foo. Other traffic accesses are denied.\nI do strongly suggest you to define authorization policies if you have installed Istio. In case you have not done it yet, visit my previous post with some tips for Istio. Check out the official Istio documentation for more details about Authorization Policies.\n","date":"26 May 2020","permalink":"/six-tips-before-you-start-using-istio/","section":"","summary":"A forgotten aspect of Kubernetes deployments is access control. Kubernetes assumes services are isolated depending on the namespace they belong to. This is a simple rule of thumb that may be good enough for several solutions.","title":"Six tips before you start using Istio"},{"content":"","date":"14 May 2020","permalink":"/tags/borges/","section":"Tags","summary":"","title":"borges"},{"content":"","date":"14 May 2020","permalink":"/tags/machine-learning/","section":"Tags","summary":"","title":"machine learning"},{"content":" Jorge Luis Borges (1899-1986) was a brilliant writter and one of the most influential authors of all times. His tales are a must-read for anyone interested in science/science-fiction. But what can Borges teach you about overfitting?\nFunes the memorious # Borges wrote Funes the memorious (originally \u0026ldquo;Funes el memorioso\u0026rdquo;) in 1954. This tale was born after the author suffered from insomnia. This tale tells the story of Ireneo Funes who suffers from hypermnesia. After a horseback riding accident, Funes discovers that he can remember absolutely everything: the shape of the clouds in every moment of the day, the position of the light in every corner of the house, what he did minute by minute two months ago, etc.\nIn this tale, Borges explores various topics regarding several aspects of our life that require the \u0026ldquo;art of forgetting\u0026rdquo;. By remembering absolutely everything Funes loses one of the most important features of the thinking process: generalization. Funes cannot understand how the term \u0026ldquo;dog\u0026rdquo; can group every dog if they are clearly different. He can easily differentiate the small black dog with shiny eyes from the small black dog with that red dot in the left eye, but he cannot understand what makes a dog to be a dog.\nOverfitting or the lost of generalization # Funes hypermnesia is more a misfortune than a gift. With no generalization is impossible to use abstract thinking. And without abstract thinking, Funes is closer to a machine rather than a human being. He goes into the opposite direction of what we expect to obtain with machine learning.\nOverfitting is to machine learning what hypermnesia is to Funes. Overfitted models cannot distinguish between noisy observations and the underlying model. This is, they cannot generalize.\nThe figure below shows two binary classifiers (black and green lines). The overfitted classifier (green line) is very dependant on the training data and it is very likely to have a poor performance when new observations arrive.\nHow do I know I have an overfitted model? When you observe a much better performance in your training set than in your testing set.\nThen, how can I prevent overfitting?\nConsider large enough datasets. If your dataset is too small your model will simply learn by heart ignoring any general rules. Cross validation always in mind. Regularization always helps. Ensembles of models can help with generalization. Early stopping. Iterative algorithms (CNN, DNN, RNN, etc.) suffer from the local minima problem. Stopping on time can give you better results. Hopefully, you will consider reading Funes the memorious or any Borges tale. And hopefully, you will think about Funes when you find your next overfitted model.\n","date":"14 May 2020","permalink":"/what-can-borges-teach-you-about-overfitting/","section":"","summary":"Jorge Luis Borges (1899-1986) was a brilliant writter and one of the most influential authors of all times. His tales are a must-read for anyone interested in science/science-fiction. But what can Borges teach you about overfitting?","title":"What can Borges teach you about overfitting?"},{"content":"","date":"10 May 2020","permalink":"/tags/latex/","section":"Tags","summary":"","title":"latex"},{"content":"If you have to write down a document you will run your default text processor (probably MS Word) and you will not consider any other option. This processor will fulfil all your needs. I would say that 95% of users out there has no idea what is LaTeX. And this is perfectly fine. However, it is a pity. Because, LaTeX is one of the most successful and amazing free software projects ever done. It has been around for more than thirty years with two Turing awarded researchers directly participating in its design and implementation. LaTeX must have something special. Hopefully, after reading this post you will consider giving it a try.\nI will not showcase how to use LaTeX, there is a lot of wonderful tutorials around. I will only enumerate when you MUST, SHOULD and SHOULD NOT use LaTeX.\nA bit of history # Donald Knuth (Turing Award 1974) published his first edition of The Art of Computer Programming in 1968 when he was thirty. By then, books were printed using monotype settings. Knuth was happy with the final print. However, the second edition in 1976 had to be typeset again because the original fonts were no longer available. When Knuth received the galley proofs he was really disappointed. He found them inferior.\nHe commited himself to design his own typesetting system. We are talking about the late seventies, when digital typesetting was a problem to be solved. Actually, Steve Jobs himself contributed to this topic. Knuth planned to spend his sabbatical year in 1978 to finish the project. He really understimated the complexity of the task. The final solution was not ready until 1989! Knuth called this language TeX with each letter a capital Greek letters tau ($\\tau$), epsilon ($\\epsilon$) and chi ($\\chi$). TeX is the abbreviation for τέχνη (techne) which means \u0026ldquo;art\u0026rdquo; and \u0026ldquo;craft\u0026rdquo;. Knuth has always insisted that you should pronounce it /tɛk/.\nWhen Leslie Lamport (Turing Award 2013) started using Knuth\u0026rsquo;s TeX he started writing some macros for his own purposes. LaTeX is simply LAmport\u0026rsquo;s TeX, a collection of macros on top of TeX to make it easier. And this is the main collection we have today.\nWhat can I do with LaTeX? # With LaTeX you can have a really high quality typesetting document with a low effort. Yes with a low effort. I will discuss some details later. This claim is huge. EVERYBODY can get professional results writing plain text and using markups with a software that is free and can run virtually everywhere. That is why LaTeX is the standard in academia and engineering.\nWhen you MUST use LaTeX # You are in academia, particularly in any STEM discipline. In this scenario manuscripts are everything. Content is really important and requires a tremendous amount of work. In the case of PhD manuscripts you MUST consider spending some time learning LaTeX to make the difference in your final outcome. I have seen PhD. manuscripts written in MS Word and I have to say that somehow (for me) it diminishes the value of the manuscript. You work with abundant bibliography. If you are preparing any document with large bibliographies you want to go with LaTeX + BibTeX. Simply prepare your BibTeX file with your bibliography entries, tag them, and use the label in your latex document. The compiler will do the rest. I know, there are plugins and solutions for MS Word and other text processors. But remember, for thirty years this problem has been solved in an easy way. And from my experience, these plugins result cumbersome. You are using formulas. Formulas like $y=\\frac{1}{x}$ can be easily managed by some plugins. Formulas like $f(x)=\\frac{1}{\\sigma \\sqrt{2\\pi}}e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}$ can be simply impossible to write. Every decent solution around to manage math formulas is based on LaTeX. Why not to use it directly? You need outprints with figures using the best quality possible. Formats such as SVG cannot be available for your text processor. With LaTeX you can generate PDF documents with embed SVG figures. Not many solutions around can offer something like this. You want a free solution. You want it to be forward compatible. LaTeX has been around for more than thirty years. We can typeset really old documents and see how they were intended to be. One entry point several output formats. Because LaTeX is a typesetting system you can get outputs in DVI, PDF, HTML, XML, etc. with a single document. You want to forget about the document layout. LaTeX is somehow like HTML + CSS, once you define the document structure you simply use a markup language and the compiler will make everything coherent for you. No more paragraphs separated with double spacing instead of single space. When you SHOULD use LaTeX # You are new to LaTeX, you have to start a new project and you are looking for all the advantages that it offers. This is the moment to start with LaTeX. You want your documents to stand out among others. And you will. LaTeX outcomes have a distinguishing quality and it is loved among practitioners. You are considering to write a book, article, manuscript\u0026hellip; and maybe self-publish it. This is a common situation nowadays with the adoption of platforms such as Amazon Self Publishing. With LaTeX you can go from your raw text to a high quality .epub, .mobi ebook file. When you SHOULD NOT use LaTeX # Your document is already written in other format. The content is probably easy to be moved to LaTeX. However, the layout of the document could be hard to get. You are doing a collaborative work and you are the only LaTeX practitioner. Do not move into LaTeX, do not even consider it. My experience says that after starting everything to LaTeX, your colleagues will complain and you will finally move everything to a commonly understood format two hours before the deadline. The layout of your document means everything to you. This means that you are thinking about a mesmerizing print with 30 types of fonts, text lines crossing the text body, images in every possible place in your document\u0026hellip; Then probably LaTeX is not your candidate. Think about QuarkXPress. When people complain about LaTeX they say\u0026hellip; # It is difficult. LaTeX has a much stepper learning curve when compared with MS Word that is true. However, getting a basic LaTeX (text, figures, titles, tables) is not so difficult. There is a million examples out there. The complexity comes in understanding the concepts used by LaTeX such as floating objects. I cannot see what I am doing. LaTeX is not a WYSIWYG solution, this means that you have to compile and then check the output. Fortunately, there are some programs such as TeXMaker that give you a better user experience. Figures do not appear where I want. Well, this is a classic misconception about how figures placement works in LaTeX. LaTeX computes the best placement for your figures. This can be configured using modifiers. I cannot easily change the layout of my document. This is true. If you want to set your own document structure you need to have a deeper understanding of the macros. There is a nice community to help you with it. However, this may require some time and effort. However, there is a vast number of templates already defined ready to be used. You can look for the one the best suits you. And now\u0026hellip; # If you read so far this probably means I captured your interest. If so, you can start learning some basics here and if you need some help check the StackExchange.\n","date":"10 May 2020","permalink":"/posts/2020-05-10-why-you-should-learn-latex-or-at-least-give-it-a-try/","section":"","summary":"If you have to write down a document you will run your default text processor (probably MS Word) and you will not consider any other option. This processor will fulfil all your needs.","title":"Why should you learn LaTeX, or at least give it a try?"},{"content":"","date":"30 April 2020","permalink":"/tags/covid19/","section":"Tags","summary":"","title":"covid19"},{"content":"The original Jupyter notebook can be found here.\nIntroduction # This notebook is the second part of our analysis of how air traffic may have contributed to spread the Covid19 virus. In the previous part, we did a visual analysis of the OpenFlights dataset and showed how to plot geo data with Plotly. Then, we run a first intro to graph-tool and SIR modelling. In this part, we use tools such as Numpy, Pandas, Plotly, and graph-tool to run our own SIR pandemic model and learn how to leverage available data to improve our graph based models.\nNote: we assume you are familiar with the previous part of this notebook. If not, please check it out before continuing.\nDataset # The OpenFlights dataset contains airlines, air routes and airports per country. The information contained in the dataset is up to 2014. This may be a bit outdated. However, it is detailed enough for our purposes.\nAdditionally, we use the Kraggle Covid19 challenge data as we did in our previous Covid19 series.\nimport pandas as pd import numpy as np import plotly.graph_objects as go import plotly.express as px import datetime routes_dataset_url = \u0026#39;https://raw.githubusercontent.com/jpatokal/openflights/master/data/routes.dat\u0026#39; airports_dataset_url = \u0026#39;https://raw.githubusercontent.com/jpatokal/openflights/master/data/airports.dat\u0026#39; countries_dataset_url = \u0026#39;https://raw.githubusercontent.com/jpatokal/openflights/master/data/countries.dat\u0026#39; # Load the file with the routes routes = pd.read_csv(routes_dataset_url, names=[\u0026#39;Airline\u0026#39;,\u0026#39;AirlineID\u0026#39;,\u0026#39;SourceAirport\u0026#39;, \u0026#39;SourceAirportID\u0026#39;, \u0026#39;DestinationAirport\u0026#39;, \u0026#39;DestinationAirportID\u0026#39;, \u0026#39;CodeShare\u0026#39;,\u0026#39;Stops\u0026#39;,\u0026#39;Equipment\u0026#39;], na_values=\u0026#39;\\\\N\u0026#39;) # Load the file with the airports airports = pd.read_csv(airports_dataset_url, names=[\u0026#39;AirportID\u0026#39;,\u0026#39;Name\u0026#39;,\u0026#39;City\u0026#39;,\u0026#39;Country\u0026#39;,\u0026#39;IATA\u0026#39;,\u0026#39;ICAO\u0026#39;, \u0026#39;Latitude\u0026#39;, \u0026#39;Longitude\u0026#39;, \u0026#39;Altitude\u0026#39;, \u0026#39;Timezone\u0026#39;, \u0026#39;DST\u0026#39;, \u0026#39;TzDatabaseTimeZone\u0026#39;, \u0026#39;Type\u0026#39;, \u0026#39;Source\u0026#39;], index_col=\u0026#39;AirportID\u0026#39;, na_values=\u0026#39;\\\\N\u0026#39;) # Covid19 data covid_data = pd.read_csv(\u0026#39;covid19-global-forecasting-week-2/train.csv\u0026#39;,parse_dates=[\u0026#39;Date\u0026#39;], date_parser=lambda x: datetime.datetime.strptime(x, \u0026#39;%Y-%m-%d\u0026#39;)) # We compute some intermediate dataframes that will be useful later # Aggregate the number of flights per airport. # Source_airport -\u0026gt; destination_airport, number of flights flights_airport = routes.groupby([\u0026#39;SourceAirportID\u0026#39;,\u0026#39;DestinationAirportID\u0026#39;]).size().reset_index(name=\u0026#39;num_flights\u0026#39;) # Get the countries for the previous airport IDs flights_country=flights_airport.join(airports.Country,on=\u0026#39;SourceAirportID\u0026#39;,lsuffix=\u0026#39;Source\u0026#39;).\\ join(airports.Country,on=\u0026#39;DestinationAirportID\u0026#39;,lsuffix=\u0026#39;Destination\u0026#39;)[[\u0026#39;Country\u0026#39;,\u0026#39;CountryDestination\u0026#39;,\u0026#39;num_flights\u0026#39;]] # Aggregate the number of flights between countries # SourceCountry -\u0026gt; DestinationCountry, number of flights paths = flights_country.groupby([\u0026#39;Country\u0026#39;,\u0026#39;CountryDestination\u0026#39;]).sum() # Aggregate the number of flights to other countries. paths_other_countries = flights_country[flights_country.Country != flights_country.CountryDestination].groupby([\u0026#39;Country\u0026#39;,\u0026#39;CountryDestination\u0026#39;]).sum() # Total number of flights per country total_flights_country = paths.groupby([\u0026#39;Country\u0026#39;]).sum() # Total number of flights to other countries total_flights_other_countries = paths_other_countries.groupby([\u0026#39;Country\u0026#39;]).sum() Air traffic network # As we did in the previous part of this notebook, we create a directed graph $G(V,E)$ where the set of vertices $V$ represents the countries and the set of edges $E$ represents existing air routes from country $A$ to $B$ for every $E(A,B)$. For additional purposes, we set weights $W(E)$ with the number of flights from $A$ to $B$. We also set a vertex value with with the number of flights for every country.\nAs we observed in the previous notebook, some countries have a large number of domestic flights. In our graph, this is translated into vertex loops. This may have a relevant impact in the pandemic process, for this reason, we generate graphs with and without loops. We study the differences later.\n# Run this to ensure that the drawing module is correctly stablished from graph_tool.all import * import graph_tool as gt def generate_graph(list_countries, self_loops=False): \u0026#39;\u0026#39;\u0026#39; Generate the graph for a list of countries indicating if using loops or not. The generated output is a graph with: vertex property \u0026#34;country\u0026#34; with the country every vertex represents vertex property \u0026#34;out_flights\u0026#34; with the number of outgoing flights from a country edge property \u0026#34;num_flights\u0026#34; with the number of flights from country A to country B Finally, the function returns a dictionary with the vertex reference for every country. \u0026#39;\u0026#39;\u0026#39; g = gt.Graph() # Create the country property with the country names countries_prop = g.new_vertex_property(\u0026#39;string\u0026#39;) g.vertex_properties[\u0026#39;country\u0026#39;] = countries_prop # Total number of outgoing flights per vertex out_flights_prop = g.new_vertex_property(\u0026#39;int\u0026#39;) g.vertex_properties[\u0026#39;out_flights\u0026#39;] = out_flights_prop # Create edge property with the number of flights between countries num_flights_prop = g.new_edge_property(\u0026#39;double\u0026#39;) g.edge_properties[\u0026#39;num_flights\u0026#39;] = num_flights_prop g.add_vertex(len(list_countries)) # For a given country, get the vertex country_vertex = {} index=0 for c in list_countries: v = g.vertex(index) index=index+1 countries_prop[v] = c country_vertex[c] = v # We do not have info for all the countries. Skip when exception try: if self_loops: out_flights_prop[v] = total_flights_country.loc[c][\u0026#39;num_flights\u0026#39;] else: out_flights_prop[v] = total_flights_other_countries.loc[c][\u0026#39;num_flights\u0026#39;] except: out_flights_prop[v] = 0.0 to_iterate = paths if not self_loops: to_iterate = paths_other_countries # Add the edges for index,num_flights in to_iterate.iterrows(): s = country_vertex[index[0]] d = country_vertex[index[1]] # No self-loops if not self_loops and s != d: e = g.add_edge(s,d) num_flights_prop[e] = num_flights else: e = g.add_edge(s,d) num_flights_prop[e] = num_flights # link properties to the graph g.vertex_properties[\u0026#39;country\u0026#39;] = countries_prop g.vertex_properties[\u0026#39;out_flights\u0026#39;] = out_flights_prop g.edge_properties[\u0026#39;num_flights\u0026#39;] = num_flights_prop return g, country_vertex # load the graph with and without loops # Get the complete list of countries list_countries = np.union1d(paths.reset_index().Country.unique(), paths.reset_index().CountryDestination.unique()) g_loops, country_vertex_loops = generate_graph(list_countries,True) g_noloops, country_vertex_noloops = generate_graph(list_countries,False) print(\u0026#39;With loops -\u0026gt; vertices: %d and edges %d\u0026#39; % (g_loops.num_vertices(), g_loops.num_edges())) print(\u0026#39;Without loops -\u0026gt; vertices: %d and edges %d\u0026#39; % (g_noloops.num_vertices(), g_noloops.num_edges())) With loops -\u0026gt; vertices: 225 and edges 4697 Without loops -\u0026gt; vertices: 225 and edges 4558 SIR epidemic process # The SIR model is a simple, yet powerful, compartimental model for epidemics. The model consists of three compartments $S$, $I$ and $R$. $S$ is the number of susceptible, $I$ the number of infectuous and $R$ the number of recovered or deceased individuals. This is the most common model used right now and has many extensions and enhancements. The number of individuals in $S$ falls down as they become infected ($I$) and then recovered $R$. This is one of the famous infection curves used to forecast Covid19 evolution.\nThis model is particularly suitable for graphs. Edges determine transmission channels to propagate a contagious entity. Vertices transitions between the different compartments modeling the behaviour of pandemics. Using our graph, we assume that air traffic is a potential channel for contagious actors. The larger number of flights between two countries, the higher probabilities of a country to become infected.\nAs we presented in the previous notebook, graph-tool comes with an implementation of the SIR model easily configurable. We are going to explore four different scenarios combining our graph with an enhanced version using our air traffic information.\nName Loops Airtraffic BasicA No No BasicB Yes No EnhA No Yes EnhB Yes Yes The idea is to understand if domestic flights have an impact in the pandemia and whether we can get a better solution enhancing the graph with detailed air traffic data.\nWe create a helping function called runSIR that iterates the SIR model and collects information in a Pandas dataframe. Before that, we add a susceptible vertex property and a beta property to our graphs. The first property, is set to 0 for all the vertices but the root in the pandemia, in this case China. The beta property is a probability computed by dividing $W(E)$ by the total number of outgoing flights for the source vertex of $E$.\ndef runSIR(g,state,num_iter=100): \u0026#39;\u0026#39;\u0026#39; For a graph tool epidemic model run the state and return a dataframe with the countries infected in every step \u0026#39;\u0026#39;\u0026#39; infections = pd.DataFrame() previous = state.get_state().fa initial = np.where(previous==1)[0] countries_prop=g.vertex_properties[\u0026#39;country\u0026#39;] for t in range(num_iter): ret=state.iterate_sync() new = state.get_state().fa # Find new infected countries diff = new - previous already_infected = state.get_state().fa.sum() non_infected = len(state.get_state().fa)-already_infected new_infected = [countries_prop[g.vertex(v)] for v in np.where(diff==1)[0]] previous = new.copy() # collect the results in a Pandas friendly way infections = infections.append([{\u0026#39;step\u0026#39;:t,\u0026#39;infected\u0026#39;:already_infected, \u0026#39;new_infected\u0026#39;: len(new_infected),\u0026#39;non_infected\u0026#39;: non_infected, \u0026#39;new_infected_countries\u0026#39;: new_infected}], ignore_index=True) return infections def prepare_graph_sir(g, root_vertex): num_flights = g.edge_properties[\u0026#39;num_flights\u0026#39;] out_flights = g.vertex_properties[\u0026#39;out_flights\u0026#39;] # Create an edge property map that can help us to define the beta probability between vertices beta_prop = g.new_edge_property(\u0026#39;double\u0026#39;) for e in g.edges(): beta_prop[e] = num_flights[e]/out_flights[e.source()] # Assume that vertices are not susceptible, except the seed. susceptible_prop = g.new_vertex_property(\u0026#39;float\u0026#39;) for v in g.vertices(): susceptible_prop[v] = 0.0 susceptible_prop[root_vertex]=1.0 g.vertex_properties[\u0026#39;susceptible\u0026#39;] = susceptible_prop g.edge_properties[\u0026#39;beta\u0026#39;] = beta_prop # We can set the prepare_graph_sir(g_loops, country_vertex_loops[\u0026#39;China\u0026#39;]) prepare_graph_sir(g_noloops, country_vertex_noloops[\u0026#39;China\u0026#39;]) # For the basic version, we set a fixed beta 0.03 basicA_state = gt.dynamics.SIRSState(g_loops, beta=0.03,v0 = country_vertex_loops[\u0026#39;China\u0026#39;], constant_beta=True,gamma=0, r=0, s=g_loops.vertex_properties[\u0026#39;susceptible\u0026#39;],mu=0,epsilon=0) basicB_state = gt.dynamics.SIRSState(g_noloops, beta=0.03,v0 = country_vertex_noloops[\u0026#39;China\u0026#39;], constant_beta=True,gamma=0, r=0, s=g_noloops.vertex_properties[\u0026#39;susceptible\u0026#39;],mu=0,epsilon=0) enhA_state = gt.dynamics.SIRSState(g_loops, beta=g_loops.edge_properties[\u0026#39;beta\u0026#39;],v0 = country_vertex_loops[\u0026#39;China\u0026#39;], constant_beta=True,gamma=0, r=0,s=g_loops.vertex_properties[\u0026#39;susceptible\u0026#39;],mu=0,epsilon=0) enhB_state = gt.dynamics.SIRSState(g_noloops, beta=g_noloops.edge_properties[\u0026#39;beta\u0026#39;],v0 = country_vertex_noloops[\u0026#39;China\u0026#39;], constant_beta=True,gamma=0, r=0,s=g_noloops.vertex_properties[\u0026#39;susceptible\u0026#39;],mu=0,epsilon=0) # We set the number of iterations to 64, to compare with the original Covid19 data. We choose 64, because this is the # number of days elapsed until all the countries have at least one Covid19 case declared. basicA = runSIR(g_loops,basicA_state,64) basicB = runSIR(g_noloops,basicB_state,64) enhA = runSIR(g_loops,enhA_state,64) enhB = runSIR(g_noloops,enhB_state,64) print(\u0026#39;BasicA (loops, no airtraffic)\u0026#39;) display(basicA[basicA.new_infected\u0026gt;0].head(5)) print(\u0026#39;BasicB (no loops, no airtraffic)\u0026#39;) display(basicB[basicB.new_infected\u0026gt;0].head(5)) print(\u0026#39;EnhA (loops, airtraffic)\u0026#39;) display(enhA[enhA.new_infected\u0026gt;0].head(5)) print(\u0026#39;EnhB (no loops, airtraffic)\u0026#39;) display(enhB[enhB.new_infected\u0026gt;0].head(5)) BasicA (loops, no airtraffic) step infected new_infected non_infected new_infected_countries 0 0 2 1 223 [Kyrgyzstan] 1 1 4 2 221 [Israel, Mongolia] 2 2 9 5 216 [Algeria, Georgia, Iran, Maldives, Turkey] 3 3 13 4 212 [Belgium, Jordan, Nepal, United Arab Emirates] 4 4 26 13 199 [Azerbaijan, Bahrain, Czech Republic, Egypt, K... BasicB (no loops, no airtraffic) step infected new_infected non_infected new_infected_countries 0 0 5 4 220 [Finland, Japan, Maldives, Northern Mariana Is... 1 1 10 5 215 [Denmark, Kenya, Mexico, Portugal, United King... 2 2 21 11 204 [Angola, Cote d'Ivoire, Ghana, Greece, Ireland... 3 3 47 26 178 [Armenia, Belgium, Benin, Cameroon, Canada, Eg... 4 4 71 24 154 [Bahamas, Bahrain, Burkina Faso, Burundi, Cent... EnhA (loops, airtraffic) step infected new_infected non_infected new_infected_countries 4 4 2 1 223 [Kazakhstan] 7 7 3 1 222 [Ukraine] 9 9 4 1 221 [Belarus] 10 10 6 2 219 [Russia, Spain] 11 11 10 4 215 [Austria, Belgium, Greece, Poland] EnhB (no loops, airtraffic) step infected new_infected non_infected new_infected_countries 0 0 2 1 223 [Taiwan] 1 1 3 1 222 [Japan] 2 2 4 1 221 [Germany] 3 3 5 1 220 [Hong Kong] 4 4 8 3 217 [Australia, Thailand, Turkey] Only by displaying the first 5 steps of the simulation we observe differences in the evolution of the pandemia. First steps in BasicA and BasicB infect a larger group of countries when compared with EnhA and EnhB. Additionally, we expect countries with a larger number of flights coming from China to be infected first. This is not the case with BasicA and BasicB. Now we can compare it graphically with our original Covid19 dataset.\n# Compute the number of infected countries # Get the first date with confirmed cases for every country first_date = covid_data[covid_data.ConfirmedCases \u0026gt; 0].groupby(\u0026#39;Country_Region\u0026#39;, as_index=False).Date.min().sort_values(by=\u0026#39;Date\u0026#39;) # Compute the number of elapsed days since the beginning of China\u0026#39;s outbreak first_date[\u0026#39;step\u0026#39;]=first_date.Date-datetime.datetime(2020,1,22) # Convert this column into an integer with the number of days. first_date.step = first_date.step.dt.days # Get the number of infected countries per region covid_infected = first_date[[\u0026#39;step\u0026#39;,\u0026#39;Country_Region\u0026#39;]].groupby(\u0026#39;step\u0026#39;).agg(new_infected=(\u0026#39;step\u0026#39;,\u0026#39;count\u0026#39;)) covid_infected[\u0026#39;infected\u0026#39;] = covid_infected[\u0026#39;new_infected\u0026#39;].cumsum() covid_infected = covid_infected.reset_index() f = go.Figure() f.add_trace( go.Scatter(x=covid_infected.step,y=covid_infected.infected,name=\u0026#39;Covid19\u0026#39;) ) f.add_trace( go.Scatter(x=basicA.step,y=basicA.infected,name=\u0026#39;BasicA\u0026#39;) ) f.add_trace( go.Scatter(x=basicB.step,y=basicB.infected,name=\u0026#39;BasicB\u0026#39;) ) f.add_trace( go.Scatter(x=enhA.step,y=enhA.infected,name=\u0026#39;EnhA\u0026#39;) ) f.add_trace( go.Scatter(x=enhB.step,y=enhB.infected,name=\u0026#39;EnhB\u0026#39;) ) f.update_layout( title_text=\u0026#39;Comparing SIR infected countries evolution with original Covid19\u0026#39;, xaxis=dict(title=\u0026#39;Step\u0026#39;), yaxis=dict(title=\u0026#39;Infected countries\u0026#39;) ) f.show(renderer=\u0026#39;svg\u0026#39;) There is a relevant distance between the original Covid19 series and our simulations. The original Covid19 has a sustained plateau that is ignored in the simulations. However, EnhA is the best approximation to the original series. This shows that there is a relevant impact considering domestic flights (loops in the graph) and air traffic.\nOrder of infection # Using our propagation model we can forecast when a country can be considered to be infected. Assuming that every step in the simulation corresponds to a day in the real Covid19 evolution, we can compute when a country gets infected. Then, we can compute the temporal distance between our simulation and the real observed day the country was declared infected. We consider a country to be infected as soon as it has a declared case of Covid19.\ndef compute_distances(truth,approx,method): distances = pd.DataFrame() for row in approx.iterrows(): for c in row[1].new_infected_countries: elapsed = truth[truth.Country_Region==c].step.values if len(elapsed)!=1: continue elapsed = elapsed[0] distances = distances.append( [{\u0026#39;country\u0026#39;:c,\u0026#39;truth\u0026#39;: elapsed,\u0026#39;approx\u0026#39;:row[1].step,\u0026#39;distance\u0026#39;:elapsed-row[1].step,\u0026#39;method\u0026#39;:method}], ignore_index=True) return distances distances = compute_distances(first_date,basicA,\u0026#39;basicA\u0026#39;) distances = distances.append(compute_distances(first_date,basicB,\u0026#39;basicB\u0026#39;)) distances = distances.append(compute_distances(first_date,enhA,\u0026#39;enhA\u0026#39;)) distances = distances.append(compute_distances(first_date,enhB,\u0026#39;enhB\u0026#39;)) fig = px.histogram(distances,\u0026#39;distance\u0026#39;,nbins=60,color=\u0026#39;method\u0026#39;,marginal=\u0026#39;box\u0026#39;) fig.show(renderer=\u0026#39;svg\u0026#39;) The histogram above shows the temporal distance for the countries to get infected in our four scenarios. The temporal distance is the difference between the day a country declared at least one infected and the day this country appears as infected in our simulations. Positive distances indicate that our simulations predicted an infection later than it was observed. And the other way around, negative distances indicate predictions before the infection was observed.\nOur models predict infected countries with an important delay. However, enhA outperforms the other methods. The marginal boxes indicate a distribution closer to 0 than the others. This demonstrates that leveraging air traffic have a positive impact in the performance of the SIR model.\nConclusions # In this notebook, we have explored how we can enhance a network dissemination model with SIR and air traffic data. Our results, far from being perfect, demonstrate how networks enhanced with additional data can improve dissemination models. In particular, we demonstrate that using air traffic data may have a relevant impact in the understanding and performance of existing Covid19 models.\nReferences # Tiago P. Peixoto, “The graph-tool python library”, figshare. (2014) DOI: 10.6084/m9.figshare.1164194 sci-hub\n","date":"30 April 2020","permalink":"/covid19-spreading-in-a-networking-model-part-ii/","section":"","summary":"The original Jupyter notebook can be found here.\nIntroduction # This notebook is the second part of our analysis of how air traffic may have contributed to spread the Covid19 virus.","title":"Covid19 spreading in a networking model (part II)"},{"content":"","date":"30 April 2020","permalink":"/tags/graph-tool/","section":"Tags","summary":"","title":"graph-tool"},{"content":"","date":"30 April 2020","permalink":"/tags/plotly/","section":"Tags","summary":"","title":"plotly"},{"content":"","date":"30 April 2020","permalink":"/tags/python/","section":"Tags","summary":"","title":"python"},{"content":"This is a reproduction of a Jupyter notebook you can find here\nIntroduction # The Covid19 has imposed a lot of pressure on epidemiologists to come up with models that can explain the impact of pandemics. There already is lot of theory developed around this topic you can check out. In particular, understanding SIR models is really useful to understand how researchers approach the problem of virus spreading. In this notebook, I introduce some ideas about how to use Pandas, plots and graphs to explore the evolution of pandemics. I show some tools you can use to simulate networks dynamics. Of course, this cannot be considered a contribution to the state of the art. However, you may find some interesting guidelines for other problems.\nThe question # The main question we ask ourselves is has air passengers something to do with Covid19?. If so, can we use air traffic data to know something else about Covid19 evolution?\nIt is known that the virus started in China and later propagated around the world. If air traffic has contributed to the spread of the virus, this means that we can somehow leverage a model that explains or predict the evolution of the disease. Furthermore, we could even predict what countries could be potentially infected and/or what flights shall be cancelled.\nThere is a vast amount of work that can be done in this topic. To make it more accessible we will split it into smaller parts. In this first part, we will explore the data we plan to use and how to work with graphs.\nDataset # The OpenFlights dataset contains airlines, air routes and airports per country. The information contained in the dataset contains information up to 2014. This may be a bit outdated. However, it is detailed enough for our purposes.\nAdditionally, we use the Kraggle Covid19 challenge data as we did in our previous Covid19 series.\nimport pandas as pd import numpy as np import plotly.graph_objects as go import plotly.express as px import datetime routes_dataset_url = \u0026#39;https://raw.githubusercontent.com/jpatokal/openflights/master/data/routes.dat\u0026#39; airports_dataset_url = \u0026#39;https://raw.githubusercontent.com/jpatokal/openflights/master/data/airports.dat\u0026#39; countries_dataset_url = \u0026#39;https://raw.githubusercontent.com/jpatokal/openflights/master/data/countries.dat\u0026#39; # Load the file with the routes routes = pd.read_csv(routes_dataset_url, names=[\u0026#39;Airline\u0026#39;,\u0026#39;AirlineID\u0026#39;,\u0026#39;SourceAirport\u0026#39;, \u0026#39;SourceAirportID\u0026#39;, \u0026#39;DestinationAirport\u0026#39;, \u0026#39;DestinationAirportID\u0026#39;, \u0026#39;CodeShare\u0026#39;,\u0026#39;Stops\u0026#39;,\u0026#39;Equipment\u0026#39;], na_values=\u0026#39;\\\\N\u0026#39;) display(routes) # Load the file with the airports airports = pd.read_csv(airports_dataset_url, names=[\u0026#39;AirportID\u0026#39;,\u0026#39;Name\u0026#39;,\u0026#39;City\u0026#39;,\u0026#39;Country\u0026#39;,\u0026#39;IATA\u0026#39;,\u0026#39;ICAO\u0026#39;, \u0026#39;Latitude\u0026#39;, \u0026#39;Longitude\u0026#39;, \u0026#39;Altitude\u0026#39;, \u0026#39;Timezone\u0026#39;, \u0026#39;DST\u0026#39;, \u0026#39;TzDatabaseTimeZone\u0026#39;, \u0026#39;Type\u0026#39;, \u0026#39;Source\u0026#39;], index_col=\u0026#39;AirportID\u0026#39;, na_values=\u0026#39;\\\\N\u0026#39;) display(airports) # Covid19 data covid_data = pd.read_csv(\u0026#39;covid19-global-forecasting-week-2/train.csv\u0026#39;,parse_dates=[\u0026#39;Date\u0026#39;], date_parser=lambda x: datetime.datetime.strptime(x, \u0026#39;%Y-%m-%d\u0026#39;)) Airline AirlineID SourceAirport SourceAirportID DestinationAirport DestinationAirportID CodeShare Stops Equipment 0 2B 410.0 AER 2965.0 KZN 2990.0 NaN 0 CR2 1 2B 410.0 ASF 2966.0 KZN 2990.0 NaN 0 CR2 2 2B 410.0 ASF 2966.0 MRV 2962.0 NaN 0 CR2 3 2B 410.0 CEK 2968.0 KZN 2990.0 NaN 0 CR2 4 2B 410.0 CEK 2968.0 OVB 4078.0 NaN 0 CR2 ... ... ... ... ... ... ... ... ... ... 67658 ZL 4178.0 WYA 6334.0 ADL 3341.0 NaN 0 SF3 67659 ZM 19016.0 DME 4029.0 FRU 2912.0 NaN 0 734 67660 ZM 19016.0 FRU 2912.0 DME 4029.0 NaN 0 734 67661 ZM 19016.0 FRU 2912.0 OSS 2913.0 NaN 0 734 67662 ZM 19016.0 OSS 2913.0 FRU 2912.0 NaN 0 734 67663 rows × 9 columns\nName City Country IATA ICAO Latitude Longitude Altitude Timezone DST TzDatabaseTimeZone Type Source AirportID 1 Goroka Airport Goroka Papua New Guinea GKA AYGA -6.081690 145.391998 5282 10.0 U Pacific/Port_Moresby airport OurAirports 2 Madang Airport Madang Papua New Guinea MAG AYMD -5.207080 145.789001 20 10.0 U Pacific/Port_Moresby airport OurAirports 3 Mount Hagen Kagamuga Airport Mount Hagen Papua New Guinea HGU AYMH -5.826790 144.296005 5388 10.0 U Pacific/Port_Moresby airport OurAirports 4 Nadzab Airport Nadzab Papua New Guinea LAE AYNZ -6.569803 146.725977 239 10.0 U Pacific/Port_Moresby airport OurAirports 5 Port Moresby Jacksons International Airport Port Moresby Papua New Guinea POM AYPY -9.443380 147.220001 146 10.0 U Pacific/Port_Moresby airport OurAirports ... ... ... ... ... ... ... ... ... ... ... ... ... ... 14106 Rogachyovo Air Base Belaya Russia NaN ULDA 71.616699 52.478298 272 NaN NaN NaN airport OurAirports 14107 Ulan-Ude East Airport Ulan Ude Russia NaN XIUW 51.849998 107.737999 1670 NaN NaN NaN airport OurAirports 14108 Krechevitsy Air Base Novgorod Russia NaN ULLK 58.625000 31.385000 85 NaN NaN NaN airport OurAirports 14109 Desierto de Atacama Airport Copiapo Chile CPO SCAT -27.261200 -70.779198 670 NaN NaN NaN airport OurAirports 14110 Melitopol Air Base Melitopol Ukraine NaN UKDM 46.880001 35.305000 0 NaN NaN NaN airport OurAirports 7698 rows × 13 columns\n# We compute some intermediate dataframes that will be useful later # Aggregate the number of flights per airport. # Source_airport -\u0026gt; destination_airport, number of flights flights_airport = routes.groupby([\u0026#39;SourceAirportID\u0026#39;,\u0026#39;DestinationAirportID\u0026#39;]).size().reset_index(name=\u0026#39;num_flights\u0026#39;) # Get the countries for the previous airport IDs flights_country=flights_airport.join(airports.Country,on=\u0026#39;SourceAirportID\u0026#39;,lsuffix=\u0026#39;Source\u0026#39;).\\ join(airports.Country,on=\u0026#39;DestinationAirportID\u0026#39;,lsuffix=\u0026#39;Destination\u0026#39;)[[\u0026#39;Country\u0026#39;,\u0026#39;CountryDestination\u0026#39;,\u0026#39;num_flights\u0026#39;]] # Aggregate the number of flights between countries # SourceCountry -\u0026gt; DestinationCountry, number of flights paths = flights_country.groupby([\u0026#39;Country\u0026#39;,\u0026#39;CountryDestination\u0026#39;]).sum() # Aggregate the number of flights to other countries. paths_other_countries = flights_country[flights_country.Country != flights_country.CountryDestination].groupby([\u0026#39;Country\u0026#39;,\u0026#39;CountryDestination\u0026#39;]).sum() # Total number of flights per country total_flights_country = paths.groupby([\u0026#39;Country\u0026#39;]).sum() # Total number of flights to other countries total_flights_other_countries = paths_other_countries.groupby([\u0026#39;Country\u0026#39;]).sum() Air routes visualization # The OpenFlights offers information about the flights between airports. Right now we are only interested in the number of flights between countries. This will simplify the visualization of the air traffic around the globle and it is enough to spot airflights hubs in the dataset.\nNote: generating the plot may take a while. However, we added a fancy loading bar :)\n# Some code to have a nice progress bar :) from IPython.display import clear_output def update_progress(progress): bar_length = 20 if isinstance(progress, int): progress = float(progress) if not isinstance(progress, float): progress = 0 if progress \u0026lt; 0: progress = 0 if progress \u0026gt;= 1: progress = 1 block = int(round(bar_length * progress)) clear_output(wait = True) text = \u0026#39;Progress: [{0}] {1:.1f}%\u0026#39;.format( \u0026#39;#\u0026#39; * block + \u0026#39;-\u0026#39; * (bar_length - block), progress * 100) print(text) fig = go.Figure() total_rows = float(len(paths_other_countries.index)) num_row=-1 for index, row in paths_other_countries.iterrows(): num_row = num_row + 1 update_progress(num_row/total_rows) fig.add_trace( go.Scattergeo( locationmode = \u0026#39;country names\u0026#39;, locations = index, mode = \u0026#39;lines\u0026#39;, hoverinfo= \u0026#39;skip\u0026#39;, line = dict(width=1,color=\u0026#39;red\u0026#39;), opacity = float(row[\u0026#39;num_flights\u0026#39;])/float(total_flights_other_countries.loc[index[0]]), showlegend=False, ) ) fig.update_layout( geo = dict( showcountries = True, ), showlegend = False, title = \u0026#39;Aggregated air routes between countries excluding domestic flights\u0026#39; ) fig.show(renderer=\u0026#39;svg\u0026#39;) Progress: [####################] 100.0% If the air traffic has something to do with the Covid19 spread then, China\u0026rsquo;s air traffic requires our attention. The figure below shows the routes between China and other countries excluding domestic flights. The wide of every line is representative of the number of flights between countries.\nExcluding domestic flights (which are quite a few), China has connections to 62 countries. Assuming daily flights, this means that an infected passenger can propagate the disease around the globe in a few hours.\naux = paths_other_countries.reset_index() aux = aux[aux.Country==\u0026#39;China\u0026#39;] fig = go.Figure() total_rows = float(len(aux.index)) num_row=-1 the_min = aux.num_flights.min() the_max = aux.num_flights.max() the_mean = aux.num_flights.mean() the_std = aux.num_flights.std() scalator = float(the_max - the_min) for index, row in aux.iterrows(): num_row = num_row + 1 update_progress(num_row/total_rows) width = ((float(row[\u0026#39;num_flights\u0026#39;])-the_min)/scalator)*15 #width = np.max([0.5, width]) fig.add_trace( go.Scattergeo( locationmode = \u0026#39;country names\u0026#39;, locations = [row.Country,row.CountryDestination], mode = \u0026#39;lines\u0026#39;, hoverinfo= \u0026#39;skip\u0026#39;, line = dict(width=width,color=\u0026#39;red\u0026#39;), showlegend=False, ) ) fig.update_layout( geo = dict( showcountries = True, ), showlegend = False, title = \u0026#39;China aggregated air routes between countries excluding domestic flights\u0026#39; ) fig.show(renderer=\u0026#39;svg\u0026#39;) Progress: [####################] 98.4% Disseminations with graphs # Now that we have a better understanding of our dataset and how does it look, it is time to prepare some artifacts to study the spread of the virus. In this case, we are going to translate our Pandas tables into a graph that represents the network of countries connected by their flights.\nWe can create a directed graph $G(V,E)$ where the set of vertices $V$ represents the countries and the set of edges $E$ represents existing air routes between countries $A$ and $B$. Additionally, we can set weights $W(E)$ with the number of flights between two countries.\nThere are some nice libraries to work with graphs in Python. However, I particularly like Graph Tool, maintained by Tiago de Paula Peixoto from the Central European University. It has implementations of some sophisticated algorithms done in C++ with OpenMP.\n# Run this to ensure that the drawing module is correctly stablished from graph_tool.all import * import graph_tool as gt g = gt.Graph() # Create the country property with the country names countries_prop = g.new_vertex_property(\u0026#39;string\u0026#39;) g.vertex_properties[\u0026#39;country\u0026#39;] = countries_prop # Total number of outgoing flights per vertex out_flights_prop = g.new_vertex_property(\u0026#39;int\u0026#39;) g.vertex_properties[\u0026#39;out_flights\u0026#39;] = out_flights_prop # Create edge property with the number of flights between countries num_flights_prop = g.new_edge_property(\u0026#39;double\u0026#39;) g.edge_properties[\u0026#39;num_flights\u0026#39;] = num_flights_prop # Get the complete list of countries list_countries = np.union1d(paths.reset_index().Country.unique(), paths.reset_index().CountryDestination.unique()) g.add_vertex(len(list_countries)) # For a given country, get the vertex country_vertex = {} index=0 #for c in total_flights_country.index: for c in list_countries: v = g.vertex(index) countries_prop[v] = c country_vertex[c] = v # skip self-loops try: out_flights_prop[v] = total_flights_other_countries.loc[c][\u0026#39;num_flights\u0026#39;] except: out_flights_prop[v] = 0.0 index=index+1 # Add the edges for index,num_flights in paths.iterrows(): s = country_vertex[index[0]] d = country_vertex[index[1]] # No self-loops if s != d: e = g.add_edge(s,d) num_flights_prop[e] = num_flights We draw our graph using a radial layout with China in the center. The edges correspond to the neighbors that can be reached in a single step from China. This is, the 62 countries with direct flights from China.\n# plot with a radial distribution with China in the center china_vertex = country_vertex[\u0026#39;China\u0026#39;] pos = gt.draw.radial_tree_layout(g,china_vertex) gv = gt.GraphView(g, efilt=lambda e : e.source()==china_vertex) p=gt.draw.graph_draw(gv,pos) Considering a dissemination scenario, we draw the edges for all the countries that can be reached after a scale in a flight from China. As you can see the number is particularly large.\nsecond_step = [] for v in g.get_out_neighbors(china_vertex): second_step.append(v) gv2 = gt.GraphView(g, efilt=lambda e : e.target!=china_vertex and e.source() in second_step) p=gt.draw.graph_draw(gv2,pos) The graph above is a bit misleading. Not all the vertices will be reached from the root vertex (China) with the same probability. In other words, the frequency of flights between countries will increase the probabilities to transfer an infected passenger from China.\nIf we consider the weight $W$ of every edge to be the number of outgoing flights following the edge direction, everything becomes more interesting. The figure below changes the width of every edge connecting China with its flight-connected neighbors. A few number of countries (Taiwan, South Korea, Japan) accumulates most of China\u0026rsquo;s outgoing traffic as the wide arrows show.\nNote: in the figure below the $W$ weight is normalized. Otherwise, most of the connections would not be plotted.\naux = gv.edge_properties[\u0026#39;num_flights\u0026#39;].copy() aux.a = (aux.a - aux.a.min())/(aux.a.max()-aux.a.min()) * 10 p=gt.draw.graph_draw(gv,pos,edge_pen_width=aux) SIR epidemic process # The SIR model is a simple, yet powerful, compartimental model for epidemics. The model consists of three compartments $S$, $I$ and $R$. $S$ is the number of susceptible, $I$ the number of infectuous and $R$ the number of recovered or deceased individuals. This is the most common model used right now and has many extensions and enhancements. The number of individuals in $S$ falls down as they become infected ($I$) and then recovered $R$. This is one of the famous infection curves used to forecast Covid19 evolution.\nHow to determine when a new individual moves between compartments depends on transition rates. Continuing with our initial idea about how air traffic can be an important actor in Covid19 dissemination, we can use our air flight connections graph to run a SIR simulation.\nFortunately, graph tool comes with an implementation of the SIR model easily configurable. We run a SIR model in 63 steps which is the number of days the Covid19 required to expand worldwide in our dataset. For every edge in our graph we compute $\\beta_e = \\sum_{J \\in out(A)} W(A,J)$ that basically sets the transition between vertices as the ratio of flights that connection represent among the total number of outgoing flights in vertex $A$. Additionally, we configure SIR to remove spontaneous infections and reinfections and set China as the dissemination seed.\ndef runSIR(state,num_iter=100): \u0026#39;\u0026#39;\u0026#39; For a graph tool epidemic model run the state and return a dataframe with the countries infected in every step \u0026#39;\u0026#39;\u0026#39; infections = pd.DataFrame() previous = state.get_state().fa initial = np.where(previous==1)[0] for t in range(num_iter): ret=state.iterate_sync() new = state.get_state().fa # Find new infected countries diff = new - previous already_infected = state.get_state().fa.sum() non_infected = len(state.get_state().fa)-already_infected new_infected = [countries_prop[g.vertex(v)] for v in np.where(diff==1)[0]] previous = new.copy() # collect the results in a Pandas friendly way infections = infections.append([{\u0026#39;step\u0026#39;:t,\u0026#39;norm_steps\u0026#39;:t/float(num_iter),\u0026#39;infected\u0026#39;:already_infected, \u0026#39;new_infected\u0026#39;: len(new_infected),\u0026#39;non_infected\u0026#39;: non_infected, \u0026#39;new_infected_countries\u0026#39;: new_infected}], ignore_index=True) return infections # Create an edge property map that can help us to define the beta probability between vertices beta_prop = g.new_edge_property(\u0026#39;double\u0026#39;) for e in g.edges(): beta_prop[e] = num_flights_prop[e]/out_flights_prop[e.source()] # Assume that vertices are not susceptible, except the seed. susceptible_prop = g.new_vertex_property(\u0026#39;float\u0026#39;) for v in g.vertices(): susceptible_prop[v] = 0.0 susceptible_prop[china_vertex]=1.0 state = gt.dynamics.SIRState(g, beta=beta_prop,v0 = china_vertex, constant_beta=True,gamma=0, r=0,s=susceptible_prop,epsilon=0,) infections_SIR = runSIR(state,64) display(infections_SIR[infections_SIR.new_infected\u0026gt;0].head(10)) step norm_steps infected new_infected non_infected new_infected_countries 1 1 0.015625 8 5 217 [Australia, India, Indonesia, Kyrgyzstan, Sout... 2 2 0.031250 13 5 212 [Israel, Russia, Saudi Arabia, Thailand, Unite... 3 3 0.046875 20 7 205 [Egypt, Hong Kong, Malaysia, Taiwan, Turkey, U... 4 4 0.062500 27 7 198 [Burma, France, Germany, Pakistan, Peru, Polan... 5 5 0.078125 36 9 189 [Austria, Belarus, Denmark, Kenya, New Zealand... 6 6 0.093750 50 14 175 [Argentina, Bosnia and Herzegovina, Brazil, Ca... 7 7 0.109375 59 9 166 [Ethiopia, Greece, Greenland, Italy, Libya, Ma... 8 8 0.125000 71 12 154 [Bangladesh, Belgium, Cambodia, Chile, Colombi... 9 9 0.140625 79 8 146 [Azerbaijan, Bahrain, Czech Republic, Jordan, ... 10 10 0.156250 86 7 139 [Dominican Republic, Ecuador, Iran, Lebanon, M... The output above shows the number of infected countries over time. Observe that it takes a while for the network to consider new infected countries.\nIt is interesting to compare our outcomes with the current Covid19 evolution per country. We consider in our dataset that a country is infected as soon as it confirms a single case. We assume that every day after the first case appeared in China can be considered as a step in the dissemination process. This assumption is done in order to facilitate the comparison with our SIR simulation.\n# Compute the number of infected countries # Get the first date with confirmed cases for every country first_date = covid_data[covid_data.ConfirmedCases \u0026gt; 0].groupby(\u0026#39;Country_Region\u0026#39;, as_index=False).Date.min().sort_values(by=\u0026#39;Date\u0026#39;) # Compute the number of elapsed days since the beginning of China\u0026#39;s outbreak first_date[\u0026#39;step\u0026#39;]=first_date.Date-datetime.datetime(2020,1,22) # Convert this column into an integer with the number of days. first_date.step = first_date.step.dt.days # Get the number of infected countries per region covid_infected = first_date[[\u0026#39;step\u0026#39;,\u0026#39;Country_Region\u0026#39;]].groupby(\u0026#39;step\u0026#39;).agg(new_infected=(\u0026#39;step\u0026#39;,\u0026#39;count\u0026#39;)) covid_infected[\u0026#39;infected\u0026#39;] = covid_infected[\u0026#39;new_infected\u0026#39;].cumsum() covid_infected = covid_infected.reset_index() f = go.Figure() f.add_trace( go.Scatter(x=covid_infected.step,y=covid_infected.infected,name=\u0026#39;Covid19\u0026#39;) ) f.add_trace( go.Scatter(x=infections_SIR.step,y=infections_SIR.infected,name=\u0026#39;SIR\u0026#39;) ) f.update_layout( title_text=\u0026#39;Comparing SIR infected countries evolution with original Covid19\u0026#39;, xaxis=dict(title=\u0026#39;Step\u0026#39;), yaxis=dict(title=\u0026#39;Infected countries\u0026#39;) ) f.show(renderer=\u0026#39;svg\u0026#39;) The original Covid19 evolution differs in the first steps with our SIR simulation. The initial plateau before the increasing slope, takes much longer in the Covid19 series. There is probably a lag between a country welcoming infected passengers and the declaration of that case. However, it is interesting to observe this difference between the number of declared cases and the evolution of our model. Obviously, there is some remaining exploration work before we can understanding why this is happening.\nConclusions # In this notebook we have introduced some interesting tools to analyze the influence of air flights in the dissemination of the Covid19 around the world. Firstly, we have explored the OpenFlights dataset containing air traffic information around the world. Second, we have used this dataset to create an air traffic network. Finally, we have used this network to run a SIR model to understand the dissemination of the virus using air connections.\nReferences # Tiago P. Peixoto, “The graph-tool python library”, figshare. (2014) DOI: 10.6084/m9.figshare.1164194 sci-hub\n","date":"17 April 2020","permalink":"/covid19-spreading-in-a-networking-model-part-i/","section":"","summary":"This is a reproduction of a Jupyter notebook you can find here\nIntroduction # The Covid19 has imposed a lot of pressure on epidemiologists to come up with models that can explain the impact of pandemics.","title":"Covid19 spreading in a networking model (part I)"},{"content":"","date":"17 April 2020","permalink":"/tags/networks/","section":"Tags","summary":"","title":"networks"},{"content":"","date":"17 April 2020","permalink":"/tags/pandas/","section":"Tags","summary":"","title":"pandas"},{"content":"Introduction # This is the third part of our series about Covid19 data analysis. This series is inspired after the proposed Kaggle challenge. This notebook introduces some practical examples of univariate time series forecasting.\nWe introduces the Python statsmodel module which provides a set of statistical tools for conducting statistical tests and data exploration. We focus on the time series analysis module offered in this package to run basic time series forecasting predicting the evolution of cases in the Covid19 pandemia.\nAssumptions\nYou succesfully run the Covid19 visualization notebook You have an already running Jupyter environment You are familiar with Pandas You have heard about Matplotlib The covid19 files are available in the path covid19-global-forecasting-week-2 Load the data # Load the CSV data using pandas. In this case, we use the read_csv function indicating the date parser to be used. Working with dates is going to be particularly useful when working with time series.\nimport pandas as pd import datetime data = pd.read_csv(\u0026#39;covid19-global-forecasting-week-2/train.csv\u0026#39;,parse_dates=[\u0026#39;Date\u0026#39;], date_parser=lambda x: datetime.datetime.strptime(x, \u0026#39;%Y-%m-%d\u0026#39;)) data Id Country_Region Province_State Date ConfirmedCases Fatalities 0 1 Afghanistan NaN 2020-01-22 0.0 0.0 1 2 Afghanistan NaN 2020-01-23 0.0 0.0 2 3 Afghanistan NaN 2020-01-24 0.0 0.0 3 4 Afghanistan NaN 2020-01-25 0.0 0.0 4 5 Afghanistan NaN 2020-01-26 0.0 0.0 ... ... ... ... ... ... ... 18811 29360 Zimbabwe NaN 2020-03-21 3.0 0.0 18812 29361 Zimbabwe NaN 2020-03-22 3.0 0.0 18813 29362 Zimbabwe NaN 2020-03-23 3.0 1.0 18814 29363 Zimbabwe NaN 2020-03-24 3.0 1.0 18815 29364 Zimbabwe NaN 2020-03-25 3.0 1.0 18816 rows × 6 columns\nChina was the origin of Covid19. Its curve of infection is the longest one among the curves available in the dataset. We are going to get a sample filtering all the data corresponding to China. Remember that in the case of China we have information about the evolution for every province. In order, to work with an aggregated value per country and day, we have to aggregate the data. Check the previous notebooks for more details about this operation.\nchina = data.where(data.Country_Region==\u0026#39;China\u0026#39;).groupby([\u0026#39;Date\u0026#39;],as_index=False).sum() china Date Id ConfirmedCases Fatalities 0 2020-01-22 214533.0 548.0 17.0 1 2020-01-23 214566.0 643.0 18.0 2 2020-01-24 214599.0 920.0 26.0 3 2020-01-25 214632.0 1406.0 42.0 4 2020-01-26 214665.0 2075.0 56.0 ... ... ... ... ... 59 2020-03-21 216480.0 81305.0 3259.0 60 2020-03-22 216513.0 81435.0 3274.0 61 2020-03-23 216546.0 81498.0 3274.0 62 2020-03-24 216579.0 81591.0 3281.0 63 2020-03-25 216612.0 81661.0 3285.0 64 rows × 4 columns\nimport matplotlib.pyplot as pyplot fig = pyplot.figure(figsize=(10,5)) ax1 = fig.add_subplot() china.ConfirmedCases.plot(ax=ax1,color=\u0026#39;blue\u0026#39;,legend=True, title=\u0026#39;China-Covid19\u0026#39;) china.Fatalities.plot(ax=ax1,color=\u0026#39;red\u0026#39;,legend=True) pyplot.show() Univariate time series analysis # The plot above shows the evolution of confirmed cases and fatalities for China during the Covid19 crisis. This seems the common profile of any pandemic. First, the number of cases increases until it reaches a plateau. Then, it stays stable until the cases decrease with an increment in the number of fatalities.\nOne of the most important problems to be resolved in this case, like in any other time series is: can we forecast the number of new cases and/or fatalities with some accuracy?\nWe start working with the China time series. Before starting we need to make some preparation to work with datetime indices. These are common operations in Pandas. We remove the Id column and convert the Date column into our new row index. Additionally, we set the frequency of the index to days by executing china.index.freq='d'. This is particularly helpful when using Pandas time series.\nchina = china.set_index(\u0026#39;Date\u0026#39;) china.drop([\u0026#39;Id\u0026#39;], axis=1, inplace=True) # The index is a time series with a frequency of days china.index.freq=\u0026#39;d\u0026#39; Simple exponential smoothing # If we try to predict any future value in an univariate time series, the first thing we have to consider is the fact that future observations depend on past and present observations. If $\\hat{y}t$ is the forecast of variable $x$ at time $t$ and $\\hat{y}{t+h|t}$ the forecast of variable $x$ at time $t+h$ given time $t$, a forecast equation could be something like this.\n$$ \\begin{align} \\hat{y}_{t+h|t} = l_t \\end{align} $$\nWhich basically means that our predictions at time $t+h$ given observations at time $t$ are the result of a function $l$ at time $t$. And what is this function? We are going to use a simple exponential smoothing (SES) function.\n$$ \\begin{align} l_1 \u0026amp;= \\alpha y_1 + (1- \\alpha) l_{0} \\end{align} $$\nWhich is a formal way of saying that the current value depends on the previous value multiplied by a smoothing factor $ \\alpha $. If we continue developing the equation we have\n$$ \\begin{align*} l_1 \u0026amp;=\\alpha y_1 + (1-\\alpha) l_{0} \\ l_2 \u0026amp;=\\alpha y_2 + (1-\\alpha) l_{1} = \\alpha y_2 + \\alpha(1-\\alpha) y_1 + (1-\\alpha)^2 l_{0} \\ l_3 \u0026amp;=\\alpha y_3 + (1-\\alpha) l_{2} = \\sum_{j=0}^{2}\\alpha(1-\\alpha)^jy_{3-j}+(1-\\alpha)^3l_{0} \\ \\vdots \\ l_t \u0026amp;= \\sum_{j=0}^{t-1}\\alpha(1-\\alpha)^j y_{t-j}+(1-\\alpha)^tl_0 \\end{align*} $$\nTherefore, the forecasting equation is:\n$$ \\begin{align*} \\hat{y}{t+h|t} \u0026amp;= \\sum{j=1}^{t}\\alpha(1-\\alpha)^{t-j} y_{j}+(1-\\alpha)^tl_0 \\end{align*} $$\nThe $\\alpha$ value should be chosen to minimize the mean squared error (MSE)\n$$ \\begin{align*} MSE = \\frac{1}{T} \\sum_{t=1}^{T} (y_t+\\hat{y}_{t|t-1})^2 \\end{align*} $$\nThere is no closed solution to find $\\alpha$. Least squares can be a simple and valid method to set the value.\nWe are going to use the implementation available at the statsmodels library to make some predictions for the number of confirmed cases in China starting at 2020-03-20.\nfrom statsmodels.tsa.holtwinters import SimpleExpSmoothing # We previously set dates as the row index. Now we can use indexes to select rows. china_train = china[:\u0026#39;2020-03-20\u0026#39;] china_test = china[\u0026#39;2020-03-21\u0026#39;:] # Chose a model for confirmed cases model = SimpleExpSmoothing(china_train.ConfirmedCases) # Fit the model for the given observations model_fit = model.fit() We have fit a model using SES and the number of confirmed cases. It is extremely useful to check what are the params for this model and understand how well our fitting process did.\n# Check the params for the model display(model_fit.params) # Lets observe a summary about the fitting model_fit.summary() {'smoothing_level': 1.0, 'smoothing_slope': nan, 'smoothing_seasonal': nan, 'damping_slope': nan, 'initial_level': 548.0, 'initial_slope': nan, 'initial_seasons': array([], dtype=float64), 'use_boxcox': False, 'lamda': None, 'remove_bias': False} SimpleExpSmoothing Model Results Dep. Variable: endog No. Observations: 59 Model: SimpleExpSmoothing SSE 421717800.000 Optimized: True AIC 935.156 Trend: None BIC 939.311 Seasonal: None AICC 935.897 Seasonal Periods: None Date: Fri, 03 Apr 2020 Box-Cox: False Time: 20:51:35 Box-Cox Coeff.: None coeff code optimized smoothing_level 1.0000000 alpha True initial_level 548.00000 l.0 True We can set the $\\alpha$ parameter by setting the smoothing_level value in the model.fit call. However, if we do not set any value, the library will find it for us. In this case $\\alpha$ was automatically set to 1 with $l_0=548$ (initial_level in the summary table).\nNow we are going to fit some models manually setting the $\\alpha$ parameter and compare the obtained output.\nimport matplotlib.pyplot # Fit with some alphas fit025 = model.fit(smoothing_level=0.25) fit05 = model.fit(smoothing_level=0.5) fit075 = model.fit(smoothing_level=0.75) ax = china_train.ConfirmedCases.plot(color=\u0026#39;black\u0026#39;, legend=True) model_fit.fittedvalues.plot(ax=ax, color=\u0026#39;blue\u0026#39;, style=\u0026#39;--\u0026#39;, legend=True, label=r\u0026#39;Auto $\\alpha=%s$\u0026#39;%model_fit.params[\u0026#39;smoothing_level\u0026#39;]) fit025.fittedvalues.plot(ax=ax, color=\u0026#39;red\u0026#39;, style=\u0026#39;--\u0026#39;, legend=True, label=r\u0026#39;$\\alpha=0.25$\u0026#39; ) fit05.fittedvalues.plot(ax=ax, color=\u0026#39;green\u0026#39;, style=\u0026#39;--\u0026#39;, legend=True, label=r\u0026#39;$\\alpha=0.5$\u0026#39;) fit075.fittedvalues.plot(ax=ax, color=\u0026#39;grey\u0026#39;, style=\u0026#39;--\u0026#39;, legend=True, label=r\u0026#39;$\\alpha=0.75$\u0026#39; ) pyplot.show() The plot above shows that $\\alpha=1$ gives the best approximation to the curve. Now that we know that $\\alpha=1$ brings the best performance, we can think about forecasting with that model.\n# Lets make some predictions predicted = model_fit.predict(start=\u0026#39;2020-03-21\u0026#39;,end=\u0026#39;2020-03-25\u0026#39;).rename(\u0026#39;Predicted ConfirmedCases\u0026#39;) ax = china_train.ConfirmedCases.plot(color=\u0026#39;black\u0026#39;, legend=True) china_test.ConfirmedCases.plot(ax = ax, color=\u0026#39;blue\u0026#39;, legend=False) predicted.plot(ax=ax, color=\u0026#39;blue\u0026#39;, style=\u0026#39;--\u0026#39;, legend=True) pyplot.show() The plot the whole time series and a five days prediction. We better use some zoom.\nax = china_train.ConfirmedCases[\u0026#39;2020-03-15\u0026#39;:\u0026#39;2020-03-25\u0026#39;].plot(color=\u0026#39;black\u0026#39;,legend=True) china_test.ConfirmedCases.plot(ax = ax, color=\u0026#39;blue\u0026#39;, legend=True, label=\u0026#39;ConfirmedCases to predict\u0026#39;) predicted.plot(ax=ax, color=\u0026#39;blue\u0026#39;, style=\u0026#39;--\u0026#39;, legend=True) pyplot.show() Observe that the iterative prediction always returns the same value. This is because this function is a flat forecast function. There is no trend, seasonality or other value that can make this prediction to evolve. So finally, what we have is the mean of a future:\n$$ \\begin{align*} \\hat{y}{T+h|T} = \\hat{y}{T+1|T}, \\space h=2,3,\u0026hellip; \\end{align*} $$\nHolt-Winters # The flat nature of multiple steps forecasting of SES is not a good solution if we need a prediction at $T+h$. However, there are other approaches which basically include additional elemnts such as seasonality or trend. For example, the Holt\u0026rsquo;s local trend method adds a $\\beta^*$ factor that controls the trend:\n$$ \\begin{align*} Forecast: \u0026amp;\u0026amp; \\hat{y}{t+h|t} \u0026amp;= l_t + hb_t \\ Level: \u0026amp;\u0026amp; l_t \u0026amp;= \\alpha y_t + (1-\\alpha)(l{t-1}+b_{t-1}) \\ Trend: \u0026amp;\u0026amp; b_t \u0026amp;= \\beta^(l_t-l_{t-1})+(1-\\beta^)b_{t-1} \\end{align*} $$\nIn this approach, $l_t$ estimates the level while $bt$ estimates the slope. The extension of the formula requires to estimate $\\alpha$, $\\beta=\\alpha\\beta^*$,$l_0$ and $b_0$.\nLet\u0026rsquo;s compare the forecast obtained from Holt-Winters for the previous scenario.\nfrom statsmodels.tsa.holtwinters import Holt # Fit a Holt-Winters model holt_model = Holt(china_train.ConfirmedCases) holt_fitted = holt_model.fit() holt_fitted.summary() Holt Model Results Dep. Variable: endog No. Observations: 59 Model: Holt SSE 243476583.261 Optimized: True AIC 906.747 Trend: Additive BIC 915.057 Seasonal: None AICC 908.362 Seasonal Periods: None Date: Fri, 03 Apr 2020 Box-Cox: False Time: 20:51:36 Box-Cox Coeff.: None coeff code optimized smoothing_level 1.0000000 alpha True smoothing_slope 0.2715660 beta True initial_level 398.89608 l.0 True initial_slope 657.75311 b.0 True holt_predicted = holt_fitted.predict(start=\u0026#39;2020-03-21\u0026#39;, end=\u0026#39;2020-03-25\u0026#39;).rename(\u0026#39;Predicted ConfirmedCases\u0026#39;) fig = pyplot.figure(figsize=(15,10)) ax1 = fig.add_subplot(2,2,1) china_train.ConfirmedCases[\u0026#39;2020-03-15\u0026#39;:\u0026#39;2020-03-25\u0026#39;].plot(ax=ax1,color=\u0026#39;black\u0026#39;,legend=True, title=\u0026#39;Simple Exponential Smoothing Excerpt\u0026#39;) china_test.ConfirmedCases.plot(ax = ax1, color=\u0026#39;blue\u0026#39;, legend=True, label=\u0026#39;ConfirmedCases to predict\u0026#39;) predicted.plot(ax=ax1, color=\u0026#39;blue\u0026#39;, style=\u0026#39;--\u0026#39;, legend=True) ax2 = fig.add_subplot(2,2,2) china_train.ConfirmedCases[\u0026#39;2020-03-15\u0026#39;:\u0026#39;2020-03-25\u0026#39;].plot(ax=ax2, color=\u0026#39;black\u0026#39;,legend=True, title=\u0026#39;Holt Winters Excerpt\u0026#39;) china_test.ConfirmedCases.plot(ax = ax2, color=\u0026#39;blue\u0026#39;, legend=True, label=\u0026#39;ConfirmedCases to predict\u0026#39;) holt_predicted.plot(ax=ax2, color=\u0026#39;blue\u0026#39;, style=\u0026#39;--\u0026#39;, legend=True) ax4 = fig.add_subplot(2,2,3) china_train.ConfirmedCases.plot(ax=ax4, color=\u0026#39;black\u0026#39;,legend=True, title=\u0026#39;Simple Exponential Smoothing\u0026#39;) model_fit.fittedvalues.plot(ax=ax4, color=\u0026#39;blue\u0026#39;, style=\u0026#39;--\u0026#39;, legend=True, label=\u0026#39;Predicted ConfirmedCases\u0026#39;) ax3 = fig.add_subplot(2,2,4) china_train.ConfirmedCases.plot(ax=ax3, color=\u0026#39;black\u0026#39;,legend=True, title=\u0026#39;Holt Winters\u0026#39;) holt_fitted.fittedvalues.plot(ax=ax3, color=\u0026#39;blue\u0026#39;, style=\u0026#39;--\u0026#39;, legend=True, label=\u0026#39;Predicted ConfirmedCases\u0026#39;) pyplot.tight_layout() pyplot.show() Comparing SES with the Holt Winters predictions there are clear differences. As we mentioned before, the prediction for the next five days is flat in the first case. Holt Winters uses its trend component to predict an increment in the number of cases.\nWhen comparing how both models fit the time series, SES reacts with some latency returning an approximation with some lag. Holt Winters reacts earlier by capturing the trend. However, it reacts with some latency when the trend changes. If we compare the residuals for both models we notice that Holt Winters performs better than SES.\nax=model_fit.resid.plot(color=\u0026#39;blue\u0026#39;, style=\u0026#39;--\u0026#39;, legend=True, label=\u0026#39;SES residuals\u0026#39;, title=\u0026#39;Comparison of residuals\u0026#39;) holt_fitted.resid.plot(ax=ax, color=\u0026#39;red\u0026#39;, legend=True, label=\u0026#39;Holt Winters residuals\u0026#39;) pyplot.show() At the moment of writing this notebook the most complete Covid-19 time series corresponds to China. However, we can check the quality of the fitting models for every country in the dataset. We can expect that any Holt Winters model will return better predictions than SES. For the sake of exhaustiveness we compare the performance of SES and Holt Winters predicting the number of confirmed cases and fatilities for every country in the dataset. The range of observed values for every country in the dataset may dramatically vary. This makes useless the comparison of RMSE values. We employ the Normalized Squared Error (NRMSE) which normalizes the RMSE using the max and min observed values. This is, $\\mathit{NRMSE} = \\frac{\\mathit{RMSE}}{y_{max}-y_{min}}$.\nNote: Observations may dramatically vary between countries. We ignore countries with no fatalities nor confirmed cases.\n# Accumulate observations per country and date. This is required to aggregate observations for multiple provinces # in the same country. data=data.groupby([\u0026#39;Country_Region\u0026#39;,\u0026#39;Date\u0026#39;],as_index=False,).sum() display(data) Country_Region Date Id ConfirmedCases Fatalities 0 Afghanistan 2020-01-22 1 0.0 0.0 1 Afghanistan 2020-01-23 2 0.0 0.0 2 Afghanistan 2020-01-24 3 0.0 0.0 3 Afghanistan 2020-01-25 4 0.0 0.0 4 Afghanistan 2020-01-26 5 0.0 0.0 ... ... ... ... ... ... 11067 Zimbabwe 2020-03-21 29360 3.0 0.0 11068 Zimbabwe 2020-03-22 29361 3.0 0.0 11069 Zimbabwe 2020-03-23 29362 3.0 1.0 11070 Zimbabwe 2020-03-24 29363 3.0 1.0 11071 Zimbabwe 2020-03-25 29364 3.0 1.0 11072 rows × 5 columns\nimport numpy as np def computeErrors(ymax,ymin,fitted): \u0026#39;\u0026#39;\u0026#39; Compute the normalized RMSE for this model $$ \\mathit{NRMSE} = \\frac{\\mathit{RMSE}}{y_{max}-y_{min}} $$ \u0026#39;\u0026#39;\u0026#39; RMSE = np.sqrt(np.sum(np.power(fitted.resid,2))/len(fitted.resid)) NRMSE = RMSE / (ymax-ymin) return NRMSE # Fit a model for every country # Get the list of all the available countries countries = data.Country_Region.unique() nrmse = pd.DataFrame(columns=[\u0026#39;SES-Confirmed\u0026#39;,\u0026#39;SES-Fatalities\u0026#39;,\u0026#39;HoltWinters-Confirmed\u0026#39;,\u0026#39;HoltWinters-Fatalities\u0026#39;], index=countries) # Because we are blindly computing a large number of models, some of them may result in convergence problems. # We ignore them. import warnings warnings.filterwarnings(\u0026#39;ignore\u0026#39;) for country in countries: excerpt = data[data.Country_Region==country] maxConfirmed = excerpt.ConfirmedCases.max() minConfirmed = excerpt.ConfirmedCases.min() maxFatalities = excerpt.Fatalities.max() minFatalities = excerpt.Fatalities.min() # Skip it if we have no relevant data if maxConfirmed == 0 or maxFatalities==0: print(\u0026#39;%s skipped because there is no relevant data\u0026#39;%(country)) next else: print(\u0026#39;%s processing\u0026#39;%(country)) # Models with ses ses_confirmed = SimpleExpSmoothing(excerpt.ConfirmedCases) ses_fatalities = SimpleExpSmoothing(excerpt.Fatalities) # Models with Holt Winters hw_confirmed = Holt(excerpt.ConfirmedCases) hw_fatalities = Holt(excerpt.Fatalities) # Fit models ses_confirmed_fit = ses_confirmed.fit() ses_fatalities_fit = ses_fatalities.fit() hw_confirmed_fit = hw_confirmed.fit() hw_fatalities_fit = hw_fatalities.fit() # Fill our dataframe nrmse.loc[country]={ \u0026#39;SES-Confirmed\u0026#39;: computeErrors(maxConfirmed,minConfirmed,ses_confirmed_fit), \u0026#39;SES-Fatalities\u0026#39;: computeErrors(maxFatalities,minFatalities,ses_fatalities_fit), \u0026#39;HoltWinters-Confirmed\u0026#39;: computeErrors(maxConfirmed,minConfirmed,hw_confirmed_fit), \u0026#39;HoltWinters-Fatalities\u0026#39;: computeErrors(maxFatalities,minFatalities,hw_fatalities_fit) } nrmse Afghanistan processing Albania processing Algeria processing Andorra processing Angola skipped because there is no relevant data Antigua and Barbuda skipped because there is no relevant data Argentina processing Armenia skipped because there is no relevant data Australia processing Austria processing Azerbaijan processing Bahamas skipped because there is no relevant data Bahrain processing Bangladesh processing Barbados skipped because there is no relevant data Belarus skipped because there is no relevant data Belgium processing Belize skipped because there is no relevant data Benin skipped because there is no relevant data Bhutan skipped because there is no relevant data Bolivia skipped because there is no relevant data Bosnia and Herzegovina processing Brazil processing Brunei skipped because there is no relevant data Bulgaria processing Burkina Faso processing Cabo Verde processing Cambodia skipped because there is no relevant data Cameroon processing Canada processing Central African Republic skipped because there is no relevant data Chad skipped because there is no relevant data Chile processing China processing Colombia processing Congo (Brazzaville) skipped because there is no relevant data Congo (Kinshasa) processing Costa Rica processing Cote d'Ivoire skipped because there is no relevant data Croatia processing Cuba processing Cyprus processing Czechia processing Denmark processing Diamond Princess processing Djibouti skipped because there is no relevant data Dominica skipped because there is no relevant data Dominican Republic processing Ecuador processing Egypt processing El Salvador skipped because there is no relevant data Equatorial Guinea skipped because there is no relevant data Eritrea skipped because there is no relevant data Estonia processing Eswatini skipped because there is no relevant data Ethiopia skipped because there is no relevant data Fiji skipped because there is no relevant data Finland processing France processing Gabon processing Gambia processing Georgia skipped because there is no relevant data Germany processing Ghana processing Greece processing Grenada skipped because there is no relevant data Guatemala processing Guinea skipped because there is no relevant data Guinea-Bissau skipped because there is no relevant data Guyana processing Haiti skipped because there is no relevant data Holy See skipped because there is no relevant data Honduras skipped because there is no relevant data Hungary processing Iceland processing India processing Indonesia processing Iran processing Iraq processing Ireland processing Israel processing Italy processing Jamaica processing Japan processing Jordan skipped because there is no relevant data Kazakhstan processing Kenya skipped because there is no relevant data Korea, South processing Kuwait skipped because there is no relevant data Kyrgyzstan skipped because there is no relevant data Laos skipped because there is no relevant data Latvia skipped because there is no relevant data Lebanon processing Liberia skipped because there is no relevant data Libya skipped because there is no relevant data Liechtenstein skipped because there is no relevant data Lithuania processing Luxembourg processing Madagascar skipped because there is no relevant data Malaysia processing Maldives skipped because there is no relevant data Mali skipped because there is no relevant data Malta skipped because there is no relevant data Mauritania skipped because there is no relevant data Mauritius processing Mexico processing Moldova processing Monaco skipped because there is no relevant data Mongolia skipped because there is no relevant data Montenegro processing Morocco processing Mozambique skipped because there is no relevant data Namibia skipped because there is no relevant data Nepal skipped because there is no relevant data Netherlands processing New Zealand skipped because there is no relevant data Nicaragua skipped because there is no relevant data Niger processing Nigeria processing North Macedonia processing Norway processing Oman skipped because there is no relevant data Pakistan processing Panama processing Papua New Guinea skipped because there is no relevant data Paraguay processing Peru processing Philippines processing Poland processing Portugal processing Qatar skipped because there is no relevant data Romania processing Russia processing Rwanda skipped because there is no relevant data Saint Kitts and Nevis skipped because there is no relevant data Saint Lucia skipped because there is no relevant data Saint Vincent and the Grenadines skipped because there is no relevant data San Marino processing Saudi Arabia processing Senegal skipped because there is no relevant data Serbia processing Seychelles skipped because there is no relevant data Singapore processing Slovakia processing Slovenia processing Somalia skipped because there is no relevant data South Africa skipped because there is no relevant data Spain processing Sri Lanka skipped because there is no relevant data Sudan processing Suriname skipped because there is no relevant data Sweden processing Switzerland processing Syria skipped because there is no relevant data Taiwan* processing Tanzania skipped because there is no relevant data Thailand processing Timor-Leste skipped because there is no relevant data Togo skipped because there is no relevant data Trinidad and Tobago processing Tunisia processing Turkey processing US processing Uganda skipped because there is no relevant data Ukraine processing United Arab Emirates processing United Kingdom processing Uruguay skipped because there is no relevant data Uzbekistan skipped because there is no relevant data Venezuela skipped because there is no relevant data Vietnam skipped because there is no relevant data Zambia skipped because there is no relevant data Zimbabwe processing SES-Confirmed SES-Fatalities HoltWinters-Confirmed HoltWinters-Fatalities Afghanistan 0.0594866 0.0883883 0.0482386 0.0761006 Albania 0.0366054 0.0661438 0.0148385 0.062477 Algeria 0.0423765 0.0416667 0.0244344 0.0277485 Andorra 0.0459787 0.125 0.0268921 0.124003 Angola 0.0721688 NaN 0.066931 inf ... ... ... ... ... Uzbekistan 0.0444878 NaN 0.0276604 inf Venezuela 0.0504516 NaN 0.0391757 inf Vietnam 0.031406 NaN 0.0188569 inf Zambia 0.0966002 NaN 0.0899481 inf Zimbabwe 0.0931695 0.125 0.0918469 0.124003 173 rows × 4 columns\nNow we have a dataframe named nrmse with the NRMSE values obtained fitting SES and Holt Winters models for confirmed cases and fatalities. We create two barplots to visually compare the NRMSE obtained in both cases. The lower the NRMSE, the better.\ntoPlotConfirmed = pd.DataFrame({\u0026#39;SES\u0026#39;:nrmse[\u0026#39;SES-Confirmed\u0026#39;], \u0026#39;HW\u0026#39;:nrmse[\u0026#39;HoltWinters-Confirmed\u0026#39;]}) toPlotFatalities = pd.DataFrame({\u0026#39;SES\u0026#39;:nrmse[\u0026#39;SES-Fatalities\u0026#39;], \u0026#39;HW\u0026#39;:nrmse[\u0026#39;HoltWinters-Fatalities\u0026#39;]}) fig = pyplot.figure(figsize=(15,30)) ax1 = fig.add_subplot(1,2,1) toPlotConfirmed.plot.barh(ax=ax1,legend=True,title=\u0026#39;Comparison of NRMSE for confirmed cases\u0026#39;) ax2 = fig.add_subplot(1,2,2) toPlotFatalities.plot.barh(ax=ax2,legend=True,title=\u0026#39;Comparison of NRMSE for fatalities\u0026#39;) ax2.legend(loc=\u0026#39;upper right\u0026#39;) pyplot.show() Conclusions # In this notebook, we have introduced the problem of univariate time series forecasting. In particular, we have proposed to forecast the evolution of the Covid-19 time series. We have run a coarse grain analysis with two prediction methods (Simple Exponential Smoothing and Holt Winters) and compared the results. We cannot claim any of these methods to be powerful enough to precisely run predictions given the complexity and implications of the Covid-19 problem. However, this exercise is a good starter for any discussion about time series, pandemics and predictors. There is a vast literature about this topic, in particular pandemics, and this notebook does not pretend to be a contribution.\nReferences # I strongly suggest to visit professor Rob J. Hyndman web site for a full list of content about time series. I have followed the notation mentioned here.\n","date":"4 April 2020","permalink":"/covid-19-forecasting/","section":"","summary":"Introduction # This is the third part of our series about Covid19 data analysis. This series is inspired after the proposed Kaggle challenge. This notebook introduces some practical examples of univariate time series forecasting.","title":"Covid-19 forecasting"},{"content":"","date":"4 April 2020","permalink":"/tags/forecasting/","section":"Tags","summary":"","title":"forecasting"},{"content":"","date":"4 April 2020","permalink":"/tags/matplotlib/","section":"Tags","summary":"","title":"matplotlib"},{"content":"","date":"4 April 2020","permalink":"/tags/statsmodels/","section":"Tags","summary":"","title":"statsmodels"},{"content":"","date":"4 April 2020","permalink":"/tags/time-series/","section":"Tags","summary":"","title":"time series"},{"content":"\nThis is the continuation of my series about how to manipulate Covid-19 data with Python (first entry here). This post adds some extra complexity and introduces some interesting options to define interactive plots.\nWe explore the generation of plots using Matplotlib and Plotly with Jupyter. First, we generate standard plots that permit a small level of interaction using zoom and scroll inside the plot box. Second, we generate an interactive plot with Plotly and ipywidgets where we can select the number of confirmed cases and fatalities for any country in the dataset. This is a useful example that let you understand the internals of the process.\nAs usual, I provide you with the Jupyter Notebook. You can find it at my Github repo. The last example from this notebook requires a running backend with Python. For this reason, the rendered HTML you find here, cannot display the last example. To facilitate the access to the code, you can find an interactive Binder image for you to play with the notebook by clicking the button below. I hope you find it useful.\nIntroduction # No need to say that the Covid19 crisis is a global challenge that is going to change how we see the world. There is a lot of interest in understanding the internals of virus propagation and several disciplines can be really helpful in this task. There is a lot of data going around and we have really accessible tools to work with this data.\nFor any data scientist this is a nice opportunity to explore and understand time series, graph theory and other fascinating disciplines. If you are just a newbie or a consolidated practitioner, I have decided to share a series of Jupyter Notebooks with some examples of tools and methods that you can find helpful. I will make my best to make all the code available.\nKaggle has opened a challenge to forecast the propagation of the virus. You can check the challenge with more details at the Kaggle site here. I invite you to check the notebooks uploaded by the community. I have not considered to participate in the challenge, but this could be a good opportunity if you plan to start with these kind of challenges.\nIn this part, I will use Kaggle data to show how we can visualize the virus evolution in different manners. You can download the data (after registration) here. After downloading the zip file with the dataset we have three CSV files:\ntrain.csv test.csv submission.csv For this exercise we will only use the train.csv file.\nAssumptions\nYou have an already running Jupyter environment You are familiar with Pandas You have heard about Matplotlib The covid19 files are available in the path covid19-global-forecasting-week-2 Loading a CSV with Pandas # There are several solutions to read CSV files in Python. However, with no disussion Pandas is the most suitable option for many scenarios. We import the pandas library and read the csv file with all the training data.\nimport pandas as pd data = pd.read_csv(\u0026#34;covid19-global-forecasting-week-2/train.csv\u0026#34;) data Id Country_Region Province_State Date ConfirmedCases Fatalities 0 1 Afghanistan NaN 2020-01-22 0.0 0.0 1 2 Afghanistan NaN 2020-01-23 0.0 0.0 2 3 Afghanistan NaN 2020-01-24 0.0 0.0 3 4 Afghanistan NaN 2020-01-25 0.0 0.0 4 5 Afghanistan NaN 2020-01-26 0.0 0.0 ... ... ... ... ... ... ... 18811 29360 Zimbabwe NaN 2020-03-21 3.0 0.0 18812 29361 Zimbabwe NaN 2020-03-22 3.0 0.0 18813 29362 Zimbabwe NaN 2020-03-23 3.0 1.0 18814 29363 Zimbabwe NaN 2020-03-24 3.0 1.0 18815 29364 Zimbabwe NaN 2020-03-25 3.0 1.0 18816 rows × 6 columns\nWe have a six columns dataframe indicating the country, state, date, number of confirmed cases and number of fatalities. We are going to focus on one country. Let\u0026rsquo;s say Spain.\nspain = data[data[\u0026#39;Country_Region\u0026#39;]==\u0026#39;Spain\u0026#39;] spain Id Country_Region Province_State Date ConfirmedCases Fatalities 13376 20901 Spain NaN 2020-01-22 0.0 0.0 13377 20902 Spain NaN 2020-01-23 0.0 0.0 13378 20903 Spain NaN 2020-01-24 0.0 0.0 13379 20904 Spain NaN 2020-01-25 0.0 0.0 13380 20905 Spain NaN 2020-01-26 0.0 0.0 ... ... ... ... ... ... ... 13435 20960 Spain NaN 2020-03-21 25374.0 1375.0 13436 20961 Spain NaN 2020-03-22 28768.0 1772.0 13437 20962 Spain NaN 2020-03-23 35136.0 2311.0 13438 20963 Spain NaN 2020-03-24 39885.0 2808.0 13439 20964 Spain NaN 2020-03-25 49515.0 3647.0 64 rows × 6 columns\nWe have data for 64 days with no information at a province/state level. Now we would like to have a visual representation of the time series.\nMatplotlib # The first solution to be considered is Pyplot from the Matplotlib library.\nfrom matplotlib import pyplot pyplot.plot(spain.ConfirmedCases) pyplot.title(\u0026#39;Confirmed cases in Spain\u0026#39;) pyplot.show() The figure above is the representation of the number of confirmed cases in Spain until March 26th. We have not set the X axis, so pyplot is considering the id column defined by Pandas. To define a more reasonable X ticks we simply pass a list with the same number of items of the Y axis starting from zero.\npyplot.plot(range(0,spain.ConfirmedCases.size),spain.ConfirmedCases) pyplot.title(\u0026#39;Confirmed cases in Spain\u0026#39;) pyplot.show() Now we have a clearer view of the X axis. However, we would like to have a comparison of the number of fatalities vs the number of confirmed cases.\npyplot.plot(range(0,spain.ConfirmedCases.size),spain.ConfirmedCases,label=\u0026#39;ConfirmedCases\u0026#39;) pyplot.plot(range(0,spain.Fatalities.size),spain.Fatalities,label=\u0026#39;Fatalities\u0026#39;) pyplot.legend() pyplot.title(\u0026#39;Confirmed cases vs fatalities in Spain\u0026#39;) pyplot.show() The increment shows an exponential behaviour. A logarithmic scale would help a better view.\npyplot.plot(range(0,spain.ConfirmedCases.size),spain.ConfirmedCases,label=\u0026#39;ConfirmedCases\u0026#39;) pyplot.plot(range(0,spain.Fatalities.size),spain.Fatalities,label=\u0026#39;Fatalities\u0026#39;) pyplot.yscale(\u0026#39;log\u0026#39;) pyplot.title(\u0026#39;Confirmed cases vs fatalities in Spain log scale\u0026#39;) pyplot.legend() pyplot.show() What about displaying the date in the X axis? To do that we need pyplot to format the x axis. This requires datetime structures to set the datetime of every observation. We already have them in the Date column. The main difference is setting the formatter for the x axis using mdates from matplotlib.\nimport matplotlib.dates as mdates # convert date strings to datenums dates = mdates.datestr2num(spain.Date) pyplot.gca().xaxis.set_major_formatter(mdates.DateFormatter(\u0026#39;%Y-%m-%d\u0026#39;)) pyplot.gca().xaxis.set_major_locator(mdates.DayLocator(interval=5)) pyplot.plot(dates,spain.ConfirmedCases,label=\u0026#39;confirmed\u0026#39;) pyplot.plot(dates,spain.Fatalities,label=\u0026#39;fatalities\u0026#39;) pyplot.title(\u0026#39;Confirmed cases vs fatalities in Spain with datetime in x axis\u0026#39;) pyplot.legend() pyplot.gcf().autofmt_xdate() pyplot.show() Seaborn # For those familiar with ggplot, Seaborn will look familiar. Seaborn is built on top of Matplotlib and offers a high level interface for drawing statistical graphics. It is particularly suitable to used in conjunction with Pandas.\nWe can replicate some of the plots above:\nimport seaborn as sns g = sns.relplot(x=range(spain.Date.size),y=\u0026#39;ConfirmedCases\u0026#39;, data=spain,kind=\u0026#39;line\u0026#39;,) g.set_axis_labels(x_var=\u0026#39;\u0026#39;) # I remove the xlabel for consistency with the previous plot pyplot.title(\u0026#39;Confirmed cases in Spain\u0026#39;) pyplot.show() To set the x axis with datetimes we do the same we did with matplotlib. However, now we are going to directly transform the Date column from the Pandas Dataframe so we can directly call seaborn to use it.\n# Transform the Date column to matplotlib datenum spain.Date = spain.Date.apply(lambda x : mdates.datestr2num(x)) /Users/juan/miniconda3/lib/python3.7/site-packages/pandas/core/generic.py:5303: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy self[name] = value After this, the Date column type is a datenum that can be used to correctly format the x axis.\n(By the way, this operation triggers a warning message. I let you to investigate why this is happening ;) )\nsns.relplot(x=\u0026#39;Date\u0026#39;,y=\u0026#39;ConfirmedCases\u0026#39;, data=spain,kind=\u0026#39;line\u0026#39;,) pyplot.gca().xaxis.set_major_formatter(mdates.DateFormatter(\u0026#39;%Y-%m-%d\u0026#39;)) pyplot.gca().xaxis.set_major_locator(mdates.DayLocator(interval=5)) pyplot.gcf().autofmt_xdate() pyplot.title(\u0026#39;Confirmed cases in Spain with datetime in x axis\u0026#39;) pyplot.show() So far we replicated the same plots we already created using pyplot. Why is this seaborn interesting then? I find seaborn particularly relevant to create plots where we can easily compare different series. What if we try to compare the evolution of cases in different countries? We are going to select a sample of countries and compare their evolutions.\nTo do that we have to run two operations.\nFirst. We filter the countries included in a list. Second. For some countries the values per day reflect observations per province. We are only interested in the observations per country and day. We aggregate the confirmed cases and fatalities columns for every country in the same day. # sample of countries to study chosen = [\u0026#39;Spain\u0026#39;, \u0026#39;Iran\u0026#39;, \u0026#39;Singapore\u0026#39;, \u0026#39;France\u0026#39;, \u0026#39;United Kingdom\u0026#39;] # 1) Filter rows which country is in the list 2) group by country and date and finally sum the result sample = data[data.Country_Region.isin(chosen)].groupby([\u0026#39;Date\u0026#39;,\u0026#39;Country_Region\u0026#39;], as_index=False,).sum() sample Date Country_Region Id ConfirmedCases Fatalities 0 2020-01-22 France 112510 0.0 0.0 1 2020-01-22 Iran 13601 0.0 0.0 2 2020-01-22 Singapore 20401 0.0 0.0 3 2020-01-22 Spain 20901 0.0 0.0 4 2020-01-22 United Kingdom 198807 0.0 0.0 ... ... ... ... ... ... 315 2020-03-25 France 113140 25600.0 1333.0 316 2020-03-25 Iran 13664 27017.0 2077.0 317 2020-03-25 Singapore 20464 631.0 2.0 318 2020-03-25 Spain 20964 49515.0 3647.0 319 2020-03-25 United Kingdom 199248 9640.0 466.0 320 rows × 5 columns\n# As a sanity check we are going to check that the previous operation was correct. # Lets check how many confirmed cases France had on 2020-03-24 france = data[(data.Country_Region==\u0026#39;France\u0026#39;) \u0026amp; (data.Date==\u0026#39;2020-03-24\u0026#39;)] print(\u0026#39;These are the values for France on 2020-03-24 before running the aggregation\u0026#39;) display(france) print(\u0026#39;Total number of confirmed cases: \u0026#39;, france.ConfirmedCases.sum()) print(\u0026#39;And this is the aggregation we obtained\u0026#39;) sample[(sample.Country_Region==\u0026#39;France\u0026#39;) \u0026amp; (sample.Date==\u0026#39;2020-03-24\u0026#39;)] These are the values for France on 2020-03-24 before running the aggregation Id Country_Region Province_State Date ConfirmedCases Fatalities 6974 10863 France French Guiana 2020-03-24 23.0 0.0 7038 10963 France French Polynesia 2020-03-24 25.0 0.0 7102 11063 France Guadeloupe 2020-03-24 62.0 1.0 7166 11163 France Martinique 2020-03-24 57.0 1.0 7230 11263 France Mayotte 2020-03-24 36.0 0.0 7294 11363 France New Caledonia 2020-03-24 10.0 0.0 7358 11463 France Reunion 2020-03-24 94.0 0.0 7422 11563 France Saint Barthelemy 2020-03-24 3.0 0.0 7486 11663 France St Martin 2020-03-24 8.0 0.0 7550 11763 France NaN 2020-03-24 22304.0 1100.0 Total number of confirmed cases: 22622.0 And this is the aggregation we obtained Date Country_Region Id ConfirmedCases Fatalities 310 2020-03-24 France 113130 22622.0 1102.0 We have manually checked that the values we obtained after aggregation are correct. Now we are going to plot a comparison of these values per country.\n# remember to transform the date timestamp sample.Date = sample.Date.apply(lambda x : mdates.datestr2num(x)) # Confirmed cases sns.relplot(x=\u0026#39;Date\u0026#39;,y=\u0026#39;ConfirmedCases\u0026#39;, col=\u0026#39;Country_Region\u0026#39;, hue=\u0026#39;Country_Region\u0026#39;, col_wrap=2, data=sample,kind=\u0026#39;line\u0026#39;,) pyplot.gca().xaxis.set_major_formatter(mdates.DateFormatter(\u0026#39;%Y-%m-%d\u0026#39;)) pyplot.gca().xaxis.set_major_locator(mdates.DayLocator(interval=5)) pyplot.gcf().autofmt_xdate() # Fatalities sns.relplot(x=\u0026#39;Date\u0026#39;,y=\u0026#39;Fatalities\u0026#39;, col=\u0026#39;Country_Region\u0026#39;, hue=\u0026#39;Country_Region\u0026#39;, col_wrap=2, data=sample,kind=\u0026#39;line\u0026#39;,) pyplot.gca().xaxis.set_major_formatter(mdates.DateFormatter(\u0026#39;%Y-%m-%d\u0026#39;)) pyplot.gca().xaxis.set_major_locator(mdates.DayLocator(interval=5)) pyplot.gcf().autofmt_xdate() Additionally, we can compare all the timelines in the same plot.\nsns.relplot(x=\u0026#39;Date\u0026#39;,y=\u0026#39;ConfirmedCases\u0026#39;, hue=\u0026#39;Country_Region\u0026#39;, data=sample,kind=\u0026#39;line\u0026#39;,) pyplot.gca().xaxis.set_major_formatter(mdates.DateFormatter(\u0026#39;%Y-%m-%d\u0026#39;)) pyplot.gca().xaxis.set_major_locator(mdates.DayLocator(interval=5)) pyplot.gcf().autofmt_xdate() sns.relplot(x=\u0026#39;Date\u0026#39;,y=\u0026#39;Fatalities\u0026#39;, hue=\u0026#39;Country_Region\u0026#39;, data=sample,kind=\u0026#39;line\u0026#39;,) pyplot.gca().xaxis.set_major_formatter(mdates.DateFormatter(\u0026#39;%Y-%m-%d\u0026#39;)) pyplot.gca().xaxis.set_major_locator(mdates.DayLocator(interval=5)) pyplot.gcf().autofmt_xdate() Conclusions # In this notebook we have shown how we can use Python Matplotlib and Seaborn with Pandas to plot the time series corresponding to the Covid19 virus.\n","date":"30 March 2020","permalink":"/covid-19-interactive-plots/","section":"","summary":"This is the continuation of my series about how to manipulate Covid-19 data with Python (first entry here). This post adds some extra complexity and introduces some interesting options to define interactive plots.","title":"Covid-19 interactive plots"},{"content":"","date":"30 March 2020","permalink":"/tags/jupyter/","section":"Tags","summary":"","title":"jupyter"},{"content":"This is the first post of a series of publications describing solutions used in data science. The content is self-explained in a Jupyter Notebook available at my Github account. This first post explains different ways to display time series using Python with Pandas for data management, and Matplotlib or Seaborn for data visualization. The input data used corresponds to the Covid-19 virus propagation time series.\nIntroduction # No need to say that the Covid19 crisis is a global challenge that is going to change how we see the world. There is a lot of interest in understanding the internals of virus propagation and several disciplines can be really helpful in this task. There is a lot of data going around and we have really accessible tools to work with this data.\nFor any data scientist this is a nice opportunity to explore and understand time series, graph theory and other fascinating disciplines. If you are just a newbie or a consolidated practitioner, I have decided to share a series of Jupyter Notebooks with some examples of tools and methods that you can find helpful. I will make my best to make all the code available.\nKaggle has opened a challenge to forecast the propagation of the virus. You can check the challenge with more details at the Kaggle site here. I invite you to check the notebooks uploaded by the community. I have not considered to participate in the challenge, but this could be a good opportunity if you plan to start with these kind of challenges.\nIn this part, I will use Kaggle data to show how we can visualize the virus evolution in different manners. You can download the data (after registration) here. After downloading the zip file with the dataset we have three CSV files:\ntrain.csv test.csv submission.csv For this exercise we will only use the train.csv file.\nAssumptions\nYou have an already running Jupyter environment You are familiar with Pandas You have heard about Matplotlib The covid19 files are available in the path covid19-global-forecasting-week-2 Loading a CSV with Pandas # There are several solutions to read CSV files in Python. However, with no disussion Pandas is the most suitable option for many scenarios. We import the pandas library and read the csv file with all the training data.\nimport pandas as pd data = pd.read_csv(\u0026#34;covid19-global-forecasting-week-2/train.csv\u0026#34;) data Id Country_Region Province_State Date ConfirmedCases Fatalities 0 1 Afghanistan NaN 2020-01-22 0.0 0.0 1 2 Afghanistan NaN 2020-01-23 0.0 0.0 2 3 Afghanistan NaN 2020-01-24 0.0 0.0 3 4 Afghanistan NaN 2020-01-25 0.0 0.0 4 5 Afghanistan NaN 2020-01-26 0.0 0.0 ... ... ... ... ... ... ... 18811 29360 Zimbabwe NaN 2020-03-21 3.0 0.0 18812 29361 Zimbabwe NaN 2020-03-22 3.0 0.0 18813 29362 Zimbabwe NaN 2020-03-23 3.0 1.0 18814 29363 Zimbabwe NaN 2020-03-24 3.0 1.0 18815 29364 Zimbabwe NaN 2020-03-25 3.0 1.0 18816 rows × 6 columns\nWe have a six columns dataframe indicating the country, state, date, number of confirmed cases and number of fatalities. We are going to focus on one country. Let\u0026rsquo;s say Spain.\nspain = data[data[\u0026#39;Country_Region\u0026#39;]==\u0026#39;Spain\u0026#39;] spain Id Country_Region Province_State Date ConfirmedCases Fatalities 13376 20901 Spain NaN 2020-01-22 0.0 0.0 13377 20902 Spain NaN 2020-01-23 0.0 0.0 13378 20903 Spain NaN 2020-01-24 0.0 0.0 13379 20904 Spain NaN 2020-01-25 0.0 0.0 13380 20905 Spain NaN 2020-01-26 0.0 0.0 ... ... ... ... ... ... ... 13435 20960 Spain NaN 2020-03-21 25374.0 1375.0 13436 20961 Spain NaN 2020-03-22 28768.0 1772.0 13437 20962 Spain NaN 2020-03-23 35136.0 2311.0 13438 20963 Spain NaN 2020-03-24 39885.0 2808.0 13439 20964 Spain NaN 2020-03-25 49515.0 3647.0 64 rows × 6 columns\nWe have data for 64 days with no information at a province/state level. Now we would like to have a visual representation of the time series.\nMatplotlib # The first solution to be considered is Pyplot from the Matplotlib library.\nfrom matplotlib import pyplot pyplot.plot(spain.ConfirmedCases) pyplot.title(\u0026#39;Confirmed cases in Spain\u0026#39;) pyplot.show() The figure above is the representation of the number of confirmed cases in Spain until March 26th. We have not set the X axis, so pyplot is considering the id column defined by Pandas. To define a more reasonable X ticks we simply pass a list with the same number of items of the Y axis starting from zero.\npyplot.plot(range(0,spain.ConfirmedCases.size),spain.ConfirmedCases) pyplot.title(\u0026#39;Confirmed cases in Spain\u0026#39;) pyplot.show() Now we have a clearer view of the X axis. However, we would like to have a comparison of the number of fatalities vs the number of confirmed cases.\npyplot.plot(range(0,spain.ConfirmedCases.size),spain.ConfirmedCases,label=\u0026#39;ConfirmedCases\u0026#39;) pyplot.plot(range(0,spain.Fatalities.size),spain.Fatalities,label=\u0026#39;Fatalities\u0026#39;) pyplot.legend() pyplot.title(\u0026#39;Confirmed cases vs fatalities in Spain\u0026#39;) pyplot.show() The increment shows an exponential behaviour. A logarithmic scale would help a better view.\npyplot.plot(range(0,spain.ConfirmedCases.size),spain.ConfirmedCases,label=\u0026#39;ConfirmedCases\u0026#39;) pyplot.plot(range(0,spain.Fatalities.size),spain.Fatalities,label=\u0026#39;Fatalities\u0026#39;) pyplot.yscale(\u0026#39;log\u0026#39;) pyplot.title(\u0026#39;Confirmed cases vs fatalities in Spain log scale\u0026#39;) pyplot.legend() pyplot.show() What about displaying the date in the X axis? To do that we need pyplot to format the x axis. This requires datetime structures to set the datetime of every observation. We already have them in the Date column. The main difference is setting the formatter for the x axis using mdates from matplotlib.\nimport matplotlib.dates as mdates # convert date strings to datenums dates = mdates.datestr2num(spain.Date) pyplot.gca().xaxis.set_major_formatter(mdates.DateFormatter(\u0026#39;%Y-%m-%d\u0026#39;)) pyplot.gca().xaxis.set_major_locator(mdates.DayLocator(interval=5)) pyplot.plot(dates,spain.ConfirmedCases,label=\u0026#39;confirmed\u0026#39;) pyplot.plot(dates,spain.Fatalities,label=\u0026#39;fatalities\u0026#39;) pyplot.title(\u0026#39;Confirmed cases vs fatalities in Spain with datetime in x axis\u0026#39;) pyplot.legend() pyplot.gcf().autofmt_xdate() pyplot.show() Seaborn # For those familiar with ggplot, Seaborn will look familiar. Seaborn is built on top of Matplotlib and offers a high level interface for drawing statistical graphics. It is particularly suitable to used in conjunction with Pandas.\nWe can replicate some of the plots above:\nimport seaborn as sns g = sns.relplot(x=range(spain.Date.size),y=\u0026#39;ConfirmedCases\u0026#39;, data=spain,kind=\u0026#39;line\u0026#39;,) g.set_axis_labels(x_var=\u0026#39;\u0026#39;) # I remove the xlabel for consistency with the previous plot pyplot.title(\u0026#39;Confirmed cases in Spain\u0026#39;) pyplot.show() To set the x axis with datetimes we do the same we did with matplotlib. However, now we are going to directly transform the Date column from the Pandas Dataframe so we can directly call seaborn to use it.\n# Transform the Date column to matplotlib datenum spain.Date = spain.Date.apply(lambda x : mdates.datestr2num(x)) /Users/juan/miniconda3/lib/python3.7/site-packages/pandas/core/generic.py:5303: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy self[name] = value After this, the Date column type is a datenum that can be used to correctly format the x axis.\n(By the way, this operation triggers a warning message. I let you to investigate why this is happening ;) )\nsns.relplot(x=\u0026#39;Date\u0026#39;,y=\u0026#39;ConfirmedCases\u0026#39;, data=spain,kind=\u0026#39;line\u0026#39;,) pyplot.gca().xaxis.set_major_formatter(mdates.DateFormatter(\u0026#39;%Y-%m-%d\u0026#39;)) pyplot.gca().xaxis.set_major_locator(mdates.DayLocator(interval=5)) pyplot.gcf().autofmt_xdate() pyplot.title(\u0026#39;Confirmed cases in Spain with datetime in x axis\u0026#39;) pyplot.show() So far we replicated the same plots we already created using pyplot. Why is this seaborn interesting then? I find seaborn particularly relevant to create plots where we can easily compare different series. What if we try to compare the evolution of cases in different countries? We are going to select a sample of countries and compare their evolutions.\nTo do that we have to run two operations.\nFirst. We filter the countries included in a list. Second. For some countries the values per day reflect observations per province. We are only interested in the observations per country and day. We aggregate the confirmed cases and fatalities columns for every country in the same day. # sample of countries to study chosen = [\u0026#39;Spain\u0026#39;, \u0026#39;Iran\u0026#39;, \u0026#39;Singapore\u0026#39;, \u0026#39;France\u0026#39;, \u0026#39;United Kingdom\u0026#39;] # 1) Filter rows which country is in the list 2) group by country and date and finally sum the result sample = data[data.Country_Region.isin(chosen)].groupby([\u0026#39;Date\u0026#39;,\u0026#39;Country_Region\u0026#39;], as_index=False,).sum() sample Date Country_Region Id ConfirmedCases Fatalities 0 2020-01-22 France 112510 0.0 0.0 1 2020-01-22 Iran 13601 0.0 0.0 2 2020-01-22 Singapore 20401 0.0 0.0 3 2020-01-22 Spain 20901 0.0 0.0 4 2020-01-22 United Kingdom 198807 0.0 0.0 ... ... ... ... ... ... 315 2020-03-25 France 113140 25600.0 1333.0 316 2020-03-25 Iran 13664 27017.0 2077.0 317 2020-03-25 Singapore 20464 631.0 2.0 318 2020-03-25 Spain 20964 49515.0 3647.0 319 2020-03-25 United Kingdom 199248 9640.0 466.0 320 rows × 5 columns\n# As a sanity check we are going to check that the previous operation was correct. # Lets check how many confirmed cases France had on 2020-03-24 france = data[(data.Country_Region==\u0026#39;France\u0026#39;) \u0026amp; (data.Date==\u0026#39;2020-03-24\u0026#39;)] print(\u0026#39;These are the values for France on 2020-03-24 before running the aggregation\u0026#39;) display(france) print(\u0026#39;Total number of confirmed cases: \u0026#39;, france.ConfirmedCases.sum()) print(\u0026#39;And this is the aggregation we obtained\u0026#39;) sample[(sample.Country_Region==\u0026#39;France\u0026#39;) \u0026amp; (sample.Date==\u0026#39;2020-03-24\u0026#39;)] These are the values for France on 2020-03-24 before running the aggregation Id Country_Region Province_State Date ConfirmedCases Fatalities 6974 10863 France French Guiana 2020-03-24 23.0 0.0 7038 10963 France French Polynesia 2020-03-24 25.0 0.0 7102 11063 France Guadeloupe 2020-03-24 62.0 1.0 7166 11163 France Martinique 2020-03-24 57.0 1.0 7230 11263 France Mayotte 2020-03-24 36.0 0.0 7294 11363 France New Caledonia 2020-03-24 10.0 0.0 7358 11463 France Reunion 2020-03-24 94.0 0.0 7422 11563 France Saint Barthelemy 2020-03-24 3.0 0.0 7486 11663 France St Martin 2020-03-24 8.0 0.0 7550 11763 France NaN 2020-03-24 22304.0 1100.0 Total number of confirmed cases: 22622.0 And this is the aggregation we obtained Date Country_Region Id ConfirmedCases Fatalities 310 2020-03-24 France 113130 22622.0 1102.0 We have manually checked that the values we obtained after aggregation are correct. Now we are going to plot a comparison of these values per country.\n# remember to transform the date timestamp sample.Date = sample.Date.apply(lambda x : mdates.datestr2num(x)) # Confirmed cases sns.relplot(x=\u0026#39;Date\u0026#39;,y=\u0026#39;ConfirmedCases\u0026#39;, col=\u0026#39;Country_Region\u0026#39;, hue=\u0026#39;Country_Region\u0026#39;, col_wrap=2, data=sample,kind=\u0026#39;line\u0026#39;,) pyplot.gca().xaxis.set_major_formatter(mdates.DateFormatter(\u0026#39;%Y-%m-%d\u0026#39;)) pyplot.gca().xaxis.set_major_locator(mdates.DayLocator(interval=5)) pyplot.gcf().autofmt_xdate() # Fatalities sns.relplot(x=\u0026#39;Date\u0026#39;,y=\u0026#39;Fatalities\u0026#39;, col=\u0026#39;Country_Region\u0026#39;, hue=\u0026#39;Country_Region\u0026#39;, col_wrap=2, data=sample,kind=\u0026#39;line\u0026#39;,) pyplot.gca().xaxis.set_major_formatter(mdates.DateFormatter(\u0026#39;%Y-%m-%d\u0026#39;)) pyplot.gca().xaxis.set_major_locator(mdates.DayLocator(interval=5)) pyplot.gcf().autofmt_xdate() Additionally, we can compare all the timelines in the same plot.\nsns.relplot(x=\u0026#39;Date\u0026#39;,y=\u0026#39;ConfirmedCases\u0026#39;, hue=\u0026#39;Country_Region\u0026#39;, data=sample,kind=\u0026#39;line\u0026#39;,) pyplot.gca().xaxis.set_major_formatter(mdates.DateFormatter(\u0026#39;%Y-%m-%d\u0026#39;)) pyplot.gca().xaxis.set_major_locator(mdates.DayLocator(interval=5)) pyplot.gcf().autofmt_xdate() sns.relplot(x=\u0026#39;Date\u0026#39;,y=\u0026#39;Fatalities\u0026#39;, hue=\u0026#39;Country_Region\u0026#39;, data=sample,kind=\u0026#39;line\u0026#39;,) pyplot.gca().xaxis.set_major_formatter(mdates.DateFormatter(\u0026#39;%Y-%m-%d\u0026#39;)) pyplot.gca().xaxis.set_major_locator(mdates.DayLocator(interval=5)) pyplot.gcf().autofmt_xdate() Conclusions # In this notebook we have shown how we can use Python Matplotlib and Seaborn with Pandas to plot the time series corresponding to the Covid19 virus.\n","date":"27 March 2020","permalink":"/covid19-visualization/","section":"","summary":"This is the first post of a series of publications describing solutions used in data science. The content is self-explained in a Jupyter Notebook available at my Github account. This first post explains different ways to display time series using Python with Pandas for data management, and Matplotlib or Seaborn for data visualization.","title":"Covid-19 visualization"},{"content":"","date":"1 January 0001","permalink":"/authors/","section":"Authors","summary":"","title":"Authors"},{"content":"","date":"1 January 0001","permalink":"/series/","section":"Series","summary":"","title":"Series"}]